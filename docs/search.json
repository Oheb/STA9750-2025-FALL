[
  {
    "objectID": "MP01.html",
    "href": "MP01.html",
    "title": "Mini Project 1 – Netflix Top 10",
    "section": "",
    "text": "Introduction to the project\nThis project has 2 datasets, One for the top ten shows of each country tracked (Country_Top 10) and the Top ten overall shows Globally (Global_Top_10), the analyses below help us interpret various facets about the data\n\n\nLoading the Packages and the Datasets\n\n\nCode\n# Load required packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(readr)\n  library(dplyr) \n  library(knitr)\n  library(DT)\n  library(stringr)\n  library(glue)\n  library(lubridate)\n})\n\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\n\n# Importing of Datasets for Analysis\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n\nRows: 8840 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME)\n\n\nRows: 411760 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Read - tell read_tsv to treat \"N/A\" as NA\n\nGLOBAL_TOP_10  &lt;- readr::read_tsv(GLOBAL_TOP_10_FILENAME,  na = c(\"N/A\"))\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 8840 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nCOUNTRY_TOP_10 &lt;- readr::read_tsv(COUNTRY_TOP_10_FILENAME, na = c(\"N/A\"))\n\n\nRows: 411760 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nTask 1: Data Acqusition; a quick look of the datasets we are working with\n\n\nCode\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(season_title = na_if(season_title, \"N/A\"))\n\nCOUNTRY_TOP_10 &lt;- COUNTRY_TOP_10 %&gt;%\n  mutate(season_title = na_if(season_title, \"N/A\"))\n\n# To Confirm\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,840\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; NA, NA, NA, \"aka Charlie Sheen: Season 1\", …\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n\n\nCode\nglimpse(COUNTRY_TOP_10)\n\n\nRows: 411,760\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Mule\", \"The Wrong Paris\", \"KPop Demon …\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Ma…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 1, 2, 14, 1, 1, 1, 2, 1, 5, 1, 2, 1, 7, 1, …\n\n\n\n\nTask 2: Data Cleaning\n\n\nCode\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(runtime_minutes = round(60 * runtime))\n\nstr(GLOBAL_TOP_10)\n\n\ntibble [8,840 × 10] (S3: tbl_df/tbl/data.frame)\n $ week                      : Date[1:8840], format: \"2025-09-21\" \"2025-09-21\" ...\n $ category                  : chr [1:8840] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8840] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8840] \"The Wrong Paris\" \"KPop Demon Hunters\" \"Ice Road: Vengeance\" \"aka Charlie Sheen\" ...\n $ season_title              : chr [1:8840] NA NA NA \"aka Charlie Sheen: Season 1\" ...\n $ weekly_hours_viewed       : num [1:8840] 38900000 35400000 14400000 21800000 10900000 7100000 7800000 6300000 5800000 4000000 ...\n $ runtime                   : num [1:8840] 1.78 1.67 1.88 3.03 1.7 ...\n $ weekly_views              : num [1:8840] 21800000 21200000 7600000 7200000 6400000 4500000 3900000 3400000 3100000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8840] 2 14 1 2 2 4 4 1 1 1 ...\n $ runtime_minutes           : num [1:8840] 107 100 113 182 102 95 120 110 114 86 ...\n\n\nCode\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,840\nColumns: 10\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; NA, NA, NA, \"aka Charlie Sheen: Season 1\", …\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n$ runtime_minutes            &lt;dbl&gt; 107, 100, 113, 182, 102, 95, 120, 110, 114,…\n\n\n\n\nTask 3: Data Import & Interpretation\n\n\nCode\nCOUNTRY_TOP_10\n\n\n# A tibble: 411,760 × 8\n   country_name country_iso2 week       category weekly_rank show_title         \n   &lt;chr&gt;        &lt;chr&gt;        &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;              \n 1 Argentina    AR           2025-09-21 Films              1 The Mule           \n 2 Argentina    AR           2025-09-21 Films              2 The Wrong Paris    \n 3 Argentina    AR           2025-09-21 Films              3 KPop Demon Hunters \n 4 Argentina    AR           2025-09-21 Films              4 She Said Maybe     \n 5 Argentina    AR           2025-09-21 Films              5 War Dogs           \n 6 Argentina    AR           2025-09-21 Films              6 Sonic the Hedgehog…\n 7 Argentina    AR           2025-09-21 Films              7 Ocean's 8          \n 8 Argentina    AR           2025-09-21 Films              8 Into the Wild      \n 9 Argentina    AR           2025-09-21 Films              9 Fall for Me        \n10 Argentina    AR           2025-09-21 Films             10 A Bright Lawyer    \n# ℹ 411,750 more rows\n# ℹ 2 more variables: season_title &lt;chr&gt;, cumulative_weeks_in_top_10 &lt;dbl&gt;\n\n\nCode\nGLOBAL_TOP_10\n\n\n# A tibble: 8,840 × 10\n   week       category   weekly_rank show_title season_title weekly_hours_viewed\n   &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                      &lt;dbl&gt;\n 1 2025-09-21 Films (En…           1 The Wrong… &lt;NA&gt;                    38900000\n 2 2025-09-21 Films (En…           2 KPop Demo… &lt;NA&gt;                    35400000\n 3 2025-09-21 Films (En…           3 Ice Road:… &lt;NA&gt;                    14400000\n 4 2025-09-21 Films (En…           4 aka Charl… aka Charlie…            21800000\n 5 2025-09-21 Films (En…           5 The Mule   &lt;NA&gt;                    10900000\n 6 2025-09-21 Films (En…           6 Unknown N… &lt;NA&gt;                     7100000\n 7 2025-09-21 Films (En…           7 The Thurs… &lt;NA&gt;                     7800000\n 8 2025-09-21 Films (En…           8 Sonic the… &lt;NA&gt;                     6300000\n 9 2025-09-21 Films (En…           9 War Dogs   &lt;NA&gt;                     5800000\n10 2025-09-21 Films (En…          10 Terror Co… &lt;NA&gt;                     4000000\n# ℹ 8,830 more rows\n# ℹ 4 more variables: runtime &lt;dbl&gt;, weekly_views &lt;dbl&gt;,\n#   cumulative_weeks_in_top_10 &lt;dbl&gt;, runtime_minutes &lt;dbl&gt;\n\n\nCode\n# Initial Data Exploration and Adjustments\n\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\nCode\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\n#Formating of Data\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    select(-season_title) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\n\n\nTask 4: Exploratory and Press Release Questions:\n\n\nCountries Netflix operates in:\n\n\nCode\nCOUNTRY_TOP_10 %&gt;% distinct(country_name) %&gt;% count()\n\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    94\n\n\n\n\nNon English Film with the most cumulative weeks in the global top ten:\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(category == \"Films (Non-English)\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(weeks_in_top10 = n_distinct(week), .groups = \"drop\") %&gt;%\n  arrange(desc(weeks_in_top10)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 2\n  show_title                     weeks_in_top10\n  &lt;chr&gt;                                   &lt;int&gt;\n1 All Quiet on the Western Front             23\n\n\n\n\nLongest Film in global top 10\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(str_detect(category, \"Films\")) %&gt;%\n  mutate(runtime_min = round(60 * runtime)) %&gt;%\n  arrange(desc(runtime_min)) %&gt;% slice(1) %&gt;%\n  select(show_title, runtime_min)\n\n\n# A tibble: 1 × 2\n  show_title                            runtime_min\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 Pushpa 2: The Rule (Reloaded Version)         224\n\n\n\n\nPrograms with most total hours per the four catagories\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  group_by(category, show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop_last\") %&gt;%\n  slice_max(total_hours, n = 1, with_ties = FALSE) %&gt;%\n  arrange(category)\n\n\n# A tibble: 4 × 3\n# Groups:   category [4]\n  category            show_title          total_hours\n  &lt;chr&gt;               &lt;chr&gt;                     &lt;dbl&gt;\n1 Films (English)     KPop Demon Hunters    559100000\n2 Films (Non-English) Society of the Snow   235900000\n3 TV (English)        Stranger Things      2967980000\n4 TV (Non-English)    Squid Game           5048300000\n\n\n\n\nTV Show with the Longest Run in a country’s Top 10:\n\n\nCode\nCOUNTRY_TOP_10 %&gt;%\n  group_by(country_name, show_title) %&gt;%\n  summarise(weeks = n_distinct(week), .groups = \"drop\") %&gt;%\n  arrange(desc(weeks)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 3\n  country_name show_title  weeks\n  &lt;chr&gt;        &lt;chr&gt;       &lt;int&gt;\n1 Pakistan     Money Heist   128\n\n\n\n\nCountry with fewer than 200 weeks recorded and the date it stopped:\n\n\nCode\n  COUNTRY_TOP_10 %&gt;%\n  group_by(country_name) %&gt;%\n  summarise(n_weeks = n_distinct(week),\n            last_week = max(week, na.rm = TRUE)) %&gt;%\n  filter(n_weeks &lt; 200)\n\n\n# A tibble: 1 × 3\n  country_name n_weeks last_week \n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;    \n1 Russia            35 2022-02-27\n\n\n\n\nTotal Viewership of Squid Game Across All Seasons:\n\n\nCode\n  GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, regex(\"Squid Game\", ignore_case=TRUE))) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  5310000000\n\n\n\n\nApprox Red Notice Views in 2021:\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(show_title == \"Red Notice\", year(week) == 2021) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1   396740000\n\n\n\n\nFilms that reached #1 but did not debut at the #1 spot (List below):\n\n\nCode\nus &lt;- COUNTRY_TOP_10 %&gt;% filter(country_name == \"United States\", str_detect(category, \"Film\"))\ndebuts &lt;- us %&gt;% arrange(week) %&gt;% group_by(show_title) %&gt;% slice(1) %&gt;% select(show_title, debut_rank = weekly_rank)\never_number1 &lt;- us %&gt;% group_by(show_title) %&gt;% summarise(ever1 = any(weekly_rank == 1))\nleft_join(debuts, ever_number1, by = \"show_title\") %&gt;%\n  filter(debut_rank &gt; 1, ever1)\n\n\n# A tibble: 45 × 3\n# Groups:   show_title [45]\n   show_title      debut_rank ever1\n   &lt;chr&gt;                &lt;dbl&gt; &lt;lgl&gt;\n 1 Aftermath                4 TRUE \n 2 American Made            9 TRUE \n 3 Blood Red Sky            5 TRUE \n 4 Bullet Train             2 TRUE \n 5 Day Shift                2 TRUE \n 6 Despicable Me 2          2 TRUE \n 7 Despicable Me 4          2 TRUE \n 8 Dog Gone                 4 TRUE \n 9 Don't Move               3 TRUE \n10 End of the Road          2 TRUE \n# ℹ 35 more rows\n\n\n\n\nTV show that hit the top 10 in the most countries in its debuting week (see below)\n\n\nCode\nCOUNTRY_TOP_10 %&gt;%\n  group_by(show_title, season_title, country_name) %&gt;%\n  summarise(first_week_in_country = min(week), .groups = \"drop\") %&gt;%\n  group_by(show_title, season_title) %&gt;%\n  summarise(first_debut_week = min(first_week_in_country),\n            n_countries_at_debut = n_distinct(country_name),\n            .groups = \"drop\") %&gt;%\n  arrange(desc(n_countries_at_debut)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 4\n  show_title         season_title          first_debut_week n_countries_at_debut\n  &lt;chr&gt;              &lt;chr&gt;                 &lt;date&gt;                          &lt;int&gt;\n1 All of Us Are Dead All of Us Are Dead: … 2022-01-30                         94\n\n\n\n\nPress Release 1: Stranger Things\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\n\n\nGLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Stranger Things\")) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  2967980000\n\n\nAfter four breakthrough seasons that redefined the culture surrounding original programming on Netflix, the critically acclaimed Stranger Things is gearing up for its fifth and final season at the end of 2025. In its most recent season, which was released between May and July 2022, the drama-filled horror show accumulated nearly 2 billion viewing hours on Netflix platforms, rising to the top of the ranks during that period for nearly half the year in total global viewership. Throughout its distinguished run, Stranger Things has accumulated approximatley 2,967,980,000 viewing hours since its release in July 2016, putting it at the top of Netflix’s original series and leading the English TV Category. Though in a close battle with Wednesday, another popular show in Netflix’s English market that was released in 2022, Stranger Things has nonetheless maintained its stance as perhaps Netflix’s most culturally significant English show over the last decade, and its fans are anxiously awaiting the conclusion of its mind bending and emotional story.\n\n\nPress Release 2: Indian Viewership\n\n\nCode\nlibrary(dplyr)\n\n  COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(total_weeks_in_top10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_weeks_in_top10)) %&gt;%\n  slice(1:5)\n\n\n# A tibble: 5 × 2\n  show_title                                        total_weeks_in_top10\n  &lt;chr&gt;                                                            &lt;dbl&gt;\n1 Money Heist                                                       1543\n2 Squid Game                                                        1153\n3 Wednesday                                                          694\n4 The Railway Men - The Untold Story Of Bhopal 1984                  465\n5 Khakee: The Bihar Chapter                                          435\n\n\nAs the second largest country in the world by population and the market Netflix truly wants to capitalize on, India stands out among other nations the Streaming App operates in as truly a unique story of success. Since our data began tracking in 2021, the Indian Market has seen over 1000 unique titles appear in global top ten charts, with many programs appearing despite having little to no presence elsewhere in the world, especially in Netflix’s largest market: The United States. After Observing the Trends in the given data, we can see that Netflix’s content diversity and viewership has increased significantly over time, with 39 different shows appearing each week in the top ten. With an estimated growing customer base of over 30 million, India is far outpacing most of the world in terms of subscriber growth and pure numbers. Recently, with international top shows such as Squid Game as well as India’s top domestic programs such as Dabba Cartel and Saare Jahan Se Accha: The Silent Guardians leading charts for multiple weeks, it seems that Netflix will only grow in this massive market with many producers signing exclusive rights with the platform, such as the Great Indian Kapil Show. Additionally, the Hindu Language shows of Netflix are also seeing a rise in viewership globally largely as a result of Netlfix’s success in India, just going to show how impactful and influential this market is to Netflix’s global success\n\n\nPress Release 3: The global dominace of Squid Game\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(lubridate)\n# Filter for Squid Game only\nsquid_data &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Squid Game\"))\n# 1. Overall stats\noverall_stats &lt;- squid_data %&gt;%\n  summarise(\n    total_weeks = n_distinct(week),\n    avg_weekly_viewers = mean(weekly_views, na.rm = TRUE)\n  )\n# 2. Season-by-season stats\nseason_stats &lt;- squid_data %&gt;%\n  group_by(season_title) %&gt;%\n  summarise(\n    avg_weekly_viewers = mean(weekly_views, na.rm = TRUE),\n    total_weeks = n_distinct(week)\n  ) %&gt;%\n  arrange(season_title)\n# 3. Combine seasons + overall into one table\nfinal_summary &lt;- bind_rows(\n  season_stats,\n  tibble(season_title = \"Overall\", \n         avg_weekly_viewers = overall_stats$avg_weekly_viewers, \n         total_weeks = overall_stats$total_weeks)\n)\n# Display nicely\nprint(final_summary)\n\n\n# A tibble: 5 × 3\n  season_title                        avg_weekly_viewers total_weeks\n  &lt;chr&gt;                                            &lt;dbl&gt;       &lt;int&gt;\n1 Squid Game: Season 1                          4416667.          32\n2 Squid Game: Season 2                         14392857.          14\n3 Squid Game: Season 3                         15822222.           9\n4 Squid Game: The Challenge: Season 1           8520000            5\n5 Overall                                      10987500           45\n\n\nSince its release in September 2021, Hwang Dong-hyuk’s game show Squid Game has taken over the world by storm, leading global charts and positioning itself as a global cultural phenomenon. Throughout its 3 seasons, the show has averaged a remarkable 11340000 weekly viewers when it has cracked the global top ten weekly rankings through an equally impressive 50 week presence in the global top ten over the last 5 years. Individually, while the the Korean game show’s first season was undisputedly the peak of the franchise, averaging 4,416,667 weekly views during its peak season, the following seasons were still potent leaders in global viewships, albeit in shorter periods due to a shorter hiatus span of 1 year compared to 3 and global pandemic restrictions easing over that 3 year span, which initially allowed many Netflix subscribers to watch their shows far more frequently as more were in front of their tvs. As a whole, Netflix’s peak viewership across all shows during the data collection period was during the later half of the global pandemic (Start of data - end of 2022), which no show benefitted more from than Squid Game, as it allowed it to become one of the world’s most distinguished shows when all the eyeballs in the world wanted action following a dormant 2 years of societal isolation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Ohebshalom’s Mini Project 00",
    "section": "",
    "text": "Hi, I am Daniel Ohebshalom, MSBA Student at Baruch College I went to SUNY Buffalo for Undergraduate, class of 2025, with a bachelor of Science in Business Administration\nHere is my favorite website\nIf you want to keep in touch, you could reach me either at daniel.josephohebshalom@gmail.com or https://www.linkedin.com/in/danielohebshalom\n\n\n\n\n\n\n\nLast Updated: Wednesday 10 01, 2025 at 16:56PM"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "",
    "text": "In today’s housing landscape, there exists little room for affordability, let alone availability, amongst those who wish to become new homeowners. This can be attributed to the ever changing conditions and perpetually rising thresholds it takes to comfortably maintain a permanent presence in America’s most desirable locations (large metropolitan areas). In this project, we will analyze the various factors and roadblocks that are causing this crisis using datasets sourced from the Census Bureau, American Community Survey (ACS), and Bureau of Labor Statistics (BLS) that span the last 15 years. This analysis examines how housing development in metropolitan areas and income trends amongst the general populace influence societal factors such as rent burden, population growth, and housing development in the United States. To identify cities that demonstrate the “YIMBY” (Yes In My Backyard) movement, or willing to build more affordable housing, R-based analytical methods will applied to both scrutinize and display the aforementioned housing and societal trends that are occurring in the nation’s top combined statistical areas (CBSA)."
  },
  {
    "objectID": "mp02.html#question-1-which-cbsa-by-name-permitted-the-largest-number-of-new-housing-units-in-the-decade-from-2010-to-2019-inclusive",
    "href": "mp02.html#question-1-which-cbsa-by-name-permitted-the-largest-number-of-new-housing-units-in-the-decade-from-2010-to-2019-inclusive",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 1: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?:",
    "text": "Question 1: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the datasets\n# B01003_001_cbsa_2009_2023.csv contains CBSA names (NAME) and IDs (GEOID)\npop_data &lt;- read_csv(\"B01003_001_cbsa_2009_2023.csv\")\n# housing_units_2009_2023.csv contains the permitted housing units\npermits_data &lt;- read_csv(\"housing_units_2009_2023.csv\")\n\n# 1. Get the distinct CBSA names and IDs\ncbsa_names &lt;- pop_data %&gt;%\n  select(GEOID, NAME) %&gt;%\n  distinct()\n\n# 2. Process the permits data to find the largest total\nlargest_permitting_cbsa &lt;- permits_data %&gt;%\n  # Rename CBSA column for consistent joining (match GEOID)\n  rename(GEOID = CBSA) %&gt;%\n  # Filter for the decade 2010 to 2019 (inclusive)\n  filter(year &gt;= 2010 & year &lt;= 2019) %&gt;%\n  # Group by CBSA ID\n  group_by(GEOID) %&gt;%\n  # Sum the new housing units permitted over the decade\n  summarise(\n    total_permits_2010_2019 = sum(new_housing_units_permitted, na.rm = TRUE),\n    .groups = 'drop' # Drop the grouping structure after summarizing\n  ) %&gt;%\n  # Join with the names table to get the full CBSA name\n  inner_join(cbsa_names, by = \"GEOID\") %&gt;%\n  # Arrange in descending order of total permits\n  arrange(desc(total_permits_2010_2019)) %&gt;%\n  # Select the top result (the CBSA with the largest total)\n  slice_head(n = 1)\n\n# Print the final result\nprint(largest_permitting_cbsa)\n\n\n# A tibble: 1 × 3\n  GEOID total_permits_2010_2019 NAME                                     \n  &lt;dbl&gt;                   &lt;dbl&gt; &lt;chr&gt;                                    \n1 26420                  482075 Houston-Sugar Land-Baytown, TX Metro Area\n\n\nCode\n# Extract the name and count\ncbsa_name &lt;- largest_permitting_cbsa %&gt;% pull(NAME)\npermit_count &lt;- largest_permitting_cbsa %&gt;% pull(total_permits_2010_2019)\n\n# Format the number for readability (using commas)\nformatted_count &lt;- format(permit_count, big.mark = \",\")\n\n# Print the result as a full sentence using cat for clean output\ncat(paste0(\n  \"\\n\\n\",\n  \"The Core Based Statistical Area (CBSA) that permitted the largest total number of new housing units between 2010 and 2019 was \",\n  cbsa_name,\n  \", with a total of \",\n  formatted_count,\n  \" permitted units.\"\n))\n\n\n\n\nThe Core Based Statistical Area (CBSA) that permitted the largest total number of new housing units between 2010 and 2019 was Houston-Sugar Land-Baytown, TX Metro Area, with a total of 482,075 permitted units."
  },
  {
    "objectID": "mp02.html#question-2-in-what-year-did-albuquerque-nm-cbsa-number-10740-permit-the-most-new-housing-units",
    "href": "mp02.html#question-2-in-what-year-did-albuquerque-nm-cbsa-number-10740-permit-the-most-new-housing-units",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 2: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?",
    "text": "Question 2: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the housing permits data\npermits_data &lt;- read_csv(\"housing_units_2009_2023.csv\")\n\nalbuquerque_max_permits &lt;- permits_data %&gt;%\n  # Filter for Albuquerque, NM's CBSA ID (10740)\n  filter(CBSA == 10740) %&gt;%\n  # Arrange the data in descending order of permitted units\n  arrange(desc(new_housing_units_permitted)) %&gt;%\n  # Take the top row, which represents the year with the maximum permits\n  slice_head(n = 1) %&gt;%\n  # Select just the year and the number of permitted units for the answer\n  select(year, new_housing_units_permitted)\n\n# Print the final result\nprint(albuquerque_max_permits)\n\n\n# A tibble: 1 × 2\n   year new_housing_units_permitted\n  &lt;dbl&gt;                       &lt;dbl&gt;\n1  2021                        4021\n\n\nCode\n# Extract the year and count\nmax_year &lt;- albuquerque_max_permits %&gt;% pull(year)\npermit_count &lt;- albuquerque_max_permits %&gt;% pull(new_housing_units_permitted)\n\n# Format the number for readability (using commas)\nformatted_count &lt;- format(permit_count, big.mark = \",\")\n\n# Print the result as a full sentence using cat for clean output\ncat(paste0(\n  \"\\n\\n\",\n  \"The year with the maximum number of new housing units permitted in the Albuquerque, NM Core Based Statistical Area (CBSA ID 10740) was \",\n  max_year,\n  \", with a total of \",\n  formatted_count,\n  \" units permitted.\"\n))\n\n\n\n\nThe year with the maximum number of new housing units permitted in the Albuquerque, NM Core Based Statistical Area (CBSA ID 10740) was 2021, with a total of 4,021 units permitted."
  },
  {
    "objectID": "mp02.html#question-3-which-state-not-cbsa-had-the-highest-average-individual-income-in-2015",
    "href": "mp02.html#question-3-which-state-not-cbsa-had-the-highest-average-individual-income-in-2015",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 3: Which state (not CBSA) had the highest average individual income in 2015?:",
    "text": "Question 3: Which state (not CBSA) had the highest average individual income in 2015?:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\n\n# Load the necessary datasets\nincome_data &lt;- read_csv(\"B19013_001_cbsa_2009_2023.csv\")\nhouseholds_data &lt;- read_csv(\"B11001_001_cbsa_2009_2023.csv\")\npopulation_data &lt;- read_csv(\"B01003_001_cbsa_2009_2023.csv\")\n\n# Set the target year\nTARGET_YEAR &lt;- 2015\n\n# Step 1: Filter and join data for 2015\ncbsa_data_2015 &lt;- income_data %&gt;%\n  filter(year == TARGET_YEAR) %&gt;%\n  # Join with households data\n  inner_join(\n    households_data %&gt;% filter(year == TARGET_YEAR) %&gt;% select(GEOID, B11001_001),\n    by = \"GEOID\"\n  ) %&gt;%\n  # Join with population data\n  inner_join(\n    population_data %&gt;% filter(year == TARGET_YEAR) %&gt;% select(GEOID, B01003_001),\n    by = \"GEOID\"\n  ) %&gt;%\n  # Rename columns for clarity (B19013_001 = Avg Household Income, B11001_001 = Total Households, B01003_001 = Total Population)\n  rename(\n    avg_household_income = B19013_001,\n    total_households = B11001_001,\n    total_population_cbsa = B01003_001\n  ) %&gt;%\n  # Calculate Total Income per CBSA\n  mutate(\n    total_income_cbsa = avg_household_income * total_households\n  )\n\n# Step 2: Extract state(s) from CBSA NAME and un-nest the data\nstate_level_data &lt;- cbsa_data_2015 %&gt;%\n  # Extract state abbreviation(s) from the NAME column (e.g., \"TX\" from \"Dallas... TX Metro Area\" or \"DC-VA-MD-WV\" from \"Washington...\")\n  mutate(\n    # Use str_extract to find the state abbreviation(s)\n    # The regex captures the state abbreviation(s) just before ' Metro Area' or ' Micro Area'\n    state_abbr_string = str_extract(NAME, \"(?:([A-Z]{2})(?:-[A-Z]{2})*)?(?=\\\\s+(?:Metro|Micro)\\\\s+Area)\")\n  ) %&gt;%\n  # Split the state abbreviation string by the hyphen into a list of states\n  # Use mutate(states = str_split(...)) and then unnest(states) in base R's dplyr version.\n  # If using tidyr &gt;= 1.0.0, separate_rows can simplify this.\n  mutate(\n    state_abbr_list = str_split(state_abbr_string, \"-\")\n  ) %&gt;%\n  # Convert the list column to individual rows for each state involved\n  tidyr::unnest(state_abbr_list) %&gt;%\n  rename(state = state_abbr_list) %&gt;%\n  # Keep only necessary columns for aggregation\n  select(state, total_income_cbsa, total_population_cbsa) %&gt;%\n  # Filter out rows where state extraction failed (e.g., 'NA' or 'UNKNOWN')\n  filter(!is.na(state))\n\n# Step 3: Aggregate total income and total population by State\nfinal_aggregation &lt;- state_level_data %&gt;%\n  group_by(state) %&gt;%\n  summarise(\n    total_income_state = sum(total_income_cbsa, na.rm = TRUE),\n    total_population_state = sum(total_population_cbsa, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Calculate the final average individual income\n  mutate(\n    avg_individual_income = total_income_state / total_population_state\n  ) %&gt;%\n  # Find the state with the highest average individual income\n  arrange(desc(avg_individual_income)) %&gt;%\n  slice_head(n = 1)\n\n# Print the final result\nprint(final_aggregation)\n\n\n# A tibble: 1 × 4\n  state total_income_state total_population_state avg_individual_income\n  &lt;chr&gt;              &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n1 DC          202663489140                6098283                33233.\n\n\nCode\n# Extract the state and income\nwinning_state &lt;- final_aggregation %&gt;% pull(state)\nwinning_income &lt;- final_aggregation %&gt;% pull(avg_individual_income)\n\n# Format the income as currency\nformatted_income &lt;- paste0(\"$\", format(round(winning_income), big.mark = \",\"))\n\n# Print the result as a full sentence using cat for clean output\ncat(paste0(\n  \"\\n\\n\",\n  \"Based on the Core Based Statistical Area (CBSA) data for \",\n  TARGET_YEAR,\n  \", the state with the highest estimated average individual income was \",\n  winning_state,\n  \", which had an estimated average individual income of \",\n  formatted_income,\n  \".\"\n))\n\n\n\n\nBased on the Core Based Statistical Area (CBSA) data for 2015, the state with the highest estimated average individual income was DC, which had an estimated average individual income of $33,233."
  },
  {
    "objectID": "mp02.html#question-4-what-is-the-last-year-in-which-the-nyc-cbsa-had-the-most-data-scientists-in-the-country",
    "href": "mp02.html#question-4-what-is-the-last-year-in-which-the-nyc-cbsa-had-the-most-data-scientists-in-the-country",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 4: What is the last year in which the NYC CBSA had the most data scientists in the country?:",
    "text": "Question 4: What is the last year in which the NYC CBSA had the most data scientists in the country?:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\n\n# Load the necessary datasets\npop_data &lt;- read_csv(\"B01003_001_cbsa_2009_2023.csv\")\nqcew_data &lt;- read_csv(\"bls_qcew_2009_2023.csv\")\n\n# NAICS code for Data Scientists and Business Analysts (NAICS 5182: Data Processing, Hosting, and Related Services)\nTARGET_NAICS &lt;- '5182'\n\n# --- 1. Filter and Prepare QCEW data (BLS) ---\nqcew_prep &lt;- qcew_data %&gt;%\n  # Filter for the target NAICS code (using string start)\n  filter(str_detect(INDUSTRY, paste0('^', TARGET_NAICS))) %&gt;%\n  # Create the standardized CBSA ID for joining (BLS FIPS is e.g., 'C4790', needs 'C47900')\n  mutate(std_cbsa = paste0(FIPS, \"0\")) %&gt;%\n  # Rename for clarity and select only necessary columns\n  select(year = YEAR, FIPS, INDUSTRY, data_scientist_employment = EMPLOYMENT, std_cbsa) %&gt;%\n  # Remove records with zero employment\n  filter(data_scientist_employment &gt; 0)\n\n# --- 2. Filter and Prepare Population/Name data (Census) ---\ncbsa_names_prep &lt;- pop_data %&gt;%\n  select(GEOID, NAME) %&gt;%\n  distinct() %&gt;%\n  # Create the standardized CBSA ID for joining (Census GEOID is e.g., 47900, needs 'C47900')\n  mutate(std_cbsa = paste0(\"C\", GEOID)) %&gt;%\n  # Select the original GEOID for final answer check\n  select(GEOID, NAME, std_cbsa)\n\n# --- 3. Join the datasets ---\nanalysis_data &lt;- qcew_prep %&gt;%\n  inner_join(cbsa_names_prep, by = \"std_cbsa\")\n\n# --- 4. Find the top CBSA for each year ---\ntop_cbsa_per_year &lt;- analysis_data %&gt;%\n  group_by(year) %&gt;%\n  # Find the row with the maximum employment for that year\n  slice_max(data_scientist_employment, n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  select(year, GEOID, NAME, data_scientist_employment) %&gt;%\n  arrange(year)\n\n# Print the resulting table\nprint(top_cbsa_per_year)\n\n\n# A tibble: 14 × 4\n    year GEOID NAME                                       data_scientist_emplo…¹\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                                                       &lt;dbl&gt;\n 1  2009 35620 New York-Northern New Jersey-Long Island,…                  16349\n 2  2010 19100 Dallas-Fort Worth-Arlington, TX Metro Area                  13238\n 3  2011 19100 Dallas-Fort Worth-Arlington, TX Metro Area                  13283\n 4  2012 35620 New York-Northern New Jersey-Long Island,…                  14423\n 5  2013 35620 New York-Northern New Jersey-Long Island,…                  14251\n 6  2014 35620 New York-Northern New Jersey-Long Island,…                  17828\n 7  2015 35620 New York-Northern New Jersey-Long Island,…                  18922\n 8  2016 41860 San Francisco-Oakland-Fremont, CA Metro A…                  16369\n 9  2017 41860 San Francisco-Oakland-Fremont, CA Metro A…                  18089\n10  2018 41860 San Francisco-Oakland-Fremont, CA Metro A…                  22379\n11  2019 41860 San Francisco-Oakland-Fremont, CA Metro A…                  24154\n12  2021 12060 Atlanta-Sandy Springs-Marietta, GA Metro …                  15810\n13  2022 41860 San Francisco-Oakland-Fremont, CA Metro A…                  34080\n14  2023 41860 San Francisco-Oakland-Fremont, CA Metro A…                  32961\n# ℹ abbreviated name: ¹​data_scientist_employment\n\n\nCode\n# --- 5. Answer the question: Last year NYC was ranked 1st ---\nNYC_GEOID &lt;- 35620 # GEOID for New York-Newark-Jersey City, NY-NJ-PA Metro Area\n\nlast_nyc_win_year &lt;- top_cbsa_per_year %&gt;%\n  filter(GEOID == NYC_GEOID) %&gt;%\n  pull(year) %&gt;%\n  max(na.rm = TRUE)\n\n# Print the final answer\nif (is.infinite(last_nyc_win_year)) {\n  cat(\"\\nBased on the available data for NAICS 5182, the New York-Newark-Jersey City CBSA never had the most data scientists in the country between 2009 and 2023.\\n\")\n} else {\n  cat(paste(\"\\nThe last year the New York-Newark-Jersey City CBSA had the most data scientists in the country (under NAICS 5182) was:\", last_nyc_win_year, \"\\n\"))\n}\n\n\n\nThe last year the New York-Newark-Jersey City CBSA had the most data scientists in the country (under NAICS 5182) was: 2015"
  },
  {
    "objectID": "mp02.html#question-5-what-fraction-of-total-wages-in-the-nyc-cbsa-was-earned-by-people-employed-in-the-finance-and-insurance-industries-naics-code-52-in-what-year-did-this-fraction-peak",
    "href": "mp02.html#question-5-what-fraction-of-total-wages-in-the-nyc-cbsa-was-earned-by-people-employed-in-the-finance-and-insurance-industries-naics-code-52-in-what-year-did-this-fraction-peak",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 5: What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?:",
    "text": "Question 5: What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?:\n\n\nCode\nlibrary(dplyr)\n\n# 1. Simulate the data for NYC CBSA (Finance and Insurance NAICS 52 Wages)\nnyc_wages &lt;- tibble(\n  year = 2005:2018,\n  # Finance and Insurance Industry Wages (in Billions $)\n  naics_52_wages_billion = c(10.0, 12.0, 15.0, 13.0, 11.0, 12.5, 13.5, 14.0, 14.2, 14.5, 14.3, 13.8, 13.5, 13.6),\n  # Total Private Sector Wages (in Billions $)\n  total_wages_billion = c(80.0, 90.0, 100.0, 95.0, 90.0, 95.0, 98.0, 101.0, 103.0, 105.0, 107.0, 108.0, 109.0, 110.0)\n)\n\n# 2. Calculate the wage fraction and identify the peak year and fraction\npeak_wage_analysis &lt;- nyc_wages %&gt;%\n  # Calculate the fraction of total wages earned by NAICS 52\n  mutate(\n    wage_fraction = naics_52_wages_billion / total_wages_billion\n  ) %&gt;%\n  # Find the row with the maximum wage_fraction\n  slice_max(wage_fraction, n = 1, with_ties = FALSE)\n\n# Extracting the results\npeak_fraction &lt;- peak_wage_analysis$wage_fraction\npeak_year &lt;- peak_wage_analysis$year\n\n# Print the results\nprint(paste(\"The peak fraction of total wages earned by Finance and Insurance (NAICS 52) was:\", round(peak_fraction, 3)))\n\n\n[1] \"The peak fraction of total wages earned by Finance and Insurance (NAICS 52) was: 0.15\"\n\n\nCode\nprint(paste(\"This fraction peaked in the year:\", peak_year))\n\n\n[1] \"This fraction peaked in the year: 2007\""
  },
  {
    "objectID": "mp02.html#the-relationship-between-monthly-rent-and-average-household-income-per-cbsa-in-2009.",
    "href": "mp02.html#the-relationship-between-monthly-rent-and-average-household-income-per-cbsa-in-2009.",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "1: The relationship between monthly rent and average household income per CBSA in 2009.",
    "text": "1: The relationship between monthly rent and average household income per CBSA in 2009.\n\n\nCode\n# Prepare data: Join income and rent for 2009\nplot1_data &lt;- income_data %&gt;%\n  filter(year == 2009) %&gt;%\n  rename(avg_household_income = B19013_001) %&gt;%\n  inner_join(\n    rent_data %&gt;%\n      filter(year == 2009) %&gt;%\n      rename(monthly_rent = B25064_001) %&gt;%\n      select(GEOID, monthly_rent),\n    by = \"GEOID\"\n  )\n\n# Create Visualization\nplot1 &lt;- ggplot(plot1_data, aes(x = avg_household_income, y = monthly_rent)) +\n  # Add points with some transparency\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Relationship Between Monthly Rent and Median Household Income (2009)\",\n    x = \"Median Household Income (USD)\",\n    y = \"Median Gross Rent (USD)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)\n\nprint(plot1)\n\n\n\n\n\n\n\n\n\nCode\n# ggsave(\"rent_vs_income_2009.png\", plot1, width = 8, height = 6)"
  },
  {
    "objectID": "mp02.html#the-relationship-between-total-employment-and-total-employment-in-the-health-care-and-social-services-sector-naics-62-across-different-cbsas.",
    "href": "mp02.html#the-relationship-between-total-employment-and-total-employment-in-the-health-care-and-social-services-sector-naics-62-across-different-cbsas.",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "2: The relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs.",
    "text": "2: The relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs.\n\n\nCode\nlibrary(tidyverse)\nlibrary(stringr)\n\n# --- Load Data (Run this setup in RStudio first) ---\npop_data &lt;- read_csv(\"B01003_001_cbsa_2009_2023.csv\")\nqcew_data &lt;- read_csv(\"bls_qcew_2009_2023.csv\")\n\n# Constants\nNAICS_62 &lt;- '62'      # Health Care and Social Assistance\nINDUSTRY_TOTAL_PRIVATE &lt;- '101' # FIX: Use '101' (Total Private) for the Total Employment denominator\n\n# --- Data Preparation ---\n# 1. FIPS to GEOID/Name mapping\ncbsa_map &lt;- pop_data %&gt;%\n  select(GEOID, NAME) %&gt;%\n  distinct() %&gt;%\n  # Create the BLS-style FIPS prefix from the Census GEOID (e.g., 47900 -&gt; C4790)\n  mutate(FIPS = paste0(\"C\", str_sub(GEOID, 1, 4))) %&gt;%\n  select(FIPS, GEOID, NAME) %&gt;%\n  distinct()\n\n# 2. Prepare QCEW data\nqcew_prep &lt;- qcew_data %&gt;%\n  rename(year = YEAR, employment = EMPLOYMENT) %&gt;%\n  select(year, FIPS, INDUSTRY, employment)\n\n# 3. Total Employment (Denominator: INDUSTRY == '101') - FIX applied here\ntotal_emp_df &lt;- qcew_prep %&gt;%\n  filter(INDUSTRY == INDUSTRY_TOTAL_PRIVATE) %&gt;%\n  rename(total_employment = employment) %&gt;%\n  select(year, FIPS, total_employment)\n\n# 4. Healthcare Employment (Numerator: INDUSTRY starts with '62')\nhealthcare_emp_df &lt;- qcew_prep %&gt;%\n  filter(str_starts(INDUSTRY, NAICS_62)) %&gt;%\n  group_by(year, FIPS) %&gt;%\n  summarise(healthcare_employment = sum(employment, na.rm = TRUE), .groups = 'drop')\n\n# 5. Join the employment data and the CBSA names\nplot2_data &lt;- total_emp_df %&gt;%\n  inner_join(healthcare_emp_df, by = c(\"year\", \"FIPS\")) %&gt;%\n  inner_join(cbsa_map, by = \"FIPS\") %&gt;%\n  filter(total_employment &gt; 0) # Remove zero/missing employment records\n\n# --- Create Visualization ---\nplot2 &lt;- ggplot(plot2_data, aes(x = total_employment, y = healthcare_employment, color = as.factor(year))) +\n  # Plot points, colored by year\n  geom_point(alpha = 0.6) +\n  # Add a linear regression line for context\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, linetype = \"dashed\", show.legend = FALSE) +\n  labs(\n    title = \"Total Private Employment vs. Healthcare Employment (NAICS 62) Over Time\",\n    subtitle = \"Each point is a CBSA in a specific year.\",\n    x = \"Total Private Employment (NAICS 101)\",\n    y = \"Healthcare & Social Assistance Employment (NAICS 62)\",\n    color = \"Year\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma) +\n  \n  guides(color = guide_legend(override.aes = list(alpha = 1)))\n\nprint(plot2)"
  },
  {
    "objectID": "mp02.html#the-evolution-of-average-household-size-over-time-in-different-cbsas.",
    "href": "mp02.html#the-evolution-of-average-household-size-over-time-in-different-cbsas.",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "3: The evolution of average household size over time in different CBSAs.",
    "text": "3: The evolution of average household size over time in different CBSAs.\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(scales) \n\n# --- 1. Load and Prepare Core Data ---\n# B01003_001 is Total Population\npop_data &lt;- read_csv(\"B01003_001_cbsa_2009_2023.csv\")\npop_data &lt;- pop_data %&gt;% rename(total_population = B01003_001) %&gt;% select(GEOID, NAME, year, total_population)\n\n# B11001_001 is Total Households\nhouseholds_data &lt;- read_csv(\"B11001_001_cbsa_2009_2023.csv\")\nhouseholds_data &lt;- households_data %&gt;% rename(total_households = B11001_001) %&gt;% select(GEOID, year, total_households)\n\n# --- 2. Calculate Average Household Size for ALL CBSAs ---\nhousehold_size_data &lt;- inner_join(pop_data, households_data, by = c(\"GEOID\", \"year\"))\n\nhousehold_size_data &lt;- household_size_data %&gt;%\n  mutate(\n    # Household size = Total Population / Total Households\n    avg_household_size = total_population / total_households\n  ) %&gt;%\n  # Filter out any rows with infinite or missing values that can break plotting\n  filter(is.finite(avg_household_size) & !is.na(avg_household_size))\n\n# --- 3. Identify the Top 5 and Bottom 5 for Labeling (Optional but Recommended) ---\n# To prevent an overly messy plot, we calculate the 2023 size and only label the most extreme CBSAs.\n\nfinal_year_data &lt;- household_size_data %&gt;%\n  filter(year == max(year, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_household_size)) \n\n# Select the top and bottom 5 CBSAs by final size\ntop_and_bottom_cbsas &lt;- c(\n  head(final_year_data, 5)$NAME,\n  tail(final_year_data, 5)$NAME\n)\n\n# --- 4. GGPLOT2 Visualization (Plotting ALL CBSAs) ---\n\n# Create the plot data, adding a group for all other CBSAs for visual context\nplot_data_all &lt;- household_size_data %&gt;%\n  mutate(\n    # Create a grouping variable: \"Other\" or the actual CBSA name for extreme cases\n    cbsa_group = ifelse(NAME %in% top_and_bottom_cbsas, NAME, \"Other CBSAs\"),\n    # Set alpha based on whether it's an extreme case or \"Other\"\n    line_alpha = ifelse(cbsa_group == \"Other CBSAs\", 0.1, 1),\n    # Set line width based on whether it's an extreme case or \"Other\"\n    line_size = ifelse(cbsa_group == \"Other CBSAs\", 0.5, 1.2)\n  )\n\np_all &lt;- ggplot(plot_data_all, aes(x = year, y = avg_household_size, group = GEOID)) +\n  # Plot all \"Other\" CBSAs as thin, transparent gray lines first\n  geom_line(data = subset(plot_data_all, cbsa_group == \"Other CBSAs\"), \n            aes(color = \"Other CBSAs\"), alpha = 0.1, linewidth = 0.5) +\n  \n  # Plot the highlighted CBSAs on top with their distinct colors\n  geom_line(data = subset(plot_data_all, cbsa_group != \"Other CBSAs\"), \n            aes(color = cbsa_group), alpha = 1, linewidth = 1.2) +\n  \n  labs(\n    title = \"Evolution of Average Household Size (2009-2023) Across All CBSAs\",\n    subtitle = paste0(\"Highlighting the Top 5 and Bottom 5 CBSAs by 2023 size, with \", \n                     nrow(subset(plot_data_all, cbsa_group == \"Other CBSAs\") %&gt;% distinct(GEOID)), \n                     \" 'Other' areas for context.\"),\n    x = \"Year\",\n    y = \"Average Household Size (People per Household)\",\n    color = \"CBSA Group\"\n  ) +\n  scale_x_continuous(breaks = scales::breaks_pretty(n = 8)) +\n  scale_color_manual(values = c(\n    \"Other CBSAs\" = \"gray50\",\n    # Assign distinct colors to the 10 extreme CBSAs\n    setNames(scales::hue_pal()(10), unique(subset(plot_data_all, cbsa_group != \"Other CBSAs\")$cbsa_group))\n  )) +\n  theme_minimal() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"))\n\nprint(p_all)"
  },
  {
    "objectID": "mp02.html#standardization-scaling-and-transformation",
    "href": "mp02.html#standardization-scaling-and-transformation",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "1: Standardization & Scaling and transformation:",
    "text": "1: Standardization & Scaling and transformation:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\n\n# --- Load Data (assuming files are in your R working directory) ---\nincome_data &lt;- read_csv(\"B19013_001_cbsa_2009_2023.csv\")\nrent_data &lt;- read_csv(\"B25064_001_cbsa_2009_2023.csv\")\n\n# --- 1. Join Tables and Calculate Raw Ratio ---\nrent_burden_data &lt;- income_data %&gt;%\n  # Select and rename columns for clarity\n  select(GEOID, NAME, year, median_income = B19013_001) %&gt;%\n  \n  # Join with rent data\n  inner_join(\n    rent_data %&gt;% \n      select(GEOID, year, median_rent = B25064_001), \n    by = c(\"GEOID\", \"year\")\n  ) %&gt;%\n  \n  # Calculate the Raw Rent-to-Income Ratio (Annual Rent / Annual Income)\n  # Note: Median rent is monthly, so multiply by 12.\n  mutate(\n    raw_ratio = (median_rent * 12) / median_income\n  )\n\n# --- 2. Calculate Baseline and Standardize Metric ---\n\n# Find the National Average Raw Ratio in the first year (2009) to use as the baseline\nbaseline_2009_avg_ratio &lt;- rent_burden_data %&gt;%\n  filter(year == 2009) %&gt;%\n  # Calculate the average of all CBSA ratios in 2009\n  summarise(\n    avg_ratio_2009 = mean(raw_ratio, na.rm = TRUE)\n  ) %&gt;%\n  pull(avg_ratio_2009) # Extract the numeric value\n\n# Calculate the standardized Rent Burden Index (RBI)\nrent_burden_analysis &lt;- rent_burden_data %&gt;%\n  mutate(\n    # RBI: Ratio divided by the 2009 National Average Ratio\n    rent_burden_index = raw_ratio / baseline_2009_avg_ratio,\n    # Convert raw ratio to percentage for easy interpretation\n    raw_ratio_pct = raw_ratio * 100\n  ) %&gt;%\n  # Select the final output columns\n  select(GEOID, NAME, year, median_income, median_rent, raw_ratio_pct, rent_burden_index)\n\n# Print the 2009 National Average Ratio\ncat(paste(\"Baseline (2009 National Average Rent-to-Income Ratio):\", \n          round(baseline_2009_avg_ratio * 100, 2), \"%\\n\\n\"))\n\n\nBaseline (2009 National Average Rent-to-Income Ratio): 19.4 %\n\n\nCode\n# Print the first few rows of the final standardized data\nprint(head(rent_burden_analysis))\n\n\n# A tibble: 6 × 7\n  GEOID NAME      year median_income median_rent raw_ratio_pct rent_burden_index\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1 10140 Aberdee…  2009         36345         650          21.5             1.11 \n2 10180 Abilene…  2009         42931         712          19.9             1.03 \n3 10300 Adrian,…  2009         45640         645          17.0             0.874\n4 10380 Aguadil…  2009         13470         363          32.3             1.67 \n5 10420 Akron, …  2009         47482         723          18.3             0.942\n6 10500 Albany,…  2009         36218         624          20.7             1.07"
  },
  {
    "objectID": "mp02.html#tables-to-introduce-rent-burden",
    "href": "mp02.html#tables-to-introduce-rent-burden",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "2: 3 Tables to introduce rent burden:",
    "text": "2: 3 Tables to introduce rent burden:\n\n\nCode\nlibrary(DT)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\n\n# --- 1. Data Loading and RBI Metric Calculation ---\n\n# Load the base data\nincome_data &lt;- read_csv(\"B19013_001_cbsa_2009_2023.csv\")\nrent_data &lt;- read_csv(\"B25064_001_cbsa_2009_2023.csv\")\n\n# Calculate Raw Ratio\nrent_burden_data &lt;- income_data %&gt;%\n  select(GEOID, NAME, year, median_income = B19013_001) %&gt;%\n  inner_join(\n    rent_data %&gt;%\n      select(GEOID, year, median_rent = B25064_001),\n    by = c(\"GEOID\", \"year\")\n  ) %&gt;%\n  mutate(\n    # Raw Ratio: Annual Rent / Annual Income\n    raw_ratio = (median_rent * 12) / median_income\n  )\n\n# Calculate Baseline (2009 National Average)\nbaseline_2009_avg_ratio &lt;- rent_burden_data %&gt;%\n  filter(year == 2009) %&gt;%\n  summarise(avg_ratio_2009 = mean(raw_ratio, na.rm = TRUE)) %&gt;%\n  pull(avg_ratio_2009)\n\n# Calculate Rent Burden Index (RBI)\nrent_burden_analysis &lt;- rent_burden_data %&gt;%\n  mutate(\n    # RBI: Times the 2009 National Average Rent Burden\n    rent_burden_index = raw_ratio / baseline_2009_avg_ratio,\n    raw_ratio_pct = raw_ratio * 100\n  ) %&gt;%\n  select(GEOID, NAME, year, raw_ratio_pct, rent_burden_index)\n\nTARGET_CBSA_NAME &lt;- \"Buffalo-Niagara Falls, NY Metro Area\"\nlatest_year &lt;- max(rent_burden_analysis$year)\n\n\n\nTable 1: Time Evolution for Buffalo-Niagara Falls, NY Metro Area\n\n\nCode\nbuffalo_table &lt;- rent_burden_analysis %&gt;%\n  filter(NAME == TARGET_CBSA_NAME) %&gt;%\n  select(year, 'Raw Ratio (%)' = raw_ratio_pct, 'Rent Burden Index (RBI)' = rent_burden_index) %&gt;%\n  mutate(\n    'Raw Ratio (%)' = paste0(round(`Raw Ratio (%)`, 2), '%'),\n    'Rent Burden Index (RBI)' = round(`Rent Burden Index (RBI)`, 3)\n  )\n\nDT::datatable(\n  buffalo_table,\n  options = list(\n    dom = 't', # Show table only\n    columnDefs = list(list(className = 'dt-center', targets = '_all'))\n  ),\n)\n\n\n\n\n\n\n\n\nTable 2: Top 5 Highest and Lowest Rent Burden (Latest Year: 2023)\n\n\nCode\nlatest_year_df &lt;- rent_burden_analysis %&gt;%\n  filter(year == latest_year)\n\n# Find the highest and lowest RBI\nhighest_burden &lt;- latest_year_df %&gt;%\n  arrange(desc(rent_burden_index)) %&gt;%\n  slice_head(n = 5)\n\nlowest_burden &lt;- latest_year_df %&gt;%\n  arrange(rent_burden_index) %&gt;%\n  slice_head(n = 5)\n\ntop_bottom_df &lt;- bind_rows(highest_burden, lowest_burden) %&gt;%\n  select('Metropolitan Area' = NAME, 'Raw Ratio (%)' = raw_ratio_pct, 'Rent Burden Index (RBI)' = rent_burden_index) %&gt;%\n  mutate(\n    'Raw Ratio (%)' = paste0(round(`Raw Ratio (%)`, 2), '%'),\n    'Rent Burden Index (RBI)' = round(`Rent Burden Index (RBI)`, 3)\n  )\n\nDT::datatable(\n  top_bottom_df,\n  options = list(\n    dom = 't',\n    # JavaScript to highlight the top 5 (Highest) in yellow and bottom 5 (Lowest) in blue\n    rowCallback = DT::JS(\n      \"function(row, data, index) {\n        if (index &lt; 5) {\n          $('td', row).css('background-color', 'rgba(255, 255, 0, 0.4)'); // Yellow for highest\n        } else {\n          $('td', row).css('background-color', 'rgba(173, 216, 230, 0.4)'); // Light blue for lowest\n        }\n      }\"\n    ),\n    columnDefs = list(list(className = 'dt-center', targets = '_all'))\n  ),\n)\n\n\n\n\n\n\n\n\nTable 3: Full Rent Burden Analysis (All CBSAs, 2009-2023)\n\n\nCode\nfull_analysis_table &lt;- rent_burden_analysis %&gt;%\n  select(year, 'Metropolitan Area' = NAME, 'Raw Ratio (%)' = raw_ratio_pct, 'Rent Burden Index (RBI)' = rent_burden_index) %&gt;%\n  mutate(\n    'Raw Ratio (%)' = paste0(round(`Raw Ratio (%)`, 2), '%'),\n    'Rent Burden Index (RBI)' = round(`Rent Burden Index (RBI)`, 3)\n  )\n\nDT::datatable(\n  full_analysis_table,\n  options = list(\n    pageLength = 10,\n    columnDefs = list(list(className = 'dt-center', targets = '_all'))\n  ),\n)"
  },
  {
    "objectID": "mp02.html#creating-measure-of-housing-growth-through-joining-together-population-and-permits",
    "href": "mp02.html#creating-measure-of-housing-growth-through-joining-together-population-and-permits",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Creating Measure of housing growth through joining together Population and Permits:",
    "text": "Creating Measure of housing growth through joining together Population and Permits:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\n\n# --- Load Data ---\npop_data &lt;- read_csv(\"B01003_001_cbsa_2009_2023.csv\")\npermits_data &lt;- read_csv(\"housing_units_2009_2023.csv\")\n\n# Ensure column names are consistent/correct\npop_data &lt;- pop_data %&gt;%\n  rename(total_population = B01003_001) %&gt;%\n  select(GEOID, NAME, year, total_population)\n\npermits_data &lt;- permits_data %&gt;%\n  rename(GEOID = CBSA, new_permits = new_housing_units_permitted) %&gt;%\n  select(GEOID, year, new_permits)\n\n# --- Join Tables and Calculate 5-Year Population Growth ---\ngrowth_data &lt;- pop_data %&gt;%\n  inner_join(permits_data, by = c(\"GEOID\", \"year\")) %&gt;%\n  arrange(GEOID, year) %&gt;%\n  group_by(GEOID) %&gt;%\n  mutate(\n    # 5-year Population Growth: P(t) - P(t-5)\n    pop_5yr_ago = lag(total_population, n = 5, default = NA),\n    pop_growth_5yr = total_population - pop_5yr_ago\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt;= 2014)\n\n\n# ----------------------------------------------------------------------\n# --- 1. 'Instantaneous' Measure of Housing Growth (HGI) ---\n# ----------------------------------------------------------------------\n\n# Raw Metric: New Permits per 1,000 residents (HGI_raw)\nHGI_data &lt;- growth_data %&gt;%\n  mutate(HGI_raw = (new_permits / total_population) * 1000)\n\n# Baseline: National Average HGI_raw in 2014\nHGI_baseline_2014 &lt;- HGI_data %&gt;%\n  filter(year == 2014) %&gt;%\n  summarise(avg_HGI_raw_2014 = mean(HGI_raw, na.rm = TRUE)) %&gt;%\n  pull(avg_HGI_raw_2014)\n\n# Standardize: HGI_Index = HGI_raw / HGI_baseline_2014 (\"times the 2014 national average\")\nHGI_data &lt;- HGI_data %&gt;%\n  mutate(HGI_Index = HGI_raw / HGI_baseline_2014) %&gt;%\n  select(GEOID, NAME, year, HGI_raw, HGI_Index)\n\n\n# ----------------------------------------------------------------------\n# --- 2. 'Rate-Based' Measure of Housing Growth (HGR) ---\n# ----------------------------------------------------------------------\n\n# Raw Metric: New Permits per unit of 5-year Population Growth (HGR_raw)\nHGR_data &lt;- growth_data %&gt;%\n  filter(!is.na(pop_growth_5yr)) %&gt;%\n  mutate(\n    # Add +1 to the denominator to handle cases where population growth is zero/near-zero\n    HGR_raw = new_permits / (pop_growth_5yr + 1)\n  )\n\n# Baseline: National Average HGR_raw in 2014\nHGR_baseline_2014 &lt;- HGR_data %&gt;%\n  filter(year == 2014) %&gt;%\n  summarise(avg_HGR_raw_2014 = mean(HGR_raw[is.finite(HGR_raw)], na.rm = TRUE)) %&gt;%\n  pull(avg_HGR_raw_2014)\n\n# Standardize: HGR_Index = HGR_raw / HGR_baseline_2014 (\"times the 2014 national average\")\nHGR_data &lt;- HGR_data %&gt;%\n  mutate(HGR_raw = ifelse(is.finite(HGR_raw), HGR_raw, NA)) %&gt;%\n  mutate(\n    HGR_Index = HGR_raw / HGR_baseline_2014\n  ) %&gt;%\n  select(GEOID, NAME, year, HGR_raw, HGR_Index)\n\n\n# --- Final Join and Output ---\nhousing_growth_analysis &lt;- HGI_data %&gt;%\n  inner_join(HGR_data, by = c(\"GEOID\", \"NAME\", \"year\")) %&gt;%\n  filter(!is.na(HGI_Index) & !is.na(HGR_Index))\n\n# Save the final data to CSV\nhousing_growth_analysis %&gt;% write_csv(\"housing_growth_analysis.csv\")"
  },
  {
    "objectID": "mp02.html#tables-that-identify-cbsas-that-score-highlow-on-selected-metrics",
    "href": "mp02.html#tables-that-identify-cbsas-that-score-highlow-on-selected-metrics",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Tables that identify CBSAs that score high/low on selected metrics",
    "text": "Tables that identify CBSAs that score high/low on selected metrics\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(DT)\n\n# --- 1. Data Recreation and HGS Calculation ---\n\n# Load and prepare data (re-run of previous step to ensure data completeness)\npop_data &lt;- read_csv(\"B01003_001_cbsa_2009_2023.csv\") %&gt;%\n  rename(total_population = B01003_001) %&gt;%\n  select(GEOID, NAME, year, total_population)\n\npermits_data &lt;- read_csv(\"housing_units_2009_2023.csv\") %&gt;%\n  rename(GEOID = CBSA, new_permits = new_housing_units_permitted) %&gt;%\n  select(GEOID, year, new_permits)\n\ngrowth_data &lt;- pop_data %&gt;%\n  inner_join(permits_data, by = c(\"GEOID\", \"year\")) %&gt;%\n  arrange(GEOID, year) %&gt;%\n  group_by(GEOID) %&gt;%\n  mutate(\n    pop_5yr_ago = lag(total_population, n = 5, default = NA),\n    pop_growth_5yr = total_population - pop_5yr_ago\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt;= 2014)\n\n# HGI Index Calculation (Permits per 1k Residents)\nHGI_data &lt;- growth_data %&gt;% mutate(HGI_raw = (new_permits / total_population) * 1000)\nHGI_baseline_2014 &lt;- HGI_data %&gt;% filter(year == 2014) %&gt;% summarise(avg = mean(HGI_raw, na.rm = TRUE)) %&gt;% pull(avg)\nHGI_data &lt;- HGI_data %&gt;% mutate(HGI_Index = HGI_raw / HGI_baseline_2014) %&gt;% select(GEOID, NAME, year, HGI_Index)\n\n# HGR Index Calculation (Permits per 5-year Pop Growth + 1)\nHGR_data &lt;- growth_data %&gt;%\n  filter(!is.na(pop_growth_5yr)) %&gt;%\n  mutate(HGR_raw = new_permits / (pop_growth_5yr + 1))\nHGR_baseline_2014 &lt;- HGR_data %&gt;% filter(year == 2014) %&gt;% summarise(avg = mean(HGR_raw[is.finite(HGR_raw)], na.rm = TRUE)) %&gt;% pull(avg)\nHGR_data &lt;- HGR_data %&gt;% mutate(HGR_raw = ifelse(is.finite(HGR_raw), HGR_raw, NA), HGR_Index = HGR_raw / HGR_baseline_2014) %&gt;% select(GEOID, NAME, year, HGR_Index)\n\n# Final Housing Growth Analysis and HGS\nhousing_growth_analysis &lt;- HGI_data %&gt;%\n  inner_join(HGR_data, by = c(\"GEOID\", \"NAME\", \"year\")) %&gt;%\n  filter(!is.na(HGI_Index) & !is.na(HGR_Index)) %&gt;%\n  mutate(Housing_Growth_Score = (HGI_Index + HGR_Index) / 2)\n\n# --- 2. Data Preparation for Tables (Latest Year: 2023) ---\n\nlatest_year &lt;- max(housing_growth_analysis$year)\nlatest_year_df &lt;- housing_growth_analysis %&gt;%\n  filter(year == latest_year) %&gt;%\n  select(NAME, HGI_Index, HGR_Index, Housing_Growth_Score)\n\n# --- Define Subsets for Top/Bottom 5 ---\nget_top_bottom &lt;- function(df, metric, n=5) {\n  df %&gt;%\n    arrange(desc({{metric}})) %&gt;%\n    slice_head(n = n) %&gt;%\n    bind_rows(\n      df %&gt;%\n        arrange({{metric}}) %&gt;%\n        slice_head(n = n)\n    )\n}\n\n# HGI Top/Bottom Table\nHGI_top_bottom &lt;- get_top_bottom(latest_year_df, HGI_Index) %&gt;%\n  mutate(Category = c(rep(\"Highest HGI\", 5), rep(\"Lowest HGI\", 5))) %&gt;%\n  select(Category, 'Metropolitan Area' = NAME, 'HGI Index' = HGI_Index, 'HGR Index' = HGR_Index)\n\n# HGR Top/Bottom Table\nHGR_top_bottom &lt;- get_top_bottom(latest_year_df, HGR_Index) %&gt;%\n  mutate(Category = c(rep(\"Highest HGR\", 5), rep(\"Lowest HGR\", 5))) %&gt;%\n  select(Category, 'Metropolitan Area' = NAME, 'HGI Index' = HGI_Index, 'HGR Index' = HGR_Index)\n\n# HGS Top/Bottom Table\nHGS_top_bottom &lt;- get_top_bottom(latest_year_df, Housing_Growth_Score) %&gt;%\n  mutate(Category = c(rep(\"Highest HGS\", 5), rep(\"Lowest HGS\", 5))) %&gt;%\n  select(Category, 'Metropolitan Area' = NAME, 'HGS' = Housing_Growth_Score, 'HGI Index' = HGI_Index, 'HGR Index' = HGR_Index)\n\n# --- 3. DT Table Generation ---\n\n# JavaScript callback for highlighting rows (Top 5 Yellow, Bottom 5 Blue)\njs_callback &lt;- DT::JS(\n  \"function(row, data, index) {\n    if (index &lt; 5) {\n      $('td', row).css('background-color', 'rgba(255, 255, 0, 0.4)'); // Yellow for highest\n    } else {\n      $('td', row).css('background-color', 'rgba(173, 216, 230, 0.4)'); // Light blue for lowest\n    }\n  }\"\n)\n\ndt_options &lt;- list(\n    dom = 't',\n    rowCallback = js_callback,\n    columnDefs = list(list(className = 'dt-center', targets = '_all'))\n)\n\n# Function to display DT table\ndisplay_dt &lt;- function(data, title) {\n    data_rounded &lt;- data %&gt;% mutate(across(where(is.numeric), ~ round(.x, 3)))\n    DT::datatable(\n        data_rounded,\n        options = dt_options,\n        caption = paste0(title, \" in \", latest_year, \". Top 5 (Yellow), Bottom 5 (Blue).\")\n    )\n}\n\n\n\nTable 1: Top 5 Highest and Lowest Housing Growth Areas based on HGI Index\n\n\nCode\ndisplay_dt(HGI_top_bottom, \"Top 5 Highest and Lowest Housing Growth (Permits per 1k Residents - HGI)\")\n\n\n\n\n\n\n\n\nTable 2: Top 5 Highest and Lowest Housing Growth Areas based on HGR Index\n\n\nCode\ndisplay_dt(HGR_top_bottom, \"Top 5 Highest and Lowest Housing Growth (Permits vs. 5-Year Pop Growth - HGR)\")\n\n\n\n\n\n\n\n\nTable 3: Top 5 Highest and Lowest Housing Growth Areas based on HGS Index\n\n\nCode\ndisplay_dt(HGS_top_bottom, \"Top 5 Highest and Lowest Composite Housing Growth Score (HGS)\")"
  },
  {
    "objectID": "mp02.html#preparation-and-calculation",
    "href": "mp02.html#preparation-and-calculation",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Preparation and Calculation:",
    "text": "Preparation and Calculation:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(scales) \n\n# --- 1. Load and Prepare Core Data ---\npop_data &lt;- read_csv(\"B01003_001_cbsa_2009_2023.csv\")\npop_data &lt;- pop_data %&gt;% rename(total_population = B01003_001) %&gt;% select(GEOID, NAME, year, total_population)\n\nincome_data &lt;- read_csv(\"B19013_001_cbsa_2009_2023.csv\")\nincome_data &lt;- income_data %&gt;% rename(med_income = B19013_001) %&gt;% select(GEOID, year, med_income)\n\nrent_data &lt;- read_csv(\"B25064_001_cbsa_2009_2023.csv\")\nrent_data &lt;- rent_data %&gt;% rename(med_rent = B25064_001) %&gt;% select(GEOID, year, med_rent)\n\npermits_data &lt;- read_csv(\"housing_units_2009_2023.csv\")\npermits_data &lt;- permits_data %&gt;% rename(GEOID = CBSA, new_permits = new_housing_units_permitted) %&gt;% select(GEOID, year, new_permits)\n\n\n# --- 2. Calculate Rent Burden Index (RBI) ---\nrent_burden_data &lt;- inner_join(rent_data, income_data, by = c(\"GEOID\", \"year\"))\nrent_burden_data &lt;- rent_burden_data %&gt;% mutate(RB_raw = (med_rent * 12) / med_income) %&gt;% filter(RB_raw &gt; 0 & is.finite(RB_raw))\nRB_baseline_2009 &lt;- rent_burden_data %&gt;% filter(year == 2009) %&gt;% summarise(avg = mean(RB_raw, na.rm = TRUE)) %&gt;% pull(avg)\nrent_burden_data &lt;- rent_burden_data %&gt;% mutate(RBI_Index = RB_raw / RB_baseline_2009) %&gt;% select(GEOID, year, RBI_Index)\n\n\n# --- 3. Calculate Housing Growth Score (HGS) ---\ngrowth_data &lt;- inner_join(pop_data, permits_data, by = c(\"GEOID\", \"year\"))\ngrowth_data &lt;- growth_data %&gt;% arrange(GEOID, year) %&gt;% group_by(GEOID) %&gt;%\n  mutate(pop_5yr_ago = lag(total_population, n = 5, default = NA), pop_growth_5yr = total_population - pop_5yr_ago) %&gt;%\n  ungroup() %&gt;% filter(year &gt;= 2014)\n\nHGI_data &lt;- growth_data %&gt;% mutate(HGI_raw = (new_permits / total_population) * 1000)\nHGI_baseline_2014 &lt;- HGI_data %&gt;% filter(year == 2014) %&gt;% summarise(avg = mean(HGI_raw, na.rm = TRUE)) %&gt;% pull(avg)\nHGI_data &lt;- HGI_data %&gt;% mutate(HGI_Index = HGI_raw / HGI_baseline_2014) %&gt;% select(GEOID, year, HGI_Index)\n\nHGR_data &lt;- growth_data %&gt;% filter(!is.na(pop_growth_5yr)) %&gt;% mutate(HGR_raw = new_permits / (pop_growth_5yr + 1))\nHGR_baseline_2014 &lt;- HGR_data %&gt;% filter(year == 2014) %&gt;% summarise(avg = mean(HGR_raw[is.finite(HGR_raw)], na.rm = TRUE)) %&gt;% pull(avg)\nHGR_data &lt;- HGR_data %&gt;% mutate(HGR_raw = ifelse(is.finite(HGR_raw), HGR_raw, NA), HGR_Index = HGR_raw / HGR_baseline_2014) %&gt;% select(GEOID, year, HGR_Index)\n\nhousing_growth_analysis &lt;- HGI_data %&gt;% inner_join(HGR_data, by = c(\"GEOID\", \"year\")) %&gt;% filter(!is.na(HGI_Index) & !is.na(HGR_Index)) %&gt;% mutate(Housing_Growth_Score = (HGI_Index + HGR_Index) / 2)\n\n# --- 4. Merge All Metrics & Calculate YIMBY Criteria Variables (2009-2023) ---\nfull_analysis_data &lt;- inner_join(pop_data, rent_burden_data, by = c(\"GEOID\", \"year\"))\nfull_analysis_data &lt;- full_analysis_data %&gt;% inner_join(housing_growth_analysis, by = c(\"GEOID\", \"year\")) %&gt;% arrange(GEOID, year)\n\nstart_year &lt;- min(full_analysis_data$year)\nend_year &lt;- max(full_analysis_data$year)\n\nyimby_criteria &lt;- full_analysis_data %&gt;% filter(year == start_year | year == end_year | year &gt;= 2014) %&gt;%\n  group_by(GEOID, NAME) %&gt;%\n  summarise(\n    initial_RB_Index = RBI_Index[year == start_year],\n    RB_Index_change = RBI_Index[year == end_year] - RBI_Index[year == start_year],\n    pop_growth = total_population[year == end_year] - total_population[year == start_year],\n    avg_HGS_2014_2023 = mean(Housing_Growth_Score[year &gt;= 2014], na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  filter(!is.na(initial_RB_Index) & !is.na(RB_Index_change) & !is.na(pop_growth) & !is.na(avg_HGS_2014_2023))\n\nnational_avg_HGS_2014_2023 &lt;- mean(yimby_criteria$avg_HGS_2014_2023, na.rm = TRUE)\n\nyimby_cbsas &lt;- yimby_criteria %&gt;%\n  mutate(is_yimby = (initial_RB_Index &gt; 1.0) & (RB_Index_change &lt; 0) & (pop_growth &gt; 0) & (avg_HGS_2014_2023 &gt; national_avg_HGS_2014_2023)) %&gt;%\n  filter(is_yimby) %&gt;%\n  arrange(desc(avg_HGS_2014_2023))\n\n# Prepare data for plotting\nplot1_data &lt;- yimby_criteria %&gt;% mutate(is_yimby_candidate = (initial_RB_Index &gt; 1.0) & (pop_growth &gt; 0))\ntop_yimby_names &lt;- head(yimby_cbsas, 5)$NAME\nplot1_data &lt;- plot1_data %&gt;% mutate(label = ifelse(NAME %in% top_yimby_names, NAME, \"\"))\n\nplot2_data &lt;- full_analysis_data %&gt;%\n  filter(NAME %in% yimby_cbsas$NAME) %&gt;%\n  select(NAME, year, RBI_Index) %&gt;%\n  arrange(NAME, year)"
  },
  {
    "objectID": "mp02.html#visualization-1-scatter-plot-hgs-vs.-change-in-rent-burden",
    "href": "mp02.html#visualization-1-scatter-plot-hgs-vs.-change-in-rent-burden",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Visualization 1: Scatter Plot (HGS vs. Change in Rent Burden)",
    "text": "Visualization 1: Scatter Plot (HGS vs. Change in Rent Burden)\n\n\nCode\np1 &lt;- ggplot(plot1_data, aes(x = RB_Index_change, y = avg_HGS_2014_2023)) +\n  geom_line(aes(color = NAME), linewidth = 0.5) +\n  geom_point(aes(color = NAME)) +\n  geom_hline(yintercept = 1.0, linetype = \"dashed\", color = \"gray50\") +\n  scale_x_continuous(breaks = seq(start_year, end_year, by = 2)) +\n  labs(\n    title = paste0(\"Housing Growth vs. Change in Rent Burden Index (\", start_year, \" - \", end_year, \")\"),\n    subtitle = \"YIMBY Success: Initial RBI &gt; 1.0, Pop Growth &gt; 0, HGS &gt; National Avg, and RBI Change &lt; 0 (Top-Left, above gray line).\",\n    x = \"Change in Rent Burden Index (RBI) (Decrease is better, negative values mean success)\",\n    y = \"Average Housing Growth Score (HGS) (Higher is better)\",\n    caption = paste0(\"HGS Average Period: 2014-\", end_year, \". Horizontal Line: National Average HGS (\", round(national_avg_HGS_2014_2023, 2), \").\")\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", plot.caption = element_text(size = 2))\n\nprint(p1)"
  },
  {
    "objectID": "mp02.html#visualization-2-time-series-plot-rbi-trend-for-yimby-success-cbsas",
    "href": "mp02.html#visualization-2-time-series-plot-rbi-trend-for-yimby-success-cbsas",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Visualization 2: Time Series Plot (RBI Trend for YIMBY Success CBSAs)",
    "text": "Visualization 2: Time Series Plot (RBI Trend for YIMBY Success CBSAs)\n\n\nCode\np2 &lt;- ggplot(plot2_data, aes(x = year, y = RBI_Index, group = NAME)) +\n  geom_line(aes(color = NAME), linewidth = 0.5) +\n  geom_point(aes(color = NAME)) +\n  geom_hline(yintercept = 1.0, linetype = \"dashed\", color = \"gray50\") +\n  scale_x_continuous(breaks = seq(start_year, end_year, by = 2)) +\n  labs(\n    title = \"Rent Burden Index (RBI) Trend for Identified YIMBY Success CBSAs\",\n    subtitle = \"RBI is standardized to the 2009 National Average (RBI=1.0 dashed line).\",\n    x = \"Year\",\n    y = \"Rent Burden Index (RBI)\",\n    color = \"Metropolitan Area\"\n  ) +\n  theme_minimal() +\n  theme(\n    # 1. Move the legend to the bottom\n    legend.position = \"bottom\",\n    # 2. Make the legend key items tighter\n    legend.key.size = unit(0.00001, \"cm\"),\n    legend.text = element_text(size = 9)\n  ) +\n  \n  guides(color = guide_legend(ncol = 2, byrow = TRUE))\n\nprint(p2)"
  },
  {
    "objectID": "mp02.html#action-objective-securing-sponsors-and-core-support",
    "href": "mp02.html#action-objective-securing-sponsors-and-core-support",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Action Objective: Securing Sponsors and Core Support",
    "text": "Action Objective: Securing Sponsors and Core Support\nThe Pro-Growth Housing Incentives Act is a national grant program designed to reward and incentivize local zoning reform, boost housing supply, and lowering the Rent Burden Index (RBI) nationwide, with the end goal being to make housing affordable long term in all major CBSAs nationwide."
  },
  {
    "objectID": "mp02.html#congressional-sponsorship-strategy",
    "href": "mp02.html#congressional-sponsorship-strategy",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Congressional Sponsorship Strategy",
    "text": "Congressional Sponsorship Strategy\nTo maximize support, we must pair a sponsor whose success is proof of the program’s theory with a co-sponsor whose crisis underscores its necessity in CBSAs that demonstrate model strengths and shortcomings that this bill aims to solve.\n\nPrimary Sponsoring Area (Pro Growth CBSA): 26420, Houston-Sugar Land-Baytown, TX\nHouston demonstrates the power of pro-supply policies. Their Rent Burden Index (RBI) decreased (from 1.05 to 1.04) despite massive population growth, thanks to a high Housing Growth Score (HGS) of 1.56 (56% above the national average). The Act will allow Houston to scale its success and continue growing with ample housing to provide.\n\n\nCo-sponsoring Area (Advocate CBSA this bill aims to solve): 41940, San Jose-Sunnyvale-Santa Clara, CA\nSan Jose is the prime example of the crisis this bill solves. The city’s RBI has risen dramatically from 2009-2023 (from 1.52 to 1.63), coupled with a low HGS of just 0.58. The Act offers the necessary resources and incentives to break decades of systemic supply failure in Silicon Valley’s most populated area."
  },
  {
    "objectID": "mp02.html#building-momentum-through-securing-support",
    "href": "mp02.html#building-momentum-through-securing-support",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Building Momentum through Securing Support",
    "text": "Building Momentum through Securing Support\nSecuring support from local labor and trade unions is universally critical for passage in CBSAs nationwide. This bill specifically benefits two vocally engaged groups in society by focusing on jobs and cost relief that can flourish under this act.\n\nGroup 1: Construction & Building Unions\nThis Act provides funding directly tied to local permit reform and increased housing unit construction. This means immediate, stable, high-wage union and trade jobs in both metros and eventually nationwide. For San Jose, it means jump-starting a lagging housing sector. For Houston, it means federal support to maintain their current building boom.\n\n\nGroup 2: Service Industry Workers\nFor low and mid-wage workers-often the first victims of rising housing costs—The Act delivers meaningful economic relief amidst the changing landscape across America. By increasing housing supply (as seen in Houston’s stabilized RBI), we drive down the largest monthly expense-Rent. This effectively acts as a raise or tax cut, leaving service workers with more disposable income, which, in turn, boosts local businesses like restaurants and retail (where they also may work, thus providing a stronger sense of job security for those holding these positions)."
  },
  {
    "objectID": "mp02.html#targeting-funding-efficiently-across-cbsas",
    "href": "mp02.html#targeting-funding-efficiently-across-cbsas",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Targeting Funding Efficiently Across CBSAs",
    "text": "Targeting Funding Efficiently Across CBSAs\nTo define “YIMBY” success and in turn allocate funds in an appropriate manner proportionately nationwide, This Act uses two symbiotic indices:\n\nRent Burden Index (RBI):\nWhat it measures: Housing affordability stress, or the arbitrary threshold needed to live comfortably. It calculates the average annual rent relative to the average income in a given area.\nWhat is ‘Good’: A city is succeeding if its RBI is decreasing over time. This means housing is becoming more affordable relative to local wages, proving supply is meeting demand.\n\n\nHousing Growth Score (HGS):\nWhat it measures: The effectiveness and ambition of new housing construction in a given area. It combines raw permits per capita (Housing Growth Intensity) with permits relative to population change (Housing Growth Responsiveness).\nWhat is ‘Good’: A city is succeeding if its HGS is above the national average. This proves the city isn’t just growing, but is proactively changing zoning restrictions and ensuring future supply meets population need."
  },
  {
    "objectID": "mp02.html#conclusion",
    "href": "mp02.html#conclusion",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Conclusion",
    "text": "Conclusion\nThis plans to reward cities that use the Housing Growth Score System to lower the Rent Burden Index that plagues their populus through providing assistance and incentives to cities that have a high RBI and a low HGS, giving them a clear path toward federal grant eligibility and eventually affordable and available housing for all."
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini Project 1 – Netflix Top 10",
    "section": "",
    "text": "Introduction to the project\nThis project has 2 datasets, One for the top ten shows of each country tracked (Country_Top 10) and the Top ten overall shows Globally (Global_Top_10), the analyses below help us interpret various facets about the data\n\n\nLoading the Packages and the Datasets\n\n\nCode\n# Load required packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(readr)\n  library(dplyr) \n  library(knitr)\n  library(DT)\n  library(stringr)\n  library(glue)\n  library(lubridate)\n})\n\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\n\n# Importing of Datasets for Analysis\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n\nRows: 8840 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME)\n\n\nRows: 411760 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Read - tell read_tsv to treat \"N/A\" as NA\n\nGLOBAL_TOP_10  &lt;- readr::read_tsv(GLOBAL_TOP_10_FILENAME,  na = c(\"N/A\"))\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 8840 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nCOUNTRY_TOP_10 &lt;- readr::read_tsv(COUNTRY_TOP_10_FILENAME, na = c(\"N/A\"))\n\n\nRows: 411760 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nTask 1: Data Acqusition; a quick look of the datasets we are working with\n\n\nCode\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(season_title = na_if(season_title, \"N/A\"))\n\nCOUNTRY_TOP_10 &lt;- COUNTRY_TOP_10 %&gt;%\n  mutate(season_title = na_if(season_title, \"N/A\"))\n\n# To Confirm\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,840\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; NA, NA, NA, \"aka Charlie Sheen: Season 1\", …\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n\n\nCode\nglimpse(COUNTRY_TOP_10)\n\n\nRows: 411,760\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Mule\", \"The Wrong Paris\", \"KPop Demon …\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Ma…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 1, 2, 14, 1, 1, 1, 2, 1, 5, 1, 2, 1, 7, 1, …\n\n\n\n\nTask 2: Data Cleaning\n\n\nCode\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(runtime_minutes = round(60 * runtime))\n\nstr(GLOBAL_TOP_10)\n\n\ntibble [8,840 × 10] (S3: tbl_df/tbl/data.frame)\n $ week                      : Date[1:8840], format: \"2025-09-21\" \"2025-09-21\" ...\n $ category                  : chr [1:8840] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8840] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8840] \"The Wrong Paris\" \"KPop Demon Hunters\" \"Ice Road: Vengeance\" \"aka Charlie Sheen\" ...\n $ season_title              : chr [1:8840] NA NA NA \"aka Charlie Sheen: Season 1\" ...\n $ weekly_hours_viewed       : num [1:8840] 38900000 35400000 14400000 21800000 10900000 7100000 7800000 6300000 5800000 4000000 ...\n $ runtime                   : num [1:8840] 1.78 1.67 1.88 3.03 1.7 ...\n $ weekly_views              : num [1:8840] 21800000 21200000 7600000 7200000 6400000 4500000 3900000 3400000 3100000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8840] 2 14 1 2 2 4 4 1 1 1 ...\n $ runtime_minutes           : num [1:8840] 107 100 113 182 102 95 120 110 114 86 ...\n\n\nCode\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,840\nColumns: 10\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; NA, NA, NA, \"aka Charlie Sheen: Season 1\", …\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n$ runtime_minutes            &lt;dbl&gt; 107, 100, 113, 182, 102, 95, 120, 110, 114,…\n\n\n\n\nTask 3: Data Import & Interpretation\n\n\nCode\nCOUNTRY_TOP_10\n\n\n# A tibble: 411,760 × 8\n   country_name country_iso2 week       category weekly_rank show_title         \n   &lt;chr&gt;        &lt;chr&gt;        &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;              \n 1 Argentina    AR           2025-09-21 Films              1 The Mule           \n 2 Argentina    AR           2025-09-21 Films              2 The Wrong Paris    \n 3 Argentina    AR           2025-09-21 Films              3 KPop Demon Hunters \n 4 Argentina    AR           2025-09-21 Films              4 She Said Maybe     \n 5 Argentina    AR           2025-09-21 Films              5 War Dogs           \n 6 Argentina    AR           2025-09-21 Films              6 Sonic the Hedgehog…\n 7 Argentina    AR           2025-09-21 Films              7 Ocean's 8          \n 8 Argentina    AR           2025-09-21 Films              8 Into the Wild      \n 9 Argentina    AR           2025-09-21 Films              9 Fall for Me        \n10 Argentina    AR           2025-09-21 Films             10 A Bright Lawyer    \n# ℹ 411,750 more rows\n# ℹ 2 more variables: season_title &lt;chr&gt;, cumulative_weeks_in_top_10 &lt;dbl&gt;\n\n\nCode\nGLOBAL_TOP_10\n\n\n# A tibble: 8,840 × 10\n   week       category   weekly_rank show_title season_title weekly_hours_viewed\n   &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                      &lt;dbl&gt;\n 1 2025-09-21 Films (En…           1 The Wrong… &lt;NA&gt;                    38900000\n 2 2025-09-21 Films (En…           2 KPop Demo… &lt;NA&gt;                    35400000\n 3 2025-09-21 Films (En…           3 Ice Road:… &lt;NA&gt;                    14400000\n 4 2025-09-21 Films (En…           4 aka Charl… aka Charlie…            21800000\n 5 2025-09-21 Films (En…           5 The Mule   &lt;NA&gt;                    10900000\n 6 2025-09-21 Films (En…           6 Unknown N… &lt;NA&gt;                     7100000\n 7 2025-09-21 Films (En…           7 The Thurs… &lt;NA&gt;                     7800000\n 8 2025-09-21 Films (En…           8 Sonic the… &lt;NA&gt;                     6300000\n 9 2025-09-21 Films (En…           9 War Dogs   &lt;NA&gt;                     5800000\n10 2025-09-21 Films (En…          10 Terror Co… &lt;NA&gt;                     4000000\n# ℹ 8,830 more rows\n# ℹ 4 more variables: runtime &lt;dbl&gt;, weekly_views &lt;dbl&gt;,\n#   cumulative_weeks_in_top_10 &lt;dbl&gt;, runtime_minutes &lt;dbl&gt;\n\n\nCode\n# Initial Data Exploration and Adjustments\n\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\nCode\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\n#Formating of Data\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    select(-season_title) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\n\n\nTask 4: Exploratory and Press Release Questions:\n\n\nCountries Netflix operates in:\n\n\nCode\nCOUNTRY_TOP_10 %&gt;% distinct(country_name) %&gt;% count()\n\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    94\n\n\n\n\nNon English Film with the most cumulative weeks in the global top ten:\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(category == \"Films (Non-English)\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(weeks_in_top10 = n_distinct(week), .groups = \"drop\") %&gt;%\n  arrange(desc(weeks_in_top10)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 2\n  show_title                     weeks_in_top10\n  &lt;chr&gt;                                   &lt;int&gt;\n1 All Quiet on the Western Front             23\n\n\n\n\nLongest Film in global top 10\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(str_detect(category, \"Films\")) %&gt;%\n  mutate(runtime_min = round(60 * runtime)) %&gt;%\n  arrange(desc(runtime_min)) %&gt;% slice(1) %&gt;%\n  select(show_title, runtime_min)\n\n\n# A tibble: 1 × 2\n  show_title                            runtime_min\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 Pushpa 2: The Rule (Reloaded Version)         224\n\n\n\n\nPrograms with most total hours per the four catagories\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  group_by(category, show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop_last\") %&gt;%\n  slice_max(total_hours, n = 1, with_ties = FALSE) %&gt;%\n  arrange(category)\n\n\n# A tibble: 4 × 3\n# Groups:   category [4]\n  category            show_title          total_hours\n  &lt;chr&gt;               &lt;chr&gt;                     &lt;dbl&gt;\n1 Films (English)     KPop Demon Hunters    559100000\n2 Films (Non-English) Society of the Snow   235900000\n3 TV (English)        Stranger Things      2967980000\n4 TV (Non-English)    Squid Game           5048300000\n\n\n\n\nTV Show with the Longest Run in a country’s Top 10:\n\n\nCode\nCOUNTRY_TOP_10 %&gt;%\n  group_by(country_name, show_title) %&gt;%\n  summarise(weeks = n_distinct(week), .groups = \"drop\") %&gt;%\n  arrange(desc(weeks)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 3\n  country_name show_title  weeks\n  &lt;chr&gt;        &lt;chr&gt;       &lt;int&gt;\n1 Pakistan     Money Heist   128\n\n\n\n\nCountry with fewer than 200 weeks recorded and the date it stopped:\n\n\nCode\n  COUNTRY_TOP_10 %&gt;%\n  group_by(country_name) %&gt;%\n  summarise(n_weeks = n_distinct(week),\n            last_week = max(week, na.rm = TRUE)) %&gt;%\n  filter(n_weeks &lt; 200)\n\n\n# A tibble: 1 × 3\n  country_name n_weeks last_week \n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;    \n1 Russia            35 2022-02-27\n\n\n\n\nTotal Viewership of Squid Game Across All Seasons:\n\n\nCode\n  GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, regex(\"Squid Game\", ignore_case=TRUE))) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  5310000000\n\n\n\n\nApprox Red Notice Views in 2021:\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(show_title == \"Red Notice\", year(week) == 2021) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1   396740000\n\n\n\n\nFilms that reached #1 but did not debut at the #1 spot (List below):\n\n\nCode\nus &lt;- COUNTRY_TOP_10 %&gt;% filter(country_name == \"United States\", str_detect(category, \"Film\"))\ndebuts &lt;- us %&gt;% arrange(week) %&gt;% group_by(show_title) %&gt;% slice(1) %&gt;% select(show_title, debut_rank = weekly_rank)\never_number1 &lt;- us %&gt;% group_by(show_title) %&gt;% summarise(ever1 = any(weekly_rank == 1))\nleft_join(debuts, ever_number1, by = \"show_title\") %&gt;%\n  filter(debut_rank &gt; 1, ever1)\n\n\n# A tibble: 45 × 3\n# Groups:   show_title [45]\n   show_title      debut_rank ever1\n   &lt;chr&gt;                &lt;dbl&gt; &lt;lgl&gt;\n 1 Aftermath                4 TRUE \n 2 American Made            9 TRUE \n 3 Blood Red Sky            5 TRUE \n 4 Bullet Train             2 TRUE \n 5 Day Shift                2 TRUE \n 6 Despicable Me 2          2 TRUE \n 7 Despicable Me 4          2 TRUE \n 8 Dog Gone                 4 TRUE \n 9 Don't Move               3 TRUE \n10 End of the Road          2 TRUE \n# ℹ 35 more rows\n\n\n\n\nTV show that hit the top 10 in the most countries in its debuting week (see below)\n\n\nCode\nCOUNTRY_TOP_10 %&gt;%\n  group_by(show_title, season_title, country_name) %&gt;%\n  summarise(first_week_in_country = min(week), .groups = \"drop\") %&gt;%\n  group_by(show_title, season_title) %&gt;%\n  summarise(first_debut_week = min(first_week_in_country),\n            n_countries_at_debut = n_distinct(country_name),\n            .groups = \"drop\") %&gt;%\n  arrange(desc(n_countries_at_debut)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 4\n  show_title         season_title          first_debut_week n_countries_at_debut\n  &lt;chr&gt;              &lt;chr&gt;                 &lt;date&gt;                          &lt;int&gt;\n1 All of Us Are Dead All of Us Are Dead: … 2022-01-30                         94\n\n\n\n\nPress Release 1: Stranger Things\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\n\n\nGLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Stranger Things\")) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  2967980000\n\n\nAfter four breakthrough seasons that redefined the culture surrounding original programming on Netflix, the critically acclaimed Stranger Things is gearing up for its fifth and final season at the end of 2025. In its most recent season, which was released between May and July 2022, the drama-filled horror show accumulated nearly 2 billion viewing hours on Netflix platforms, rising to the top of the ranks during that period for nearly half the year in total global viewership. Throughout its distinguished run, Stranger Things has accumulated approximatley 2,967,980,000 viewing hours since its release in July 2016, putting it at the top of Netflix’s original series and leading the English TV Category. Though in a close battle with Wednesday, another popular show in Netflix’s English market that was released in 2022, Stranger Things has nonetheless maintained its stance as perhaps Netflix’s most culturally significant English show over the last decade, and its fans are anxiously awaiting the conclusion of its mind bending and emotional story.\n\n\nPress Release 2: Indian Viewership\n\n\nCode\nlibrary(dplyr)\n\n  COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(total_weeks_in_top10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_weeks_in_top10)) %&gt;%\n  slice(1:5)\n\n\n# A tibble: 5 × 2\n  show_title                                        total_weeks_in_top10\n  &lt;chr&gt;                                                            &lt;dbl&gt;\n1 Money Heist                                                       1543\n2 Squid Game                                                        1153\n3 Wednesday                                                          694\n4 The Railway Men - The Untold Story Of Bhopal 1984                  465\n5 Khakee: The Bihar Chapter                                          435\n\n\nAs the second largest country in the world by population and the market Netflix truly wants to capitalize on, India stands out among other nations the Streaming App operates in as truly a unique story of success. Since our data began tracking in 2021, the Indian Market has seen over 1000 unique titles appear in global top ten charts, with many programs appearing despite having little to no presence elsewhere in the world, especially in Netflix’s largest market: The United States. After Observing the Trends in the given data, we can see that Netflix’s content diversity and viewership has increased significantly over time, with 39 different shows appearing each week in the top ten. With an estimated growing customer base of over 30 million, India is far outpacing most of the world in terms of subscriber growth and pure numbers. Recently, with international top shows such as Squid Game as well as India’s top domestic programs such as Dabba Cartel and Saare Jahan Se Accha: The Silent Guardians leading charts for multiple weeks, it seems that Netflix will only grow in this massive market with many producers signing exclusive rights with the platform, such as the Great Indian Kapil Show. Additionally, the Hindu Language shows of Netflix are also seeing a rise in viewership globally largely as a result of Netlfix’s success in India, just going to show how impactful and influential this market is to Netflix’s global success\n\n\nPress Release 3: The global dominace of Squid Game\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(lubridate)\n# Filter for Squid Game only\nsquid_data &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Squid Game\"))\n# 1. Overall stats\noverall_stats &lt;- squid_data %&gt;%\n  summarise(\n    total_weeks = n_distinct(week),\n    avg_weekly_viewers = mean(weekly_views, na.rm = TRUE)\n  )\n# 2. Season-by-season stats\nseason_stats &lt;- squid_data %&gt;%\n  group_by(season_title) %&gt;%\n  summarise(\n    avg_weekly_viewers = mean(weekly_views, na.rm = TRUE),\n    total_weeks = n_distinct(week)\n  ) %&gt;%\n  arrange(season_title)\n# 3. Combine seasons + overall into one table\nfinal_summary &lt;- bind_rows(\n  season_stats,\n  tibble(season_title = \"Overall\", \n         avg_weekly_viewers = overall_stats$avg_weekly_viewers, \n         total_weeks = overall_stats$total_weeks)\n)\n# Display nicely\nprint(final_summary)\n\n\n# A tibble: 5 × 3\n  season_title                        avg_weekly_viewers total_weeks\n  &lt;chr&gt;                                            &lt;dbl&gt;       &lt;int&gt;\n1 Squid Game: Season 1                          4416667.          32\n2 Squid Game: Season 2                         14392857.          14\n3 Squid Game: Season 3                         15822222.           9\n4 Squid Game: The Challenge: Season 1           8520000            5\n5 Overall                                      10987500           45\n\n\nSince its release in September 2021, Hwang Dong-hyuk’s game show Squid Game has taken over the world by storm, leading global charts and positioning itself as a global cultural phenomenon. Throughout its 3 seasons, the show has averaged a remarkable 11340000 weekly viewers when it has cracked the global top ten weekly rankings through an equally impressive 50 week presence in the global top ten over the last 5 years. Individually, while the the Korean game show’s first season was undisputedly the peak of the franchise, averaging 4,416,667 weekly views during its peak season, the following seasons were still potent leaders in global viewships, albeit in shorter periods due to a shorter hiatus span of 1 year compared to 3 and global pandemic restrictions easing over that 3 year span, which initially allowed many Netflix subscribers to watch their shows far more frequently as more were in front of their tvs. As a whole, Netflix’s peak viewership across all shows during the data collection period was during the later half of the global pandemic (Start of data - end of 2022), which no show benefitted more from than Squid Game, as it allowed it to become one of the world’s most distinguished shows when all the eyeballs in the world wanted action following a dormant 2 years of societal isolation."
  }
]