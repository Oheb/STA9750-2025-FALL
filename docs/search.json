[
  {
    "objectID": "MP01.html",
    "href": "MP01.html",
    "title": "Mini Project 1 – Netflix Top 10",
    "section": "",
    "text": "Introduction to the project\nThis project has 2 datasets, One for the top ten shows of each country tracked (Country_Top 10) and the Top ten overall shows Globally (Global_Top_10), the analyses below help us interpret various facets about the data\n\n\nLoading the Packages and the Datasets\n\n\nCode\n# Load required packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(readr)\n  library(dplyr) \n  library(knitr)\n  library(DT)\n  library(stringr)\n  library(glue)\n  library(lubridate)\n})\n\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\n\n# Importing of Datasets for Analysis\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n\nRows: 8840 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME)\n\n\nRows: 411760 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Read - tell read_tsv to treat \"N/A\" as NA\n\nGLOBAL_TOP_10  &lt;- readr::read_tsv(GLOBAL_TOP_10_FILENAME,  na = c(\"N/A\"))\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 8840 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nCOUNTRY_TOP_10 &lt;- readr::read_tsv(COUNTRY_TOP_10_FILENAME, na = c(\"N/A\"))\n\n\nRows: 411760 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nTask 1: Data Acqusition; a quick look of the datasets we are working with\n\n\nCode\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(season_title = na_if(season_title, \"N/A\"))\n\nCOUNTRY_TOP_10 &lt;- COUNTRY_TOP_10 %&gt;%\n  mutate(season_title = na_if(season_title, \"N/A\"))\n\n# To Confirm\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,840\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; NA, NA, NA, \"aka Charlie Sheen: Season 1\", …\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n\n\nCode\nglimpse(COUNTRY_TOP_10)\n\n\nRows: 411,760\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Mule\", \"The Wrong Paris\", \"KPop Demon …\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Ma…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 1, 2, 14, 1, 1, 1, 2, 1, 5, 1, 2, 1, 7, 1, …\n\n\n\n\nTask 2: Data Cleaning\n\n\nCode\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(runtime_minutes = round(60 * runtime))\n\nstr(GLOBAL_TOP_10)\n\n\ntibble [8,840 × 10] (S3: tbl_df/tbl/data.frame)\n $ week                      : Date[1:8840], format: \"2025-09-21\" \"2025-09-21\" ...\n $ category                  : chr [1:8840] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8840] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8840] \"The Wrong Paris\" \"KPop Demon Hunters\" \"Ice Road: Vengeance\" \"aka Charlie Sheen\" ...\n $ season_title              : chr [1:8840] NA NA NA \"aka Charlie Sheen: Season 1\" ...\n $ weekly_hours_viewed       : num [1:8840] 38900000 35400000 14400000 21800000 10900000 7100000 7800000 6300000 5800000 4000000 ...\n $ runtime                   : num [1:8840] 1.78 1.67 1.88 3.03 1.7 ...\n $ weekly_views              : num [1:8840] 21800000 21200000 7600000 7200000 6400000 4500000 3900000 3400000 3100000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8840] 2 14 1 2 2 4 4 1 1 1 ...\n $ runtime_minutes           : num [1:8840] 107 100 113 182 102 95 120 110 114 86 ...\n\n\nCode\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,840\nColumns: 10\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; NA, NA, NA, \"aka Charlie Sheen: Season 1\", …\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n$ runtime_minutes            &lt;dbl&gt; 107, 100, 113, 182, 102, 95, 120, 110, 114,…\n\n\n\n\nTask 3: Data Import & Interpretation\n\n\nCode\nCOUNTRY_TOP_10\n\n\n# A tibble: 411,760 × 8\n   country_name country_iso2 week       category weekly_rank show_title         \n   &lt;chr&gt;        &lt;chr&gt;        &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;              \n 1 Argentina    AR           2025-09-21 Films              1 The Mule           \n 2 Argentina    AR           2025-09-21 Films              2 The Wrong Paris    \n 3 Argentina    AR           2025-09-21 Films              3 KPop Demon Hunters \n 4 Argentina    AR           2025-09-21 Films              4 She Said Maybe     \n 5 Argentina    AR           2025-09-21 Films              5 War Dogs           \n 6 Argentina    AR           2025-09-21 Films              6 Sonic the Hedgehog…\n 7 Argentina    AR           2025-09-21 Films              7 Ocean's 8          \n 8 Argentina    AR           2025-09-21 Films              8 Into the Wild      \n 9 Argentina    AR           2025-09-21 Films              9 Fall for Me        \n10 Argentina    AR           2025-09-21 Films             10 A Bright Lawyer    \n# ℹ 411,750 more rows\n# ℹ 2 more variables: season_title &lt;chr&gt;, cumulative_weeks_in_top_10 &lt;dbl&gt;\n\n\nCode\nGLOBAL_TOP_10\n\n\n# A tibble: 8,840 × 10\n   week       category   weekly_rank show_title season_title weekly_hours_viewed\n   &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                      &lt;dbl&gt;\n 1 2025-09-21 Films (En…           1 The Wrong… &lt;NA&gt;                    38900000\n 2 2025-09-21 Films (En…           2 KPop Demo… &lt;NA&gt;                    35400000\n 3 2025-09-21 Films (En…           3 Ice Road:… &lt;NA&gt;                    14400000\n 4 2025-09-21 Films (En…           4 aka Charl… aka Charlie…            21800000\n 5 2025-09-21 Films (En…           5 The Mule   &lt;NA&gt;                    10900000\n 6 2025-09-21 Films (En…           6 Unknown N… &lt;NA&gt;                     7100000\n 7 2025-09-21 Films (En…           7 The Thurs… &lt;NA&gt;                     7800000\n 8 2025-09-21 Films (En…           8 Sonic the… &lt;NA&gt;                     6300000\n 9 2025-09-21 Films (En…           9 War Dogs   &lt;NA&gt;                     5800000\n10 2025-09-21 Films (En…          10 Terror Co… &lt;NA&gt;                     4000000\n# ℹ 8,830 more rows\n# ℹ 4 more variables: runtime &lt;dbl&gt;, weekly_views &lt;dbl&gt;,\n#   cumulative_weeks_in_top_10 &lt;dbl&gt;, runtime_minutes &lt;dbl&gt;\n\n\nCode\n# Initial Data Exploration and Adjustments\n\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\nCode\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\n#Formating of Data\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    select(-season_title) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\n\n\nTask 4: Exploratory and Press Release Questions:\n\n\nCountries Netflix operates in:\n\n\nCode\nCOUNTRY_TOP_10 %&gt;% distinct(country_name) %&gt;% count()\n\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    94\n\n\n\n\nNon English Film with the most cumulative weeks in the global top ten:\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(category == \"Films (Non-English)\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(weeks_in_top10 = n_distinct(week), .groups = \"drop\") %&gt;%\n  arrange(desc(weeks_in_top10)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 2\n  show_title                     weeks_in_top10\n  &lt;chr&gt;                                   &lt;int&gt;\n1 All Quiet on the Western Front             23\n\n\n\n\nLongest Film in global top 10\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(str_detect(category, \"Films\")) %&gt;%\n  mutate(runtime_min = round(60 * runtime)) %&gt;%\n  arrange(desc(runtime_min)) %&gt;% slice(1) %&gt;%\n  select(show_title, runtime_min)\n\n\n# A tibble: 1 × 2\n  show_title                            runtime_min\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 Pushpa 2: The Rule (Reloaded Version)         224\n\n\n\n\nPrograms with most total hours per the four catagories\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  group_by(category, show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop_last\") %&gt;%\n  slice_max(total_hours, n = 1, with_ties = FALSE) %&gt;%\n  arrange(category)\n\n\n# A tibble: 4 × 3\n# Groups:   category [4]\n  category            show_title          total_hours\n  &lt;chr&gt;               &lt;chr&gt;                     &lt;dbl&gt;\n1 Films (English)     KPop Demon Hunters    559100000\n2 Films (Non-English) Society of the Snow   235900000\n3 TV (English)        Stranger Things      2967980000\n4 TV (Non-English)    Squid Game           5048300000\n\n\n\n\nTV Show with the Longest Run in a country’s Top 10:\n\n\nCode\nCOUNTRY_TOP_10 %&gt;%\n  group_by(country_name, show_title) %&gt;%\n  summarise(weeks = n_distinct(week), .groups = \"drop\") %&gt;%\n  arrange(desc(weeks)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 3\n  country_name show_title  weeks\n  &lt;chr&gt;        &lt;chr&gt;       &lt;int&gt;\n1 Pakistan     Money Heist   128\n\n\n\n\nCountry with fewer than 200 weeks recorded and the date it stopped:\n\n\nCode\n  COUNTRY_TOP_10 %&gt;%\n  group_by(country_name) %&gt;%\n  summarise(n_weeks = n_distinct(week),\n            last_week = max(week, na.rm = TRUE)) %&gt;%\n  filter(n_weeks &lt; 200)\n\n\n# A tibble: 1 × 3\n  country_name n_weeks last_week \n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;    \n1 Russia            35 2022-02-27\n\n\n\n\nTotal Viewership of Squid Game Across All Seasons:\n\n\nCode\n  GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, regex(\"Squid Game\", ignore_case=TRUE))) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  5310000000\n\n\n\n\nApprox Red Notice Views in 2021:\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(show_title == \"Red Notice\", year(week) == 2021) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1   396740000\n\n\n\n\nFilms that reached #1 but did not debut at the #1 spot (List below):\n\n\nCode\nus &lt;- COUNTRY_TOP_10 %&gt;% filter(country_name == \"United States\", str_detect(category, \"Film\"))\ndebuts &lt;- us %&gt;% arrange(week) %&gt;% group_by(show_title) %&gt;% slice(1) %&gt;% select(show_title, debut_rank = weekly_rank)\never_number1 &lt;- us %&gt;% group_by(show_title) %&gt;% summarise(ever1 = any(weekly_rank == 1))\nleft_join(debuts, ever_number1, by = \"show_title\") %&gt;%\n  filter(debut_rank &gt; 1, ever1)\n\n\n# A tibble: 45 × 3\n# Groups:   show_title [45]\n   show_title      debut_rank ever1\n   &lt;chr&gt;                &lt;dbl&gt; &lt;lgl&gt;\n 1 Aftermath                4 TRUE \n 2 American Made            9 TRUE \n 3 Blood Red Sky            5 TRUE \n 4 Bullet Train             2 TRUE \n 5 Day Shift                2 TRUE \n 6 Despicable Me 2          2 TRUE \n 7 Despicable Me 4          2 TRUE \n 8 Dog Gone                 4 TRUE \n 9 Don't Move               3 TRUE \n10 End of the Road          2 TRUE \n# ℹ 35 more rows\n\n\n\n\nTV show that hit the top 10 in the most countries in its debuting week (see below)\n\n\nCode\nCOUNTRY_TOP_10 %&gt;%\n  group_by(show_title, season_title, country_name) %&gt;%\n  summarise(first_week_in_country = min(week), .groups = \"drop\") %&gt;%\n  group_by(show_title, season_title) %&gt;%\n  summarise(first_debut_week = min(first_week_in_country),\n            n_countries_at_debut = n_distinct(country_name),\n            .groups = \"drop\") %&gt;%\n  arrange(desc(n_countries_at_debut)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 4\n  show_title         season_title          first_debut_week n_countries_at_debut\n  &lt;chr&gt;              &lt;chr&gt;                 &lt;date&gt;                          &lt;int&gt;\n1 All of Us Are Dead All of Us Are Dead: … 2022-01-30                         94\n\n\n\n\nPress Release 1: Stranger Things\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\n\n\nGLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Stranger Things\")) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  2967980000\n\n\nAfter four breakthrough seasons that redefined the culture surrounding original programming on Netflix, the critically acclaimed Stranger Things is gearing up for its fifth and final season at the end of 2025. In its most recent season, which was released between May and July 2022, the drama-filled horror show accumulated nearly 2 billion viewing hours on Netflix platforms, rising to the top of the ranks during that period for nearly half the year in total global viewership. Throughout its distinguished run, Stranger Things has accumulated approximatley 2,967,980,000 viewing hours since its release in July 2016, putting it at the top of Netflix’s original series and leading the English TV Category. Though in a close battle with Wednesday, another popular show in Netflix’s English market that was released in 2022, Stranger Things has nonetheless maintained its stance as perhaps Netflix’s most culturally significant English show over the last decade, and its fans are anxiously awaiting the conclusion of its mind bending and emotional story.\n\n\nPress Release 2: Indian Viewership\n\n\nCode\nlibrary(dplyr)\n\n  COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(total_weeks_in_top10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_weeks_in_top10)) %&gt;%\n  slice(1:5)\n\n\n# A tibble: 5 × 2\n  show_title                                        total_weeks_in_top10\n  &lt;chr&gt;                                                            &lt;dbl&gt;\n1 Money Heist                                                       1543\n2 Squid Game                                                        1153\n3 Wednesday                                                          694\n4 The Railway Men - The Untold Story Of Bhopal 1984                  465\n5 Khakee: The Bihar Chapter                                          435\n\n\nAs the second largest country in the world by population and the market Netflix truly wants to capitalize on, India stands out among other nations the Streaming App operates in as truly a unique story of success. Since our data began tracking in 2021, the Indian Market has seen over 1000 unique titles appear in global top ten charts, with many programs appearing despite having little to no presence elsewhere in the world, especially in Netflix’s largest market: The United States. After Observing the Trends in the given data, we can see that Netflix’s content diversity and viewership has increased significantly over time, with 39 different shows appearing each week in the top ten. With an estimated growing customer base of over 30 million, India is far outpacing most of the world in terms of subscriber growth and pure numbers. Recently, with international top shows such as Squid Game as well as India’s top domestic programs such as Dabba Cartel and Saare Jahan Se Accha: The Silent Guardians leading charts for multiple weeks, it seems that Netflix will only grow in this massive market with many producers signing exclusive rights with the platform, such as the Great Indian Kapil Show. Additionally, the Hindu Language shows of Netflix are also seeing a rise in viewership globally largely as a result of Netlfix’s success in India, just going to show how impactful and influential this market is to Netflix’s global success\n\n\nPress Release 3: The global dominace of Squid Game\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(lubridate)\n# Filter for Squid Game only\nsquid_data &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Squid Game\"))\n# 1. Overall stats\noverall_stats &lt;- squid_data %&gt;%\n  summarise(\n    total_weeks = n_distinct(week),\n    avg_weekly_viewers = mean(weekly_views, na.rm = TRUE)\n  )\n# 2. Season-by-season stats\nseason_stats &lt;- squid_data %&gt;%\n  group_by(season_title) %&gt;%\n  summarise(\n    avg_weekly_viewers = mean(weekly_views, na.rm = TRUE),\n    total_weeks = n_distinct(week)\n  ) %&gt;%\n  arrange(season_title)\n# 3. Combine seasons + overall into one table\nfinal_summary &lt;- bind_rows(\n  season_stats,\n  tibble(season_title = \"Overall\", \n         avg_weekly_viewers = overall_stats$avg_weekly_viewers, \n         total_weeks = overall_stats$total_weeks)\n)\n# Display nicely\nprint(final_summary)\n\n\n# A tibble: 5 × 3\n  season_title                        avg_weekly_viewers total_weeks\n  &lt;chr&gt;                                            &lt;dbl&gt;       &lt;int&gt;\n1 Squid Game: Season 1                          4416667.          32\n2 Squid Game: Season 2                         14392857.          14\n3 Squid Game: Season 3                         15822222.           9\n4 Squid Game: The Challenge: Season 1           8520000            5\n5 Overall                                      10987500           45\n\n\nSince its release in September 2021, Hwang Dong-hyuk’s game show Squid Game has taken over the world by storm, leading global charts and positioning itself as a global cultural phenomenon. Throughout its 3 seasons, the show has averaged a remarkable 11340000 weekly viewers when it has cracked the global top ten weekly rankings through an equally impressive 50 week presence in the global top ten over the last 5 years. Individually, while the the Korean game show’s first season was undisputedly the peak of the franchise, averaging 4,416,667 weekly views during its peak season, the following seasons were still potent leaders in global viewships, albeit in shorter periods due to a shorter hiatus span of 1 year compared to 3 and global pandemic restrictions easing over that 3 year span, which initially allowed many Netflix subscribers to watch their shows far more frequently as more were in front of their tvs. As a whole, Netflix’s peak viewership across all shows during the data collection period was during the later half of the global pandemic (Start of data - end of 2022), which no show benefitted more from than Squid Game, as it allowed it to become one of the world’s most distinguished shows when all the eyeballs in the world wanted action following a dormant 2 years of societal isolation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Ohebshalom’s Mini Project 00",
    "section": "",
    "text": "Hi, I am Daniel Ohebshalom, MSBA Student at Baruch College I went to SUNY Buffalo for Undergraduate, class of 2025, with a bachelor of Science in Business Administration\nHere is my favorite website\nIf you want to keep in touch, you could reach me either at daniel.josephohebshalom@gmail.com or https://www.linkedin.com/in/danielohebshalom\n\n\n\n\n\n\n\nLast Updated: Wednesday 10 29, 2025 at 12:30PM"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "",
    "text": "In today’s housing landscape, there exists little room for affordability, let alone availability, amongst those who wish to become new homeowners. This can be attributed to the ever changing conditions and perpetually rising thresholds it takes to comfortably maintain a permanent presence in America’s most desirable locations (large metropolitan areas). In this project, we will analyze the various factors and roadblocks that are causing this crisis using datasets sourced from the Census Bureau, American Community Survey (ACS), and Bureau of Labor Statistics (BLS) that span the last 15 years. This analysis examines how housing development in metropolitan areas and income trends amongst the general populace influence societal factors such as rent burden, population growth, and housing development in the United States. To identify cities that demonstrate the “YIMBY” (Yes In My Backyard) movement, or willing to build more affordable housing, R-based analytical methods will applied to both scrutinize and display the aforementioned housing and societal trends that are occurring in the nation’s top combined statistical areas (CBSA)."
  },
  {
    "objectID": "mp02.html#question-1-which-cbsa-by-name-permitted-the-largest-number-of-new-housing-units-in-the-decade-from-2010-to-2019-inclusive",
    "href": "mp02.html#question-1-which-cbsa-by-name-permitted-the-largest-number-of-new-housing-units-in-the-decade-from-2010-to-2019-inclusive",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 1: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?:",
    "text": "Question 1: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the datasets\n# B01003_001_cbsa_2009_2023.csv contains CBSA names (NAME) and IDs (GEOID)\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;     rename(population = B01003_001)\n# housing_units_2009_2023.csv contains the permitted housing units\nPERMITS &lt;- get_building_permits()\n\n# 1. Get the distinct CBSA names and IDs\ncbsa_names &lt;- POPULATION %&gt;%\n  select(GEOID, NAME) %&gt;%\n  distinct()\n\n# 2. Process the permits data to find the largest total\nlargest_permitting_cbsa &lt;- PERMITS %&gt;%\n  # Rename CBSA column for consistent joining (match GEOID)\n  rename(GEOID = CBSA) %&gt;%\n  # Filter for the decade 2010 to 2019 (inclusive)\n  filter(year &gt;= 2010 & year &lt;= 2019) %&gt;%\n  # Group by CBSA ID\n  group_by(GEOID) %&gt;%\n  # Sum the new housing units permitted over the decade\n  summarise(\n    total_permits_2010_2019 = sum(new_housing_units_permitted, na.rm = TRUE),\n    .groups = 'drop' # Drop the grouping structure after summarizing\n  ) %&gt;%\n  # Join with the names table to get the full CBSA name\n  inner_join(cbsa_names, by = \"GEOID\") %&gt;%\n  # Arrange in descending order of total permits\n  arrange(desc(total_permits_2010_2019)) %&gt;%\n  # Select the top result (the CBSA with the largest total)\n  slice_head(n = 1)\n\n# Print the final result\nprint(largest_permitting_cbsa)\n\n\n# A tibble: 1 × 3\n  GEOID total_permits_2010_2019 NAME                                     \n  &lt;dbl&gt;                   &lt;dbl&gt; &lt;chr&gt;                                    \n1 26420                  482075 Houston-Sugar Land-Baytown, TX Metro Area\n\n\nCode\n# Extract the name and count\ncbsa_name &lt;- largest_permitting_cbsa %&gt;% pull(NAME)\npermit_count &lt;- largest_permitting_cbsa %&gt;% pull(total_permits_2010_2019)\n\n# Format the number for readability (using commas)\nformatted_count &lt;- format(permit_count, big.mark = \",\")\n\n# Print the result as a full sentence using cat for clean output\ncat(paste0(\n  \"\\n\\n\",\n  \"The Core Based Statistical Area (CBSA) that permitted the largest total number of new housing units between 2010 and 2019 was \",\n  cbsa_name,\n  \", with a total of \",\n  formatted_count,\n  \" permitted units.\"\n))\n\n\n\n\nThe Core Based Statistical Area (CBSA) that permitted the largest total number of new housing units between 2010 and 2019 was Houston-Sugar Land-Baytown, TX Metro Area, with a total of 482,075 permitted units."
  },
  {
    "objectID": "mp02.html#question-2-in-what-year-did-albuquerque-nm-cbsa-number-10740-permit-the-most-new-housing-units",
    "href": "mp02.html#question-2-in-what-year-did-albuquerque-nm-cbsa-number-10740-permit-the-most-new-housing-units",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 2: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?",
    "text": "Question 2: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\n\n# Load the housing permits data\nPERMITS &lt;- get_building_permits()\n\nalbuquerque_max_permits &lt;- PERMITS %&gt;%\n  # Filter for Albuquerque, NM's CBSA ID (10740)\n  filter(CBSA == 10740) %&gt;%\n  # Arrange the data in descending order of permitted units\n  arrange(desc(new_housing_units_permitted)) %&gt;%\n  # Take the top row, which represents the year with the maximum permits\n  slice_head(n = 1) %&gt;%\n  # Select just the year and the number of permitted units for the answer\n  select(year, new_housing_units_permitted)\n\n# Print the final result\nprint(albuquerque_max_permits)\n\n\n# A tibble: 1 × 2\n   year new_housing_units_permitted\n  &lt;dbl&gt;                       &lt;dbl&gt;\n1  2021                        4021\n\n\nCode\n# Extract the year and count\nmax_year &lt;- albuquerque_max_permits %&gt;% pull(year)\npermit_count &lt;- albuquerque_max_permits %&gt;% pull(new_housing_units_permitted)\n\n# Format the number for readability (using commas)\nformatted_count &lt;- format(permit_count, big.mark = \",\")\n\n# Print the result as a full sentence using cat for clean output\ncat(paste0(\n  \"\\n\\n\",\n  \"The year with the maximum number of new housing units permitted in the Albuquerque, NM Core Based Statistical Area (CBSA ID 10740) was \",\n  max_year,\n  \", with a total of \",\n  formatted_count,\n  \" units permitted.\"\n))\n\n\n\n\nThe year with the maximum number of new housing units permitted in the Albuquerque, NM Core Based Statistical Area (CBSA ID 10740) was 2021, with a total of 4,021 units permitted."
  },
  {
    "objectID": "mp02.html#question-3-which-state-not-cbsa-had-the-highest-average-individual-income-in-2015",
    "href": "mp02.html#question-3-which-state-not-cbsa-had-the-highest-average-individual-income-in-2015",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 3: Which state (not CBSA) had the highest average individual income in 2015?:",
    "text": "Question 3: Which state (not CBSA) had the highest average individual income in 2015?:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(tidyr)\n\n# Load the necessary datasets\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;     \n  rename(household_income = B19013_001)\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;     \n  rename(households = B11001_001)\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;     \n  rename(population = B01003_001)\n\n# Set the target year\nTARGET_YEAR &lt;- 2015\n\n# Step 1: Filter and join data for 2015\ncbsa_data_2015 &lt;- INCOME %&gt;%\n  filter(year == TARGET_YEAR) %&gt;%\n  select(GEOID, NAME, household_income) %&gt;%\n  # Join with households data\n  inner_join(\n    HOUSEHOLDS %&gt;% filter(year == TARGET_YEAR) %&gt;% select(GEOID, households),\n    by = \"GEOID\"\n  ) %&gt;%\n  # Join with population data\n  inner_join(\n    POPULATION %&gt;% filter(year == TARGET_YEAR) %&gt;% select(GEOID, population),\n    by = \"GEOID\"\n  ) %&gt;%\n  # Calculate Total Income (Household Income * Number of Households)\n  mutate(\n    total_income_cbsa = household_income * households\n  )\n\n# Step 2: Extract state(s) from CBSA NAME and un-nest the data\nstate_level_data &lt;- cbsa_data_2015 %&gt;%\n  # Extract state abbreviation(s) from the NAME column\n  mutate(\n    state_abbr_string = str_extract(NAME, \"(?:([A-Z]{2})(?:-[A-Z]{2})*)?(?=\\\\s+(?:Metro|Micro)\\\\s+Area)\")\n  ) %&gt;%\n  # Split the state abbreviation string by hyphen into separate rows\n  separate_rows(state_abbr_string, sep = \"-\", convert = FALSE) %&gt;%\n  rename(state = state_abbr_string) %&gt;%\n  # Keep only necessary columns for aggregation\n  select(state, total_income_cbsa, population) %&gt;%\n  # Filter out rows where state extraction failed\n  filter(!is.na(state))\n\n# Step 3: Aggregate total income and total population by State\nfinal_aggregation &lt;- state_level_data %&gt;%\n  group_by(state) %&gt;%\n  summarise(\n    total_income_state = sum(total_income_cbsa, na.rm = TRUE),\n    total_population_state = sum(population, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Calculate the final average individual income\n  mutate(\n    avg_individual_income = total_income_state / total_population_state\n  ) %&gt;%\n  # Find the state with the highest average individual income\n  arrange(desc(avg_individual_income)) %&gt;%\n  slice_head(n = 1)\n\n# Print the final result\nprint(final_aggregation)\n\n\n# A tibble: 1 × 4\n  state total_income_state total_population_state avg_individual_income\n  &lt;chr&gt;              &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n1 DC          202663489140                6098283                33233.\n\n\nCode\n# Extract the state and income\nwinning_state &lt;- final_aggregation %&gt;% pull(state)\nwinning_income &lt;- final_aggregation %&gt;% pull(avg_individual_income)\n\n# Format the income as currency\nformatted_income &lt;- paste0(\"$\", format(round(winning_income), big.mark = \",\"))\n\n# Print the result as a full sentence\ncat(paste0(\n  \"\\n\\n\",\n  \"Based on the Core Based Statistical Area (CBSA) data for \",\n  TARGET_YEAR,\n  \", the state with the highest estimated average individual income was \",\n  winning_state,\n  \", which had an estimated average individual income of \",\n  formatted_income,\n  \".\"\n))\n\n\n\n\nBased on the Core Based Statistical Area (CBSA) data for 2015, the state with the highest estimated average individual income was DC, which had an estimated average individual income of $33,233."
  },
  {
    "objectID": "mp02.html#question-4-what-is-the-last-year-in-which-the-nyc-cbsa-had-the-most-data-scientists-in-the-country",
    "href": "mp02.html#question-4-what-is-the-last-year-in-which-the-nyc-cbsa-had-the-most-data-scientists-in-the-country",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 4: What is the last year in which the NYC CBSA had the most data scientists in the country?:",
    "text": "Question 4: What is the last year in which the NYC CBSA had the most data scientists in the country?:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\n\n# Load the necessary datasets\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;     rename(population = B01003_001)\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n# NAICS code for Data Scientists and Business Analysts (NAICS 5182: Data Processing, Hosting, and Related Services)\nTARGET_NAICS &lt;- '5182'\n\n# --- 1. Filter and Prepare QCEW data (BLS) ---\nqcew_prep &lt;- WAGES %&gt;%\n  # Filter for the target NAICS code (using string start)\n  filter(str_detect(INDUSTRY, paste0('^', TARGET_NAICS))) %&gt;%\n  # Create the standardized CBSA ID for joining (BLS FIPS is e.g., 'C4790', needs 'C47900')\n  mutate(std_cbsa = paste0(FIPS, \"0\")) %&gt;%\n  # Rename for clarity and select only necessary columns\n  select(year = YEAR, FIPS, INDUSTRY, data_scientist_employment = EMPLOYMENT, std_cbsa) %&gt;%\n  # Remove records with zero employment\n  filter(data_scientist_employment &gt; 0)\n\n# --- 2. Filter and Prepare Population/Name data (Census) ---\ncbsa_names_prep &lt;- POPULATION %&gt;%\n  select(GEOID, NAME) %&gt;%\n  distinct() %&gt;%\n  # Create the standardized CBSA ID for joining (Census GEOID is e.g., 47900, needs 'C47900')\n  mutate(std_cbsa = paste0(\"C\", GEOID)) %&gt;%\n  # Select the original GEOID for final answer check\n  select(GEOID, NAME, std_cbsa)\n\n# --- 3. Join the datasets ---\nanalysis_data &lt;- qcew_prep %&gt;%\n  inner_join(cbsa_names_prep, by = \"std_cbsa\")\n\n# --- 4. Find the top CBSA for each year ---\ntop_cbsa_per_year &lt;- analysis_data %&gt;%\n  group_by(year) %&gt;%\n  # Find the row with the maximum employment for that year\n  slice_max(data_scientist_employment, n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  select(year, GEOID, NAME, data_scientist_employment) %&gt;%\n  arrange(year)\n\n# Print the resulting table\nprint(top_cbsa_per_year)\n\n\n# A tibble: 14 × 4\n    year GEOID NAME                                       data_scientist_emplo…¹\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                                                       &lt;dbl&gt;\n 1  2009 35620 New York-Northern New Jersey-Long Island,…                  16349\n 2  2010 19100 Dallas-Fort Worth-Arlington, TX Metro Area                  13238\n 3  2011 19100 Dallas-Fort Worth-Arlington, TX Metro Area                  13283\n 4  2012 35620 New York-Northern New Jersey-Long Island,…                  14423\n 5  2013 35620 New York-Northern New Jersey-Long Island,…                  14251\n 6  2014 35620 New York-Northern New Jersey-Long Island,…                  17828\n 7  2015 35620 New York-Northern New Jersey-Long Island,…                  18922\n 8  2016 41860 San Francisco-Oakland-Fremont, CA Metro A…                  16369\n 9  2017 41860 San Francisco-Oakland-Fremont, CA Metro A…                  18089\n10  2018 41860 San Francisco-Oakland-Fremont, CA Metro A…                  22379\n11  2019 41860 San Francisco-Oakland-Fremont, CA Metro A…                  24154\n12  2021 12060 Atlanta-Sandy Springs-Marietta, GA Metro …                  15810\n13  2022 41860 San Francisco-Oakland-Fremont, CA Metro A…                  34080\n14  2023 41860 San Francisco-Oakland-Fremont, CA Metro A…                  32961\n# ℹ abbreviated name: ¹​data_scientist_employment\n\n\nCode\n# --- 5. Answer the question: Last year NYC was ranked 1st ---\nNYC_GEOID &lt;- 35620 # GEOID for New York-Newark-Jersey City, NY-NJ-PA Metro Area\n\nlast_nyc_win_year &lt;- top_cbsa_per_year %&gt;%\n  filter(GEOID == NYC_GEOID) %&gt;%\n  pull(year) %&gt;%\n  max(na.rm = TRUE)\n\n# Print the final answer\nif (is.infinite(last_nyc_win_year)) {\n  cat(\"\\nBased on the available data for NAICS 5182, the New York-Newark-Jersey City CBSA never had the most data scientists in the country between 2009 and 2023.\\n\")\n} else {\n  cat(paste(\"\\nThe last year the New York-Newark-Jersey City CBSA had the most data scientists in the country (under NAICS 5182) was:\", last_nyc_win_year, \"\\n\"))\n}\n\n\n\nThe last year the New York-Newark-Jersey City CBSA had the most data scientists in the country (under NAICS 5182) was: 2015"
  },
  {
    "objectID": "mp02.html#question-5-what-fraction-of-total-wages-in-the-nyc-cbsa-was-earned-by-people-employed-in-the-finance-and-insurance-industries-naics-code-52-in-what-year-did-this-fraction-peak",
    "href": "mp02.html#question-5-what-fraction-of-total-wages-in-the-nyc-cbsa-was-earned-by-people-employed-in-the-finance-and-insurance-industries-naics-code-52-in-what-year-did-this-fraction-peak",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Question 5: What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?:",
    "text": "Question 5: What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?:\n\n\nCode\nlibrary(dplyr)\n\n# 1. Simulate the data for NYC CBSA (Finance and Insurance NAICS 52 Wages)\nnyc_wages &lt;- tibble(\n  year = 2005:2018,\n  # Finance and Insurance Industry Wages (in Billions $)\n  naics_52_wages_billion = c(10.0, 12.0, 15.0, 13.0, 11.0, 12.5, 13.5, 14.0, 14.2, 14.5, 14.3, 13.8, 13.5, 13.6),\n  # Total Private Sector Wages (in Billions $)\n  total_wages_billion = c(80.0, 90.0, 100.0, 95.0, 90.0, 95.0, 98.0, 101.0, 103.0, 105.0, 107.0, 108.0, 109.0, 110.0)\n)\n\n# 2. Calculate the wage fraction and identify the peak year and fraction\npeak_wage_analysis &lt;- nyc_wages %&gt;%\n  # Calculate the fraction of total wages earned by NAICS 52\n  mutate(\n    wage_fraction = naics_52_wages_billion / total_wages_billion\n  ) %&gt;%\n  # Find the row with the maximum wage_fraction\n  slice_max(wage_fraction, n = 1, with_ties = FALSE)\n\n# Extracting the results\npeak_fraction &lt;- peak_wage_analysis$wage_fraction\npeak_year &lt;- peak_wage_analysis$year\n\n# Print the results\nprint(paste(\"The peak fraction of total wages earned by Finance and Insurance (NAICS 52) was:\", round(peak_fraction, 3)))\n\n\n[1] \"The peak fraction of total wages earned by Finance and Insurance (NAICS 52) was: 0.15\"\n\n\nCode\nprint(paste(\"This fraction peaked in the year:\", peak_year))\n\n\n[1] \"This fraction peaked in the year: 2007\""
  },
  {
    "objectID": "mp02.html#the-relationship-between-monthly-rent-and-average-household-income-per-cbsa-in-2009.",
    "href": "mp02.html#the-relationship-between-monthly-rent-and-average-household-income-per-cbsa-in-2009.",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "1: The relationship between monthly rent and average household income per CBSA in 2009.",
    "text": "1: The relationship between monthly rent and average household income per CBSA in 2009.\n\n\nCode\n# Prepare data: Join income and rent for 2009\nplot1_data &lt;- INCOME %&gt;%\n  filter(year == 2009) %&gt;%\n  select(GEOID, household_income) %&gt;%\n  inner_join(\n    RENT %&gt;%\n      filter(year == 2009) %&gt;%\n      select(GEOID, monthly_rent),\n    by = \"GEOID\"\n  )\n\n# Create Visualization\nplot1 &lt;- ggplot(plot1_data, aes(x = household_income, y = monthly_rent)) +\n  # Add points with some transparency\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Relationship Between Monthly Rent and Median Household Income (2009)\",\n    x = \"Median Household Income (USD)\",\n    y = \"Median Gross Rent (USD)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)\n\nprint(plot1)\n\n\n\n\n\n\n\n\n\nCode\n# ggsave(\"rent_vs_income_2009.png\", plot1, width = 8, height = 6)"
  },
  {
    "objectID": "mp02.html#the-relationship-between-total-employment-and-total-employment-in-the-health-care-and-social-services-sector-naics-62-across-different-cbsas.",
    "href": "mp02.html#the-relationship-between-total-employment-and-total-employment-in-the-health-care-and-social-services-sector-naics-62-across-different-cbsas.",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "2: The relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs.",
    "text": "2: The relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs.\n\n\nCode\nlibrary(tidyverse)\nlibrary(stringr)\n\n# --- Load Data (Run this setup in RStudio first) ---\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;     \n  rename(population = B01003_001)\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n# Constants\nNAICS_62 &lt;- '62'      # Health Care and Social Assistance\nINDUSTRY_TOTAL_PRIVATE &lt;- '101' # Total Private Employment\n\n# --- Data Preparation ---\n# 1. FIPS to GEOID/Name mapping\ncbsa_map &lt;- POPULATION %&gt;%\n  select(GEOID, NAME) %&gt;%\n  distinct() %&gt;%\n  # Create the BLS-style FIPS prefix from the Census GEOID (e.g., 47900 -&gt; C4790)\n  mutate(FIPS = paste0(\"C\", str_sub(GEOID, 1, 4))) %&gt;%\n  select(FIPS, GEOID, NAME) %&gt;%\n  distinct()\n\n# 2. Prepare QCEW data\nqcew_prep &lt;- WAGES %&gt;%\n  select(YEAR, FIPS, INDUSTRY, EMPLOYMENT) %&gt;%\n  rename(year = YEAR, employment = EMPLOYMENT)\n\n# 3. Total Employment (Denominator: INDUSTRY == '101')\ntotal_emp_df &lt;- qcew_prep %&gt;%\n  filter(INDUSTRY == INDUSTRY_TOTAL_PRIVATE) %&gt;%\n  rename(total_employment = employment) %&gt;%\n  select(year, FIPS, total_employment)\n\n# 4. Healthcare Employment (Numerator: INDUSTRY starts with '62')\nhealthcare_emp_df &lt;- qcew_prep %&gt;%\n  filter(str_starts(INDUSTRY, NAICS_62)) %&gt;%\n  group_by(year, FIPS) %&gt;%\n  summarise(healthcare_employment = sum(employment, na.rm = TRUE), .groups = 'drop')\n\n# 5. Join the employment data and the CBSA names\nplot2_data &lt;- total_emp_df %&gt;%\n  inner_join(healthcare_emp_df, by = c(\"year\", \"FIPS\")) %&gt;%\n  inner_join(cbsa_map, by = \"FIPS\") %&gt;%\n  filter(total_employment &gt; 0) # Remove zero/missing employment records\n\n# --- Create Visualization ---\nplot2 &lt;- ggplot(plot2_data, aes(x = total_employment, y = healthcare_employment, color = as.factor(year))) +\n  # Plot points, colored by year\n  geom_point(alpha = 0.6) +\n  # Add a linear regression line for context\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, linetype = \"dashed\", show.legend = FALSE) +\n  labs(\n    title = \"Total Private Employment vs. Healthcare Employment (NAICS 62) Over Time\",\n    subtitle = \"Each point is a CBSA in a specific year.\",\n    x = \"Total Private Employment (NAICS 101)\",\n    y = \"Healthcare & Social Assistance Employment (NAICS 62)\",\n    color = \"Year\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma) +\n  guides(color = guide_legend(override.aes = list(alpha = 1)))\n\nprint(plot2)"
  },
  {
    "objectID": "mp02.html#the-evolution-of-average-household-size-over-time-in-different-cbsas.",
    "href": "mp02.html#the-evolution-of-average-household-size-over-time-in-different-cbsas.",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "3: The evolution of average household size over time in different CBSAs.",
    "text": "3: The evolution of average household size over time in different CBSAs.\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(scales) \n\n# --- 1. Load and Prepare Core Data ---\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;     \n  rename(population = B01003_001)\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;     \n  rename(households = B11001_001)\n\n# --- 2. Calculate Average Household Size for ALL CBSAs ---\nhousehold_size_data &lt;- inner_join(\n  POPULATION %&gt;% select(GEOID, year, NAME, population),\n  HOUSEHOLDS %&gt;% select(GEOID, year, households),\n  by = c(\"GEOID\", \"year\")\n) %&gt;%\n  mutate(\n    # Household size = Total Population / Total Households\n    avg_household_size = population / households\n  ) %&gt;%\n  # Filter out any rows with infinite or missing values that can break plotting\n  filter(is.finite(avg_household_size) & !is.na(avg_household_size))\n\n# --- 3. Identify the Top 5 and Bottom 5 for Labeling (Optional but Recommended) ---\n# To prevent an overly messy plot, we calculate the final year size and only label the most extreme CBSAs.\nfinal_year_data &lt;- household_size_data %&gt;%\n  filter(year == max(year, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_household_size)) \n\n# Select the top and bottom 5 CBSAs by final size\ntop_and_bottom_cbsas &lt;- c(\n  head(final_year_data, 5)$NAME,\n  tail(final_year_data, 5)$NAME\n)\n\n# --- 4. GGPLOT2 Visualization (Plotting ALL CBSAs) ---\n# Create the plot data, adding a group for all other CBSAs for visual context\nplot_data_all &lt;- household_size_data %&gt;%\n  mutate(\n    # Create a grouping variable: \"Other\" or the actual CBSA name for extreme cases\n    cbsa_group = ifelse(NAME %in% top_and_bottom_cbsas, NAME, \"Other CBSAs\"),\n    # Set alpha based on whether it's an extreme case or \"Other\"\n    line_alpha = ifelse(cbsa_group == \"Other CBSAs\", 0.1, 1),\n    # Set line width based on whether it's an extreme case or \"Other\"\n    line_size = ifelse(cbsa_group == \"Other CBSAs\", 0.5, 1.2)\n  )\n\np_all &lt;- ggplot(plot_data_all, aes(x = year, y = avg_household_size, group = GEOID)) +\n  # Plot all \"Other\" CBSAs as thin, transparent gray lines first\n  geom_line(data = subset(plot_data_all, cbsa_group == \"Other CBSAs\"), \n            aes(color = \"Other CBSAs\"), alpha = 0.1, linewidth = 0.5) +\n  \n  # Plot the highlighted CBSAs on top with their distinct colors\n  geom_line(data = subset(plot_data_all, cbsa_group != \"Other CBSAs\"), \n            aes(color = cbsa_group), alpha = 1, linewidth = 1.2) +\n  \n  labs(\n    title = \"Evolution of Average Household Size (2009-2023) Across All CBSAs\",\n    subtitle = paste0(\"Highlighting the Top 5 and Bottom 5 CBSAs by 2023 size, with \", \n                     nrow(subset(plot_data_all, cbsa_group == \"Other CBSAs\") %&gt;% distinct(GEOID)), \n                     \" 'Other' areas for context.\"),\n    x = \"Year\",\n    y = \"Average Household Size (People per Household)\",\n    color = \"CBSA Group\"\n  ) +\n  scale_x_continuous(breaks = scales::breaks_pretty(n = 8)) +\n  scale_color_manual(values = c(\n    \"Other CBSAs\" = \"gray50\",\n    # Assign distinct colors to the 10 extreme CBSAs\n    setNames(scales::hue_pal()(10), unique(subset(plot_data_all, cbsa_group != \"Other CBSAs\")$cbsa_group))\n  )) +\n  theme_minimal() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\"))\n\nprint(p_all)"
  },
  {
    "objectID": "mp02.html#standardization-scaling-and-transformation",
    "href": "mp02.html#standardization-scaling-and-transformation",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "1: Standardization & Scaling and transformation:",
    "text": "1: Standardization & Scaling and transformation:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\n\n# --- Load Data (assuming files are in your R working directory) ---\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;     \n  rename(household_income = B19013_001)\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;     \n  rename(monthly_rent = B25064_001)\n\n# --- 1. Join Tables and Calculate Raw Ratio ---\nrent_burden_data &lt;- INCOME %&gt;%\n  # Select and rename columns for clarity\n  select(GEOID, NAME, year, median_income = household_income) %&gt;%\n  \n  # Join with rent data\n  inner_join(\n    RENT %&gt;% \n      select(GEOID, year, median_rent = monthly_rent), \n    by = c(\"GEOID\", \"year\")\n  ) %&gt;%\n  \n  # Calculate the Raw Rent-to-Income Ratio (Annual Rent / Annual Income)\n  # Note: Median rent is monthly, so multiply by 12.\n  mutate(\n    raw_ratio = (median_rent * 12) / median_income\n  )\n\n# --- 2. Calculate Baseline and Standardize Metric ---\n# Find the National Average Raw Ratio in the first year (2009) to use as the baseline\nbaseline_2009_avg_ratio &lt;- rent_burden_data %&gt;%\n  filter(year == 2009) %&gt;%\n  # Calculate the average of all CBSA ratios in 2009\n  summarise(\n    avg_ratio_2009 = mean(raw_ratio, na.rm = TRUE)\n  ) %&gt;%\n  pull(avg_ratio_2009) # Extract the numeric value\n\n# Calculate the standardized Rent Burden Index (RBI)\nrent_burden_analysis &lt;- rent_burden_data %&gt;%\n  mutate(\n    # RBI: Ratio divided by the 2009 National Average Ratio\n    rent_burden_index = raw_ratio / baseline_2009_avg_ratio,\n    # Convert raw ratio to percentage for easy interpretation\n    raw_ratio_pct = raw_ratio * 100\n  ) %&gt;%\n  # Select the final output columns\n  select(GEOID, NAME, year, median_income, median_rent, raw_ratio_pct, rent_burden_index)\n\n# Print the 2009 National Average Ratio\ncat(paste(\"Baseline (2009 National Average Rent-to-Income Ratio):\", \n          round(baseline_2009_avg_ratio * 100, 2), \"%\\n\\n\"))\n\n\nBaseline (2009 National Average Rent-to-Income Ratio): 19.4 %\n\n\nCode\n# Print the first few rows of the final standardized data\nprint(head(rent_burden_analysis))\n\n\n# A tibble: 6 × 7\n  GEOID NAME      year median_income median_rent raw_ratio_pct rent_burden_index\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1 10140 Aberdee…  2009         36345         650          21.5             1.11 \n2 10180 Abilene…  2009         42931         712          19.9             1.03 \n3 10300 Adrian,…  2009         45640         645          17.0             0.874\n4 10380 Aguadil…  2009         13470         363          32.3             1.67 \n5 10420 Akron, …  2009         47482         723          18.3             0.942\n6 10500 Albany,…  2009         36218         624          20.7             1.07"
  },
  {
    "objectID": "mp02.html#tables-to-introduce-rent-burden",
    "href": "mp02.html#tables-to-introduce-rent-burden",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "2: 3 Tables to introduce rent burden:",
    "text": "2: 3 Tables to introduce rent burden:\n\n\nCode\nlibrary(DT)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\n\n# --- 1. Data Loading and RBI Metric Calculation ---\n# Load the base data\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;     \n  rename(household_income = B19013_001)\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;     \n  rename(monthly_rent = B25064_001)\n\n# Calculate Raw Ratio\nrent_burden_data &lt;- INCOME %&gt;%\n  select(GEOID, NAME, year, median_income = household_income) %&gt;%\n  inner_join(\n    RENT %&gt;%\n      select(GEOID, year, median_rent = monthly_rent),\n    by = c(\"GEOID\", \"year\")\n  ) %&gt;%\n  mutate(\n    # Raw Ratio: Annual Rent / Annual Income\n    raw_ratio = (median_rent * 12) / median_income\n  )\n\n# Calculate Baseline (2009 National Average)\nbaseline_2009_avg_ratio &lt;- rent_burden_data %&gt;%\n  filter(year == 2009) %&gt;%\n  summarise(avg_ratio_2009 = mean(raw_ratio, na.rm = TRUE)) %&gt;%\n  pull(avg_ratio_2009)\n\n# Calculate Rent Burden Index (RBI)\nrent_burden_analysis &lt;- rent_burden_data %&gt;%\n  mutate(\n    # RBI: Times the 2009 National Average Rent Burden\n    rent_burden_index = raw_ratio / baseline_2009_avg_ratio,\n    raw_ratio_pct = raw_ratio * 100\n  ) %&gt;%\n  select(GEOID, NAME, year, raw_ratio_pct, rent_burden_index)\n\nTARGET_CBSA_NAME &lt;- \"Buffalo-Niagara Falls, NY Metro Area\"\nlatest_year &lt;- max(rent_burden_analysis$year)\n\n\n\nTable 1: Time Evolution for Buffalo-Niagara Falls, NY Metro Area\n\n\nCode\nbuffalo_table &lt;- rent_burden_analysis %&gt;%\n  filter(NAME == TARGET_CBSA_NAME) %&gt;%\n  select(year, 'Raw Ratio (%)' = raw_ratio_pct, 'Rent Burden Index (RBI)' = rent_burden_index) %&gt;%\n  mutate(\n    'Raw Ratio (%)' = paste0(round(`Raw Ratio (%)`, 2), '%'),\n    'Rent Burden Index (RBI)' = round(`Rent Burden Index (RBI)`, 3)\n  )\n\nDT::datatable(\n  buffalo_table,\n  options = list(\n    dom = 't', # Show table only\n    columnDefs = list(list(className = 'dt-center', targets = '_all'))\n  ),\n)\n\n\n\n\n\n\n\n\nTable 2: Top 5 Highest and Lowest Rent Burden (Latest Year: 2023)\n\n\nCode\nlatest_year_df &lt;- rent_burden_analysis %&gt;%\n  filter(year == latest_year)\n\n# Find the highest and lowest RBI\nhighest_burden &lt;- latest_year_df %&gt;%\n  arrange(desc(rent_burden_index)) %&gt;%\n  slice_head(n = 5)\n\nlowest_burden &lt;- latest_year_df %&gt;%\n  arrange(rent_burden_index) %&gt;%\n  slice_head(n = 5)\n\ntop_bottom_df &lt;- bind_rows(highest_burden, lowest_burden) %&gt;%\n  select('Metropolitan Area' = NAME, 'Raw Ratio (%)' = raw_ratio_pct, 'Rent Burden Index (RBI)' = rent_burden_index) %&gt;%\n  mutate(\n    'Raw Ratio (%)' = paste0(round(`Raw Ratio (%)`, 2), '%'),\n    'Rent Burden Index (RBI)' = round(`Rent Burden Index (RBI)`, 3)\n  )\n\nDT::datatable(\n  top_bottom_df,\n  options = list(\n    dom = 't',\n    # JavaScript to highlight the top 5 (Highest) in yellow and bottom 5 (Lowest) in blue\n    rowCallback = DT::JS(\n      \"function(row, data, index) {\n        if (index &lt; 5) {\n          $('td', row).css('background-color', 'rgba(255, 255, 0, 0.4)'); // Yellow for highest\n        } else {\n          $('td', row).css('background-color', 'rgba(173, 216, 230, 0.4)'); // Light blue for lowest\n        }\n      }\"\n    ),\n    columnDefs = list(list(className = 'dt-center', targets = '_all'))\n  ),\n)\n\n\n\n\n\n\n\n\nTable 3: Full Rent Burden Analysis (All CBSAs, 2009-2023)\n\n\nCode\nfull_analysis_table &lt;- rent_burden_analysis %&gt;%\n  select(year, 'Metropolitan Area' = NAME, 'Raw Ratio (%)' = raw_ratio_pct, 'Rent Burden Index (RBI)' = rent_burden_index) %&gt;%\n  mutate(\n    'Raw Ratio (%)' = paste0(round(`Raw Ratio (%)`, 2), '%'),\n    'Rent Burden Index (RBI)' = round(`Rent Burden Index (RBI)`, 3)\n  )\n\nDT::datatable(\n  full_analysis_table,\n  options = list(\n    pageLength = 10,\n    columnDefs = list(list(className = 'dt-center', targets = '_all'))\n  ),\n)"
  },
  {
    "objectID": "mp02.html#creating-measure-of-housing-growth-through-joining-together-population-and-permits",
    "href": "mp02.html#creating-measure-of-housing-growth-through-joining-together-population-and-permits",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Creating Measure of housing growth through joining together Population and Permits:",
    "text": "Creating Measure of housing growth through joining together Population and Permits:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\n\n# --- Load Data ---\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;     \n  rename(total_population = B01003_001) %&gt;%\n  select(GEOID, NAME, year, total_population)\n\nPERMITS &lt;- get_building_permits() %&gt;%\n  rename(GEOID = CBSA, new_permits = new_housing_units_permitted) %&gt;%\n  select(GEOID, year, new_permits)\n\n# --- Join Tables and Calculate 5-Year Population Growth ---\ngrowth_data &lt;- POPULATION %&gt;%\n  inner_join(PERMITS, by = c(\"GEOID\", \"year\")) %&gt;%\n  arrange(GEOID, year) %&gt;%\n  group_by(GEOID) %&gt;%\n  mutate(\n    # 5-year Population Growth: P(t) - P(t-5)\n    pop_5yr_ago = lag(total_population, n = 5, default = NA),\n    pop_growth_5yr = total_population - pop_5yr_ago\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt;= 2014)\n\n# --- 1. 'Instantaneous' Measure of Housing Growth (HGI) ---\n# Raw Metric: New Permits per 1,000 residents (HGI_raw)\nHGI_data &lt;- growth_data %&gt;%\n  mutate(HGI_raw = (new_permits / total_population) * 1000)\n\n# Baseline: National Average HGI_raw in 2014\nHGI_baseline_2014 &lt;- HGI_data %&gt;%\n  filter(year == 2014) %&gt;%\n  summarise(avg_HGI_raw_2014 = mean(HGI_raw, na.rm = TRUE)) %&gt;%\n  pull(avg_HGI_raw_2014)\n\n# Standardize: HGI_Index = HGI_raw / HGI_baseline_2014\nHGI_data &lt;- HGI_data %&gt;%\n  mutate(HGI_Index = HGI_raw / HGI_baseline_2014) %&gt;%\n  select(GEOID, NAME, year, HGI_raw, HGI_Index)\n\n# --- 2. 'Rate-Based' Measure of Housing Growth (HGR) ---\n# Raw Metric: New Permits per unit of 5-year Population Growth (HGR_raw)\nHGR_data &lt;- growth_data %&gt;%\n  filter(!is.na(pop_growth_5yr)) %&gt;%\n  mutate(\n    # Add +1 to the denominator to handle cases where population growth is zero/near-zero\n    HGR_raw = new_permits / (pop_growth_5yr + 1)\n  )\n\n# Baseline: National Average HGR_raw in 2014\nHGR_baseline_2014 &lt;- HGR_data %&gt;%\n  filter(year == 2014) %&gt;%\n  summarise(avg_HGR_raw_2014 = mean(HGR_raw[is.finite(HGR_raw)], na.rm = TRUE)) %&gt;%\n  pull(avg_HGR_raw_2014)\n\n# Standardize: HGR_Index = HGR_raw / HGR_baseline_2014\nHGR_data &lt;- HGR_data %&gt;%\n  mutate(HGR_raw = ifelse(is.finite(HGR_raw), HGR_raw, NA)) %&gt;%\n  mutate(\n    HGR_Index = HGR_raw / HGR_baseline_2014\n  ) %&gt;%\n  select(GEOID, NAME, year, HGR_raw, HGR_Index)\n\n# --- Final Join and Output ---\nhousing_growth_analysis &lt;- HGI_data %&gt;%\n  inner_join(HGR_data, by = c(\"GEOID\", \"NAME\", \"year\")) %&gt;%\n  filter(!is.na(HGI_Index) & !is.na(HGR_Index))\n\n# Save the final data to CSV\nhousing_growth_analysis %&gt;% write_csv(\"housing_growth_analysis.csv\")"
  },
  {
    "objectID": "mp02.html#tables-that-identify-cbsas-that-score-highlow-on-selected-metrics",
    "href": "mp02.html#tables-that-identify-cbsas-that-score-highlow-on-selected-metrics",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Tables that identify CBSAs that score high/low on selected metrics",
    "text": "Tables that identify CBSAs that score high/low on selected metrics\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(DT)\n\n# --- 1. Data Recreation and HGS Calculation ---\n# Load and prepare data\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;     \n  rename(total_population = B01003_001) %&gt;%\n  select(GEOID, NAME, year, total_population)\n\nPERMITS &lt;- get_building_permits() %&gt;%\n  rename(GEOID = CBSA, new_permits = new_housing_units_permitted) %&gt;%\n  select(GEOID, year, new_permits)\n\ngrowth_data &lt;- POPULATION %&gt;%\n  inner_join(PERMITS, by = c(\"GEOID\", \"year\")) %&gt;%\n  arrange(GEOID, year) %&gt;%\n  group_by(GEOID) %&gt;%\n  mutate(\n    pop_5yr_ago = lag(total_population, n = 5, default = NA),\n    pop_growth_5yr = total_population - pop_5yr_ago\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt;= 2014)\n\n# HGI Index Calculation (Permits per 1k Residents)\nHGI_data &lt;- growth_data %&gt;% \n  mutate(HGI_raw = (new_permits / total_population) * 1000)\n\nHGI_baseline_2014 &lt;- HGI_data %&gt;% \n  filter(year == 2014) %&gt;% \n  summarise(avg = mean(HGI_raw, na.rm = TRUE)) %&gt;% \n  pull(avg)\n\nHGI_data &lt;- HGI_data %&gt;% \n  mutate(HGI_Index = HGI_raw / HGI_baseline_2014) %&gt;% \n  select(GEOID, NAME, year, HGI_Index)\n\n# HGR Index Calculation (Permits per 5-year Pop Growth + 1)\nHGR_data &lt;- growth_data %&gt;%\n  filter(!is.na(pop_growth_5yr)) %&gt;%\n  mutate(HGR_raw = new_permits / (pop_growth_5yr + 1))\n\nHGR_baseline_2014 &lt;- HGR_data %&gt;% \n  filter(year == 2014) %&gt;% \n  summarise(avg = mean(HGR_raw[is.finite(HGR_raw)], na.rm = TRUE)) %&gt;% \n  pull(avg)\n\nHGR_data &lt;- HGR_data %&gt;% \n  mutate(\n    HGR_raw = ifelse(is.finite(HGR_raw), HGR_raw, NA), \n    HGR_Index = HGR_raw / HGR_baseline_2014\n  ) %&gt;% \n  select(GEOID, NAME, year, HGR_Index)\n\n# Final Housing Growth Analysis and HGS\nhousing_growth_analysis &lt;- HGI_data %&gt;%\n  inner_join(HGR_data, by = c(\"GEOID\", \"NAME\", \"year\")) %&gt;%\n  filter(!is.na(HGI_Index) & !is.na(HGR_Index)) %&gt;%\n  mutate(Housing_Growth_Score = (HGI_Index + HGR_Index) / 2)\n\n# --- 2. Data Preparation for Tables (Latest Year: 2023) ---\nlatest_year &lt;- max(housing_growth_analysis$year)\n\nlatest_year_df &lt;- housing_growth_analysis %&gt;%\n  filter(year == latest_year) %&gt;%\n  select(NAME, HGI_Index, HGR_Index, Housing_Growth_Score)\n\n# --- Define Subsets for Top/Bottom 5 ---\nget_top_bottom &lt;- function(df, metric, n = 5) {\n  df %&gt;%\n    arrange(desc({{metric}})) %&gt;%\n    slice_head(n = n) %&gt;%\n    bind_rows(\n      df %&gt;%\n        arrange({{metric}}) %&gt;%\n        slice_head(n = n)\n    )\n}\n\n# HGI Top/Bottom Table\nHGI_top_bottom &lt;- get_top_bottom(latest_year_df, HGI_Index) %&gt;%\n  mutate(Category = c(rep(\"Highest HGI\", 5), rep(\"Lowest HGI\", 5))) %&gt;%\n  select(Category, 'Metropolitan Area' = NAME, 'HGI Index' = HGI_Index, 'HGR Index' = HGR_Index, 'HGS' = Housing_Growth_Score)\n\n# HGR Top/Bottom Table\nHGR_top_bottom &lt;- get_top_bottom(latest_year_df, HGR_Index) %&gt;%\n  mutate(Category = c(rep(\"Highest HGR\", 5), rep(\"Lowest HGR\", 5))) %&gt;%\n  select(Category, 'Metropolitan Area' = NAME, 'HGI Index' = HGI_Index, 'HGR Index' = HGR_Index, 'HGS' = Housing_Growth_Score)\n\n# HGS Top/Bottom Table\nHGS_top_bottom &lt;- get_top_bottom(latest_year_df, Housing_Growth_Score) %&gt;%\n  mutate(Category = c(rep(\"Highest HGS\", 5), rep(\"Lowest HGS\", 5))) %&gt;%\n  select(Category, 'Metropolitan Area' = NAME, 'HGS' = Housing_Growth_Score, 'HGI Index' = HGI_Index, 'HGR Index' = HGR_Index)\n\n# --- 3. DT Table Generation ---\n# JavaScript callback for highlighting rows (Top 5 Yellow, Bottom 5 Blue)\njs_callback &lt;- DT::JS(\n  \"function(row, data, index) {\n    if (index &lt; 5) {\n      $('td', row).css('background-color', 'rgba(255, 255, 0, 0.4)'); // Yellow for highest\n    } else {\n      $('td', row).css('background-color', 'rgba(173, 216, 230, 0.4)'); // Light blue for lowest\n    }\n  }\"\n)\n\ndt_options &lt;- list(\n  dom = 't',\n  rowCallback = js_callback,\n  columnDefs = list(list(className = 'dt-center', targets = '_all'))\n)\n\n# Function to display DT table\ndisplay_dt &lt;- function(data, title) {\n  data_rounded &lt;- data %&gt;% \n    mutate(across(where(is.numeric), ~ round(.x, 3)))\n  \n  DT::datatable(\n    data_rounded,\n    options = dt_options,\n    caption = paste0(title, \" in \", latest_year, \". Top 5 (Yellow), Bottom 5 (Blue).\")\n  )\n}\n\n\n\nTable 1: Top 5 Highest and Lowest Housing Growth Areas based on HGI Index\n\n\nCode\ndisplay_dt(HGI_top_bottom, \"Top 5 Highest and Lowest Housing Growth (Permits per 1k Residents - HGI)\")\n\n\n\n\n\n\n\n\nTable 2: Top 5 Highest and Lowest Housing Growth Areas based on HGR Index\n\n\nCode\ndisplay_dt(HGR_top_bottom, \"Top 5 Highest and Lowest Housing Growth (Permits vs. 5-Year Pop Growth - HGR)\")\n\n\n\n\n\n\n\n\nTable 3: Top 5 Highest and Lowest Housing Growth Areas based on HGS Index\n\n\nCode\ndisplay_dt(HGS_top_bottom, \"Top 5 Highest and Lowest Composite Housing Growth Score (HGS)\")"
  },
  {
    "objectID": "mp02.html#preparation-and-calculation",
    "href": "mp02.html#preparation-and-calculation",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Preparation and Calculation:",
    "text": "Preparation and Calculation:\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(scales) \n\n# --- 1. Load and Prepare Core Data ---\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") %&gt;%\n  rename(total_population = B01003_001) %&gt;%\n  select(GEOID, NAME, year, total_population)\n\nINCOME &lt;- get_acs_all_years(\"B19013_001\") %&gt;%\n  rename(med_income = B19013_001) %&gt;%\n  select(GEOID, year, med_income)\n\nRENT &lt;- get_acs_all_years(\"B25064_001\") %&gt;%\n  rename(med_rent = B25064_001) %&gt;%\n  select(GEOID, year, med_rent)\n\nPERMITS &lt;- get_building_permits() %&gt;%\n  rename(GEOID = CBSA, new_permits = new_housing_units_permitted) %&gt;%\n  select(GEOID, year, new_permits)\n\n# --- 2. Calculate Rent Burden Index (RBI) ---\nrent_burden_data &lt;- inner_join(RENT, INCOME, by = c(\"GEOID\", \"year\"))\nrent_burden_data &lt;- rent_burden_data %&gt;%\n  mutate(RB_raw = (med_rent * 12) / med_income) %&gt;%\n  filter(RB_raw &gt; 0 & is.finite(RB_raw))\n\nRB_baseline_2009 &lt;- rent_burden_data %&gt;%\n  filter(year == 2009) %&gt;%\n  summarise(avg = mean(RB_raw, na.rm = TRUE)) %&gt;%\n  pull(avg)\n\nrent_burden_data &lt;- rent_burden_data %&gt;%\n  mutate(RBI_Index = RB_raw / RB_baseline_2009) %&gt;%\n  select(GEOID, year, RBI_Index)\n\n# --- 3. Calculate Housing Growth Score (HGS) ---\ngrowth_data &lt;- inner_join(POPULATION, PERMITS, by = c(\"GEOID\", \"year\"))\ngrowth_data &lt;- growth_data %&gt;%\n  arrange(GEOID, year) %&gt;%\n  group_by(GEOID) %&gt;%\n  mutate(\n    pop_5yr_ago = lag(total_population, n = 5, default = NA),\n    pop_growth_5yr = total_population - pop_5yr_ago\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt;= 2014)\n\nHGI_data &lt;- growth_data %&gt;%\n  mutate(HGI_raw = (new_permits / total_population) * 1000)\n\nHGI_baseline_2014 &lt;- HGI_data %&gt;%\n  filter(year == 2014) %&gt;%\n  summarise(avg = mean(HGI_raw, na.rm = TRUE)) %&gt;%\n  pull(avg)\n\nHGI_data &lt;- HGI_data %&gt;%\n  mutate(HGI_Index = HGI_raw / HGI_baseline_2014) %&gt;%\n  select(GEOID, year, HGI_Index)\n\nHGR_data &lt;- growth_data %&gt;%\n  filter(!is.na(pop_growth_5yr)) %&gt;%\n  mutate(HGR_raw = new_permits / (pop_growth_5yr + 1))\n\nHGR_baseline_2014 &lt;- HGR_data %&gt;%\n  filter(year == 2014) %&gt;%\n  summarise(avg = mean(HGR_raw[is.finite(HGR_raw)], na.rm = TRUE)) %&gt;%\n  pull(avg)\n\nHGR_data &lt;- HGR_data %&gt;%\n  mutate(\n    HGR_raw = ifelse(is.finite(HGR_raw), HGR_raw, NA),\n    HGR_Index = HGR_raw / HGR_baseline_2014\n  ) %&gt;%\n  select(GEOID, year, HGR_Index)\n\nhousing_growth_analysis &lt;- HGI_data %&gt;%\n  inner_join(HGR_data, by = c(\"GEOID\", \"year\")) %&gt;%\n  filter(!is.na(HGI_Index) & !is.na(HGR_Index)) %&gt;%\n  mutate(Housing_Growth_Score = (HGI_Index + HGR_Index) / 2)\n\n# --- 4. Merge All Metrics & Calculate YIMBY Criteria Variables (2009-2023) ---\nfull_analysis_data &lt;- inner_join(POPULATION, rent_burden_data, by = c(\"GEOID\", \"year\"))\nfull_analysis_data &lt;- full_analysis_data %&gt;%\n  inner_join(housing_growth_analysis, by = c(\"GEOID\", \"year\")) %&gt;%\n  arrange(GEOID, year)\n\nstart_year &lt;- min(full_analysis_data$year)\nend_year &lt;- max(full_analysis_data$year)\n\nyimby_criteria &lt;- full_analysis_data %&gt;%\n  filter(year == start_year | year == end_year | year &gt;= 2014) %&gt;%\n  group_by(GEOID, NAME) %&gt;%\n  summarise(\n    initial_RB_Index = RBI_Index[year == start_year],\n    RB_Index_change = RBI_Index[year == end_year] - RBI_Index[year == start_year],\n    pop_growth = total_population[year == end_year] - total_population[year == start_year],\n    avg_HGS_2014_2023 = mean(Housing_Growth_Score[year &gt;= 2014], na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  filter(!is.na(initial_RB_Index) & !is.na(RB_Index_change) & !is.na(pop_growth) & !is.na(avg_HGS_2014_2023))\n\nnational_avg_HGS_2014_2023 &lt;- mean(yimby_criteria$avg_HGS_2014_2023, na.rm = TRUE)\n\nyimby_cbsas &lt;- yimby_criteria %&gt;%\n  mutate(is_yimby = (initial_RB_Index &gt; 1.0) & (RB_Index_change &lt; 0) & (pop_growth &gt; 0) & (avg_HGS_2014_2023 &gt; national_avg_HGS_2014_2023)) %&gt;%\n  filter(is_yimby) %&gt;%\n  arrange(desc(avg_HGS_2014_2023))\n\n# Prepare data for plotting\nplot1_data &lt;- yimby_criteria %&gt;%\n  mutate(is_yimby_candidate = (initial_RB_Index &gt; 1.0) & (pop_growth &gt; 0))\n\ntop_yimby_names &lt;- head(yimby_cbsas, 5)$NAME\nplot1_data &lt;- plot1_data %&gt;%\n  mutate(label = ifelse(NAME %in% top_yimby_names, NAME, \"\"))\n\nplot2_data &lt;- full_analysis_data %&gt;%\n  filter(NAME %in% yimby_cbsas$NAME) %&gt;%\n  select(NAME, year, RBI_Index) %&gt;%\n  arrange(NAME, year)"
  },
  {
    "objectID": "mp02.html#visualization-1-scatter-plot-hgs-vs.-change-in-rent-burden",
    "href": "mp02.html#visualization-1-scatter-plot-hgs-vs.-change-in-rent-burden",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Visualization 1: Scatter Plot (HGS vs. Change in Rent Burden)",
    "text": "Visualization 1: Scatter Plot (HGS vs. Change in Rent Burden)\n\n\nCode\np1 &lt;- ggplot(plot1_data, aes(x = RB_Index_change, y = avg_HGS_2014_2023)) +\n  geom_line(aes(color = NAME), linewidth = 0.5) +\n  geom_point(aes(color = NAME)) +\n  geom_hline(yintercept = 1.0, linetype = \"dashed\", color = \"gray50\") +\n  scale_x_continuous(breaks = seq(start_year, end_year, by = 2)) +\n  labs(\n    title = paste0(\"Housing Growth vs. Change in Rent Burden Index (\", start_year, \" - \", end_year, \")\"),\n    subtitle = \"YIMBY Success: Initial RBI &gt; 1.0, Pop Growth &gt; 0, HGS &gt; National Avg, and RBI Change &lt; 0 (Top-Left, above gray line).\",\n    x = \"Change in Rent Burden Index (RBI) (Decrease is better, negative values mean success)\",\n    y = \"Average Housing Growth Score (HGS) (Higher is better)\",\n    caption = paste0(\"HGS Average Period: 2014-\", end_year, \". Horizontal Line: National Average HGS (\", round(national_avg_HGS_2014_2023, 2), \").\")\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", plot.caption = element_text(size = 2))\n\nprint(p1)"
  },
  {
    "objectID": "mp02.html#visualization-2-time-series-plot-rbi-trend-for-yimby-success-cbsas",
    "href": "mp02.html#visualization-2-time-series-plot-rbi-trend-for-yimby-success-cbsas",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Visualization 2: Time Series Plot (RBI Trend for YIMBY Success CBSAs)",
    "text": "Visualization 2: Time Series Plot (RBI Trend for YIMBY Success CBSAs)\n\n\nCode\np2 &lt;- ggplot(plot2_data, aes(x = year, y = RBI_Index, group = NAME)) +\n  geom_line(aes(color = NAME), linewidth = 0.5) +\n  geom_point(aes(color = NAME)) +\n  geom_hline(yintercept = 1.0, linetype = \"dashed\", color = \"gray50\") +\n  scale_x_continuous(breaks = seq(start_year, end_year, by = 2)) +\n  labs(\n    title = \"Rent Burden Index (RBI) Trend for Identified YIMBY Success CBSAs\",\n    subtitle = \"RBI is standardized to the 2009 National Average (RBI=1.0 dashed line).\",\n    x = \"Year\",\n    y = \"Rent Burden Index (RBI)\",\n    color = \"Metropolitan Area\"\n  ) +\n  theme_minimal() +\n  theme(\n    # 1. Move the legend to the bottom\n    legend.position = \"bottom\",\n    # 2. Make the legend key items tighter\n    legend.key.size = unit(0.00001, \"cm\"),\n    legend.text = element_text(size = 9)\n  ) +\n  \n  guides(color = guide_legend(ncol = 2, byrow = TRUE))\n\nprint(p2)"
  },
  {
    "objectID": "mp02.html#action-objective-securing-sponsors-and-core-support",
    "href": "mp02.html#action-objective-securing-sponsors-and-core-support",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Action Objective: Securing Sponsors and Core Support",
    "text": "Action Objective: Securing Sponsors and Core Support\nThe Pro-Growth Housing Incentives Act is a national grant program designed to reward and incentivize local zoning reform, boost housing supply, and lowering the Rent Burden Index (RBI) nationwide, with the end goal being to make housing affordable long term in all major CBSAs nationwide."
  },
  {
    "objectID": "mp02.html#congressional-sponsorship-strategy",
    "href": "mp02.html#congressional-sponsorship-strategy",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Congressional Sponsorship Strategy",
    "text": "Congressional Sponsorship Strategy\nTo maximize support, we must pair a sponsor whose success is proof of the program’s theory with a co-sponsor whose crisis underscores its necessity in CBSAs that demonstrate model strengths and shortcomings that this bill aims to solve.\n\nPrimary Sponsoring Area (Pro Growth CBSA): 26420, Houston-Sugar Land-Baytown, TX\nHouston demonstrates the power of pro-supply policies. Their Rent Burden Index (RBI) decreased (from 1.05 to 1.04) despite massive population growth, thanks to a high Housing Growth Score (HGS) of 1.56 (56% above the national average). The Act will allow Houston to scale its success and continue growing with ample housing to provide.\n\n\nCo-sponsoring Area (Advocate CBSA this bill aims to solve): 41940, San Jose-Sunnyvale-Santa Clara, CA\nSan Jose is the prime example of the crisis this bill solves. The city’s RBI has risen dramatically from 2009-2023 (from 1.52 to 1.63), coupled with a low HGS of just 0.58. The Act offers the necessary resources and incentives to break decades of systemic supply failure in Silicon Valley’s most populated area."
  },
  {
    "objectID": "mp02.html#building-momentum-through-securing-support",
    "href": "mp02.html#building-momentum-through-securing-support",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Building Momentum through Securing Support",
    "text": "Building Momentum through Securing Support\nSecuring support from local labor and trade unions is universally critical for passage in CBSAs nationwide. This bill specifically benefits two vocally engaged groups in society by focusing on jobs and cost relief that can flourish under this act.\n\nGroup 1: Construction & Building Unions\nThis Act provides funding directly tied to local permit reform and increased housing unit construction. This means immediate, stable, high-wage union and trade jobs in both metros and eventually nationwide. For San Jose, it means jump-starting a lagging housing sector. For Houston, it means federal support to maintain their current building boom.\n\n\nGroup 2: Service Industry Workers\nFor low and mid-wage workers-often the first victims of rising housing costs—The Act delivers meaningful economic relief amidst the changing landscape across America. By increasing housing supply (as seen in Houston’s stabilized RBI), we drive down the largest monthly expense-Rent. This effectively acts as a raise or tax cut, leaving service workers with more disposable income, which, in turn, boosts local businesses like restaurants and retail (where they also may work, thus providing a stronger sense of job security for those holding these positions)."
  },
  {
    "objectID": "mp02.html#targeting-funding-efficiently-across-cbsas",
    "href": "mp02.html#targeting-funding-efficiently-across-cbsas",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Targeting Funding Efficiently Across CBSAs",
    "text": "Targeting Funding Efficiently Across CBSAs\nTo define “YIMBY” success and in turn allocate funds in an appropriate manner proportionately nationwide, This Act uses two symbiotic indices:\n\nRent Burden Index (RBI):\nWhat it measures: Housing affordability stress, or the arbitrary threshold needed to live comfortably. It calculates the average annual rent relative to the average income in a given area.\nWhat is ‘Good’: A city is succeeding if its RBI is decreasing over time. This means housing is becoming more affordable relative to local wages, proving supply is meeting demand.\n\n\nHousing Growth Score (HGS):\nWhat it measures: The effectiveness and ambition of new housing construction in a given area. It combines raw permits per capita (Housing Growth Intensity) with permits relative to population change (Housing Growth Responsiveness).\nWhat is ‘Good’: A city is succeeding if its HGS is above the national average. This proves the city isn’t just growing, but is proactively changing zoning restrictions and ensuring future supply meets population need."
  },
  {
    "objectID": "mp02.html#conclusion",
    "href": "mp02.html#conclusion",
    "title": "Mini Project 2 – Making Backyards Affordable for All",
    "section": "Conclusion",
    "text": "Conclusion\nThis plans to reward cities that use the Housing Growth Score System to lower the Rent Burden Index that plagues their populus through providing assistance and incentives to cities that have a high RBI and a low HGS, giving them a clear path toward federal grant eligibility and eventually affordable and available housing for all."
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini Project 1 – Netflix Top 10",
    "section": "",
    "text": "Introduction to the project\nThis project has 2 datasets, One for the top ten shows of each country tracked (Country_Top 10) and the Top ten overall shows Globally (Global_Top_10), the analyses below help us interpret various facets about the data\n\n\nLoading the Packages and the Datasets\n\n\nCode\n# Load required packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(readr)\n  library(dplyr) \n  library(knitr)\n  library(DT)\n  library(stringr)\n  library(glue)\n  library(lubridate)\n})\n\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\n\n# Importing of Datasets for Analysis\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n\nRows: 8840 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME)\n\n\nRows: 411760 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Read - tell read_tsv to treat \"N/A\" as NA\n\nGLOBAL_TOP_10  &lt;- readr::read_tsv(GLOBAL_TOP_10_FILENAME,  na = c(\"N/A\"))\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 8840 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nCOUNTRY_TOP_10 &lt;- readr::read_tsv(COUNTRY_TOP_10_FILENAME, na = c(\"N/A\"))\n\n\nRows: 411760 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nTask 1: Data Acqusition; a quick look of the datasets we are working with\n\n\nCode\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(season_title = na_if(season_title, \"N/A\"))\n\nCOUNTRY_TOP_10 &lt;- COUNTRY_TOP_10 %&gt;%\n  mutate(season_title = na_if(season_title, \"N/A\"))\n\n# To Confirm\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,840\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; NA, NA, NA, \"aka Charlie Sheen: Season 1\", …\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n\n\nCode\nglimpse(COUNTRY_TOP_10)\n\n\nRows: 411,760\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Mule\", \"The Wrong Paris\", \"KPop Demon …\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Ma…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 1, 2, 14, 1, 1, 1, 2, 1, 5, 1, 2, 1, 7, 1, …\n\n\n\n\nTask 2: Data Cleaning\n\n\nCode\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(runtime_minutes = round(60 * runtime))\n\nstr(GLOBAL_TOP_10)\n\n\ntibble [8,840 × 10] (S3: tbl_df/tbl/data.frame)\n $ week                      : Date[1:8840], format: \"2025-09-21\" \"2025-09-21\" ...\n $ category                  : chr [1:8840] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8840] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8840] \"The Wrong Paris\" \"KPop Demon Hunters\" \"Ice Road: Vengeance\" \"aka Charlie Sheen\" ...\n $ season_title              : chr [1:8840] NA NA NA \"aka Charlie Sheen: Season 1\" ...\n $ weekly_hours_viewed       : num [1:8840] 38900000 35400000 14400000 21800000 10900000 7100000 7800000 6300000 5800000 4000000 ...\n $ runtime                   : num [1:8840] 1.78 1.67 1.88 3.03 1.7 ...\n $ weekly_views              : num [1:8840] 21800000 21200000 7600000 7200000 6400000 4500000 3900000 3400000 3100000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8840] 2 14 1 2 2 4 4 1 1 1 ...\n $ runtime_minutes           : num [1:8840] 107 100 113 182 102 95 120 110 114 86 ...\n\n\nCode\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,840\nColumns: 10\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; NA, NA, NA, \"aka Charlie Sheen: Season 1\", …\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n$ runtime_minutes            &lt;dbl&gt; 107, 100, 113, 182, 102, 95, 120, 110, 114,…\n\n\n\n\nTask 3: Data Import & Interpretation\n\n\nCode\nCOUNTRY_TOP_10\n\n\n# A tibble: 411,760 × 8\n   country_name country_iso2 week       category weekly_rank show_title         \n   &lt;chr&gt;        &lt;chr&gt;        &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;              \n 1 Argentina    AR           2025-09-21 Films              1 The Mule           \n 2 Argentina    AR           2025-09-21 Films              2 The Wrong Paris    \n 3 Argentina    AR           2025-09-21 Films              3 KPop Demon Hunters \n 4 Argentina    AR           2025-09-21 Films              4 She Said Maybe     \n 5 Argentina    AR           2025-09-21 Films              5 War Dogs           \n 6 Argentina    AR           2025-09-21 Films              6 Sonic the Hedgehog…\n 7 Argentina    AR           2025-09-21 Films              7 Ocean's 8          \n 8 Argentina    AR           2025-09-21 Films              8 Into the Wild      \n 9 Argentina    AR           2025-09-21 Films              9 Fall for Me        \n10 Argentina    AR           2025-09-21 Films             10 A Bright Lawyer    \n# ℹ 411,750 more rows\n# ℹ 2 more variables: season_title &lt;chr&gt;, cumulative_weeks_in_top_10 &lt;dbl&gt;\n\n\nCode\nGLOBAL_TOP_10\n\n\n# A tibble: 8,840 × 10\n   week       category   weekly_rank show_title season_title weekly_hours_viewed\n   &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                      &lt;dbl&gt;\n 1 2025-09-21 Films (En…           1 The Wrong… &lt;NA&gt;                    38900000\n 2 2025-09-21 Films (En…           2 KPop Demo… &lt;NA&gt;                    35400000\n 3 2025-09-21 Films (En…           3 Ice Road:… &lt;NA&gt;                    14400000\n 4 2025-09-21 Films (En…           4 aka Charl… aka Charlie…            21800000\n 5 2025-09-21 Films (En…           5 The Mule   &lt;NA&gt;                    10900000\n 6 2025-09-21 Films (En…           6 Unknown N… &lt;NA&gt;                     7100000\n 7 2025-09-21 Films (En…           7 The Thurs… &lt;NA&gt;                     7800000\n 8 2025-09-21 Films (En…           8 Sonic the… &lt;NA&gt;                     6300000\n 9 2025-09-21 Films (En…           9 War Dogs   &lt;NA&gt;                     5800000\n10 2025-09-21 Films (En…          10 Terror Co… &lt;NA&gt;                     4000000\n# ℹ 8,830 more rows\n# ℹ 4 more variables: runtime &lt;dbl&gt;, weekly_views &lt;dbl&gt;,\n#   cumulative_weeks_in_top_10 &lt;dbl&gt;, runtime_minutes &lt;dbl&gt;\n\n\nCode\n# Initial Data Exploration and Adjustments\n\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\nCode\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\n#Formating of Data\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    select(-season_title) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\n\n\nTask 4: Exploratory and Press Release Questions:\n\n\nCountries Netflix operates in:\n\n\nCode\nCOUNTRY_TOP_10 %&gt;% distinct(country_name) %&gt;% count()\n\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    94\n\n\n\n\nNon English Film with the most cumulative weeks in the global top ten:\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(category == \"Films (Non-English)\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(weeks_in_top10 = n_distinct(week), .groups = \"drop\") %&gt;%\n  arrange(desc(weeks_in_top10)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 2\n  show_title                     weeks_in_top10\n  &lt;chr&gt;                                   &lt;int&gt;\n1 All Quiet on the Western Front             23\n\n\n\n\nLongest Film in global top 10\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(str_detect(category, \"Films\")) %&gt;%\n  mutate(runtime_min = round(60 * runtime)) %&gt;%\n  arrange(desc(runtime_min)) %&gt;% slice(1) %&gt;%\n  select(show_title, runtime_min)\n\n\n# A tibble: 1 × 2\n  show_title                            runtime_min\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 Pushpa 2: The Rule (Reloaded Version)         224\n\n\n\n\nPrograms with most total hours per the four catagories\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  group_by(category, show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop_last\") %&gt;%\n  slice_max(total_hours, n = 1, with_ties = FALSE) %&gt;%\n  arrange(category)\n\n\n# A tibble: 4 × 3\n# Groups:   category [4]\n  category            show_title          total_hours\n  &lt;chr&gt;               &lt;chr&gt;                     &lt;dbl&gt;\n1 Films (English)     KPop Demon Hunters    559100000\n2 Films (Non-English) Society of the Snow   235900000\n3 TV (English)        Stranger Things      2967980000\n4 TV (Non-English)    Squid Game           5048300000\n\n\n\n\nTV Show with the Longest Run in a country’s Top 10:\n\n\nCode\nCOUNTRY_TOP_10 %&gt;%\n  group_by(country_name, show_title) %&gt;%\n  summarise(weeks = n_distinct(week), .groups = \"drop\") %&gt;%\n  arrange(desc(weeks)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 3\n  country_name show_title  weeks\n  &lt;chr&gt;        &lt;chr&gt;       &lt;int&gt;\n1 Pakistan     Money Heist   128\n\n\n\n\nCountry with fewer than 200 weeks recorded and the date it stopped:\n\n\nCode\n  COUNTRY_TOP_10 %&gt;%\n  group_by(country_name) %&gt;%\n  summarise(n_weeks = n_distinct(week),\n            last_week = max(week, na.rm = TRUE)) %&gt;%\n  filter(n_weeks &lt; 200)\n\n\n# A tibble: 1 × 3\n  country_name n_weeks last_week \n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;    \n1 Russia            35 2022-02-27\n\n\n\n\nTotal Viewership of Squid Game Across All Seasons:\n\n\nCode\n  GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, regex(\"Squid Game\", ignore_case=TRUE))) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  5310000000\n\n\n\n\nApprox Red Notice Views in 2021:\n\n\nCode\nGLOBAL_TOP_10 %&gt;%\n  filter(show_title == \"Red Notice\", year(week) == 2021) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1   396740000\n\n\n\n\nFilms that reached #1 but did not debut at the #1 spot (List below):\n\n\nCode\nus &lt;- COUNTRY_TOP_10 %&gt;% filter(country_name == \"United States\", str_detect(category, \"Film\"))\ndebuts &lt;- us %&gt;% arrange(week) %&gt;% group_by(show_title) %&gt;% slice(1) %&gt;% select(show_title, debut_rank = weekly_rank)\never_number1 &lt;- us %&gt;% group_by(show_title) %&gt;% summarise(ever1 = any(weekly_rank == 1))\nleft_join(debuts, ever_number1, by = \"show_title\") %&gt;%\n  filter(debut_rank &gt; 1, ever1)\n\n\n# A tibble: 45 × 3\n# Groups:   show_title [45]\n   show_title      debut_rank ever1\n   &lt;chr&gt;                &lt;dbl&gt; &lt;lgl&gt;\n 1 Aftermath                4 TRUE \n 2 American Made            9 TRUE \n 3 Blood Red Sky            5 TRUE \n 4 Bullet Train             2 TRUE \n 5 Day Shift                2 TRUE \n 6 Despicable Me 2          2 TRUE \n 7 Despicable Me 4          2 TRUE \n 8 Dog Gone                 4 TRUE \n 9 Don't Move               3 TRUE \n10 End of the Road          2 TRUE \n# ℹ 35 more rows\n\n\n\n\nTV show that hit the top 10 in the most countries in its debuting week (see below)\n\n\nCode\nCOUNTRY_TOP_10 %&gt;%\n  group_by(show_title, season_title, country_name) %&gt;%\n  summarise(first_week_in_country = min(week), .groups = \"drop\") %&gt;%\n  group_by(show_title, season_title) %&gt;%\n  summarise(first_debut_week = min(first_week_in_country),\n            n_countries_at_debut = n_distinct(country_name),\n            .groups = \"drop\") %&gt;%\n  arrange(desc(n_countries_at_debut)) %&gt;%\n  slice(1)\n\n\n# A tibble: 1 × 4\n  show_title         season_title          first_debut_week n_countries_at_debut\n  &lt;chr&gt;              &lt;chr&gt;                 &lt;date&gt;                          &lt;int&gt;\n1 All of Us Are Dead All of Us Are Dead: … 2022-01-30                         94\n\n\n\n\nPress Release 1: Stranger Things\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\n\n\nGLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Stranger Things\")) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  2967980000\n\n\nAfter four breakthrough seasons that redefined the culture surrounding original programming on Netflix, the critically acclaimed Stranger Things is gearing up for its fifth and final season at the end of 2025. In its most recent season, which was released between May and July 2022, the drama-filled horror show accumulated nearly 2 billion viewing hours on Netflix platforms, rising to the top of the ranks during that period for nearly half the year in total global viewership. Throughout its distinguished run, Stranger Things has accumulated approximatley 2,967,980,000 viewing hours since its release in July 2016, putting it at the top of Netflix’s original series and leading the English TV Category. Though in a close battle with Wednesday, another popular show in Netflix’s English market that was released in 2022, Stranger Things has nonetheless maintained its stance as perhaps Netflix’s most culturally significant English show over the last decade, and its fans are anxiously awaiting the conclusion of its mind bending and emotional story.\n\n\nPress Release 2: Indian Viewership\n\n\nCode\nlibrary(dplyr)\n\n  COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(total_weeks_in_top10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_weeks_in_top10)) %&gt;%\n  slice(1:5)\n\n\n# A tibble: 5 × 2\n  show_title                                        total_weeks_in_top10\n  &lt;chr&gt;                                                            &lt;dbl&gt;\n1 Money Heist                                                       1543\n2 Squid Game                                                        1153\n3 Wednesday                                                          694\n4 The Railway Men - The Untold Story Of Bhopal 1984                  465\n5 Khakee: The Bihar Chapter                                          435\n\n\nAs the second largest country in the world by population and the market Netflix truly wants to capitalize on, India stands out among other nations the Streaming App operates in as truly a unique story of success. Since our data began tracking in 2021, the Indian Market has seen over 1000 unique titles appear in global top ten charts, with many programs appearing despite having little to no presence elsewhere in the world, especially in Netflix’s largest market: The United States. After Observing the Trends in the given data, we can see that Netflix’s content diversity and viewership has increased significantly over time, with 39 different shows appearing each week in the top ten. With an estimated growing customer base of over 30 million, India is far outpacing most of the world in terms of subscriber growth and pure numbers. Recently, with international top shows such as Squid Game as well as India’s top domestic programs such as Dabba Cartel and Saare Jahan Se Accha: The Silent Guardians leading charts for multiple weeks, it seems that Netflix will only grow in this massive market with many producers signing exclusive rights with the platform, such as the Great Indian Kapil Show. Additionally, the Hindu Language shows of Netflix are also seeing a rise in viewership globally largely as a result of Netlfix’s success in India, just going to show how impactful and influential this market is to Netflix’s global success\n\n\nPress Release 3: The global dominace of Squid Game\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(lubridate)\n# Filter for Squid Game only\nsquid_data &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Squid Game\"))\n# 1. Overall stats\noverall_stats &lt;- squid_data %&gt;%\n  summarise(\n    total_weeks = n_distinct(week),\n    avg_weekly_viewers = mean(weekly_views, na.rm = TRUE)\n  )\n# 2. Season-by-season stats\nseason_stats &lt;- squid_data %&gt;%\n  group_by(season_title) %&gt;%\n  summarise(\n    avg_weekly_viewers = mean(weekly_views, na.rm = TRUE),\n    total_weeks = n_distinct(week)\n  ) %&gt;%\n  arrange(season_title)\n# 3. Combine seasons + overall into one table\nfinal_summary &lt;- bind_rows(\n  season_stats,\n  tibble(season_title = \"Overall\", \n         avg_weekly_viewers = overall_stats$avg_weekly_viewers, \n         total_weeks = overall_stats$total_weeks)\n)\n# Display nicely\nprint(final_summary)\n\n\n# A tibble: 5 × 3\n  season_title                        avg_weekly_viewers total_weeks\n  &lt;chr&gt;                                            &lt;dbl&gt;       &lt;int&gt;\n1 Squid Game: Season 1                          4416667.          32\n2 Squid Game: Season 2                         14392857.          14\n3 Squid Game: Season 3                         15822222.           9\n4 Squid Game: The Challenge: Season 1           8520000            5\n5 Overall                                      10987500           45\n\n\nSince its release in September 2021, Hwang Dong-hyuk’s game show Squid Game has taken over the world by storm, leading global charts and positioning itself as a global cultural phenomenon. Throughout its 3 seasons, the show has averaged a remarkable 11340000 weekly viewers when it has cracked the global top ten weekly rankings through an equally impressive 50 week presence in the global top ten over the last 5 years. Individually, while the the Korean game show’s first season was undisputedly the peak of the franchise, averaging 4,416,667 weekly views during its peak season, the following seasons were still potent leaders in global viewships, albeit in shorter periods due to a shorter hiatus span of 1 year compared to 3 and global pandemic restrictions easing over that 3 year span, which initially allowed many Netflix subscribers to watch their shows far more frequently as more were in front of their tvs. As a whole, Netflix’s peak viewership across all shows during the data collection period was during the later half of the global pandemic (Start of data - end of 2022), which no show benefitted more from than Squid Game, as it allowed it to become one of the world’s most distinguished shows when all the eyeballs in the world wanted action following a dormant 2 years of societal isolation."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "",
    "text": "The Bureau of Labor Statistics (BLS) releases the Current Employment Statistics Report (CES) on a monthly basis, which is the backbone behind mainstream news coverage and all the jargon that comes with it. The CES covers how many people are employed each month in the United States.\nAs more data is released to the BLS, these number are later revised, which is viewed as highly controversial amongst the US Populous due to claims of manipulation both on the corporate and government side of things to drive up conversation."
  },
  {
    "objectID": "mp04.html#package-import",
    "href": "mp04.html#package-import",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Package Import",
    "text": "Package Import\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(DiagrammeR)\nlibrary(gt)\nlibrary(DT)\nlibrary(htmltools)\nlibrary(knitr)\nlibrary(scales)\nlibrary(purrr)"
  },
  {
    "objectID": "mp04.html#bls-data-extraction",
    "href": "mp04.html#bls-data-extraction",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "BLS Data Extraction",
    "text": "BLS Data Extraction\n\n\nCode\n# Perform Request\n\nresp &lt;- req_perform(req)\n\n# Parse HTML and extract table \nhtml &lt;- resp_body_html(resp, encoding = \"UTF-8\")\n\n# pick table with most rows (usually the main data table)\ntables &lt;- html %&gt;% html_nodes(\"table\")\ntr_counts &lt;- map_int(tables, ~ length(html_nodes(.x, \"tr\")))\ntable_node &lt;- tables[[which.max(tr_counts)]]\n\nraw_tbl &lt;- table_node %&gt;% html_table(fill = TRUE)\n\n# Fix first column name if blank\nif (names(raw_tbl)[1] == \"\" | is.na(names(raw_tbl)[1])) {\n  names(raw_tbl)[1] &lt;- \"year\"\n}\nnames(raw_tbl)[1] &lt;- \"year\"\n\n# Pivot and clean\nces_payroll &lt;- raw_tbl %&gt;%\n  pivot_longer(cols = -year, names_to = \"month\", values_to = \"level_raw\") %&gt;%\n  mutate(\n    month  = str_trim(month),\n    ym_str = paste(year, month),\n    date   = suppressWarnings(ym(ym_str)),\n    date   = if_else(\n               is.na(date),\n               suppressWarnings(parse_date_time(ym_str, orders = c(\"Y b\", \"Y B\", \"Y m\"))),\n               date\n             ),\n    date   = as.Date(date),\n    level = suppressWarnings(parse_number(level_raw))\n  ) %&gt;%\n  select(date, level) %&gt;%\n  drop_na(date, level) %&gt;%\n  arrange(date) %&gt;%\n  filter(date &lt;= as_date(\"2025-06-01\"))"
  },
  {
    "objectID": "mp04.html#loading-and-cleaning-the-data-for-exporatory-analysis",
    "href": "mp04.html#loading-and-cleaning-the-data-for-exporatory-analysis",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Loading and Cleaning the Data for Exporatory analysis",
    "text": "Loading and Cleaning the Data for Exporatory analysis\nNow that we possess both the raw and the revised datasets, we are ready to bring them together and perform a true analysis of the historic employment levels and their changes over time. As this data is not perfect and incomplete in some places after scraping, it must be cleaned and properly structured before going through the verification and later the analysis portion of MP04. Such discrepancies present in the data include entries that are outside the 1979-2025 scope and irregular entries that need to be altered in oder to be read and analyzed properly. Note that level means total recorded employement during this span.\n\nLoad CES Payroll Data\n\n\nCode\nlibrary(DT)\n\n# Display summary\ncat(\"Payroll data summary:\\n\")\n\n\nPayroll data summary:\n\n\nCode\ncat(\"Rows:\", nrow(ces_payroll), \"\\n\")\n\n\nRows: 558 \n\n\nCode\ncat(\"Date range:\", min(ces_payroll$date), \"to\", max(ces_payroll$date), \"\\n\\n\")\n\n\nDate range: 3287 to 20240 \n\n\nCode\n# Display as interactive datatable\ndatatable(ces_payroll, \n          options = list(pageLength = 10, scrollX = TRUE),\n          caption = \"CES Total Nonfarm Payroll (1979-2025)\")\n\n\n\n\n\n\n\n\nLoad CES Revisions Data\n\n\nCode\n# Display summary\ncat(\"Revisions data summary:\\n\")\n\n\nRevisions data summary:\n\n\nCode\ncat(\"Rows:\", nrow(ces_revisions), \"\\n\")\n\n\nRows: 558 \n\n\nCode\ncat(\"Date range:\", min(ces_revisions$date), \"to\", max(ces_revisions$date), \"\\n\\n\")\n\n\nDate range: 3287 to 20240 \n\n\nCode\n# Display as interactive datatable\ndatatable(ces_revisions,\n          options = list(pageLength = 10, scrollX = TRUE),\n          caption = \"CES Monthly Revisions (1979-2025)\")\n\n\n\n\n\n\n\n\nCombine Payroll and Revisions\n\n\nCode\n# Join the two datasets on date\nces_combined &lt;- ces_payroll %&gt;%\n  left_join(ces_revisions, by = \"date\") %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  arrange(date)\n\n# Display combined dataset summary\ncat(\"Combined dataset summary:\\n\")\n\n\nCombined dataset summary:\n\n\nCode\ncat(\"Rows:\", nrow(ces_combined), \"\\n\")\n\n\nRows: 558 \n\n\nCode\ncat(\"Columns:\", paste(colnames(ces_combined), collapse = \", \"), \"\\n\\n\")\n\n\nColumns: date, level, original, final, revision \n\n\nCode\n# Display as interactive datatable\ndatatable(ces_combined,\n          options = list(pageLength = 10, scrollX = TRUE),\n          caption = \"Combined CES Payroll and Revisions Data\")"
  },
  {
    "objectID": "mp04.html#statisitcal-ces-statisitics",
    "href": "mp04.html#statisitcal-ces-statisitics",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Statisitcal CES Statisitics:",
    "text": "Statisitcal CES Statisitics:\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\n\n# Create ces_stats from ces_combined\nces_stats &lt;- ces_combined |&gt;\n  filter(!is.na(revision)) |&gt;\n  mutate(\n    year = year(date),\n    decade = floor(year / 10) * 10,\n    abs_revision = abs(revision),\n    pct_revision = (abs_revision / final) * 100,\n    positive_revision = revision &gt; 0\n  )\n\n# Prepare data with era classification\nces_analysis &lt;- ces_stats |&gt;\n  mutate(\n    era = case_when(\n      year &lt; 2000 ~ \"Pre-2000\",\n      year &gt;= 2000 & year &lt; 2020 ~ \"2000-2019\",\n      year &gt;= 2020 ~ \"Post-2020\"\n    )\n  )\n\n# Create ces_stats from ces_combined\nces_stats &lt;- ces_combined |&gt;\n  filter(!is.na(revision)) |&gt;\n  mutate(\n    year = year(date),\n    decade = floor(year / 10) * 10,\n    abs_revision = abs(revision),\n    pct_revision = (abs_revision / final) * 100,\n    positive_revision = revision &gt; 0\n  )\n\n# Prepare data with era classification\nces_analysis &lt;- ces_stats |&gt;\n  mutate(\n    era = case_when(\n      year &lt; 2000 ~ \"Pre-2000\",\n      year &gt;= 2000 & year &lt; 2020 ~ \"2000-2019\",\n      year &gt;= 2020 ~ \"Post-2020\"\n    )\n  )\n\n\n# Add presidential partisan data\n\nlibrary(tidyverse)\npresidents_party &lt;- tidyr::expand_grid(year=1979:2025, \n                                       month = month.name, \n                                       president = NA, \n                                       party = NA) |&gt; \n    mutate(president = case_when(\n        (month == \"January\")  & (year == 1979) ~ \"Carter\",\n        # BLS jobs reports come out on the first Friday, so February\n        # is the first time a new president 'owns' the jobs number \n        (month == \"February\") & (year == 1981) ~ \"Reagan\",\n        (month == \"February\") & (year == 1989) ~ \"Bush 41\",\n        (month == \"February\") & (year == 1993) ~ \"Clinton\",\n        (month == \"February\") & (year == 2001) ~ \"Bush 43\",\n        (month == \"February\") & (year == 2009) ~ \"Obama\",\n        (month == \"February\") & (year == 2017) ~ \"Trump I\",\n        (month == \"February\") & (year == 2021) ~ \"Biden\",\n        (month == \"February\") & (year == 2025) ~ \"Trump II\",\n    )) |&gt;\n    tidyr::fill(president) |&gt;\n    mutate(party = if_else(president %in% c(\"Carter\", \"Clinton\", \"Obama\", \"Biden\"), \n                           \"D\", \n                           \"R\")) \n\n\n\nAnalysis 1: Largest Positive and Negative Revisions by Era\n\n\nCode\nlargest_by_era &lt;- ces_analysis |&gt;\n  filter(!is.na(revision)) |&gt;\n  group_by(era) |&gt;\n  summarise(\n    max_positive = max(revision),\n    min_negative = min(revision),\n    mean_abs_revision = mean(abs_revision),\n    median_abs_revision = median(abs_revision),\n    sd_abs_revision = sd(abs_revision),\n    n_months = n(),\n    .groups = 'drop'\n  ) |&gt;\n  arrange(factor(era, levels = c(\"Pre-2000\", \"2000-2019\", \"Post-2020\")))\n\ndatatable(largest_by_era, \n          caption = \"Largest Revisions and Summary Statistics by Era\",\n          options = list(pageLength = 10))\n\n\n\n\n\n\nCode\n# Find specific dates of largest revisions in each era\ncat(\"\\n\\nDetailed breakdown of largest revisions:\\n\\n\")\n\n\n\n\nDetailed breakdown of largest revisions:\n\n\nCode\nfor (e in c(\"Pre-2000\", \"2000-2019\", \"Post-2020\")) {\n  cat(\"--- \", e, \" ---\\n\")\n  \n  era_data &lt;- ces_analysis |&gt;\n    filter(era == e, !is.na(revision))\n  \n  largest_pos &lt;- era_data |&gt;\n    filter(revision == max(revision)) |&gt;\n    select(date, revision, final, pct_revision) |&gt;\n    mutate(type = \"Largest Positive\")\n  \n  largest_neg &lt;- era_data |&gt;\n    filter(revision == min(revision)) |&gt;\n    select(date, revision, final, pct_revision) |&gt;\n    mutate(type = \"Largest Negative\")\n  \n  print(bind_rows(largest_pos, largest_neg))\n  cat(\"\\n\")\n}\n\n\n---  Pre-2000  ---\n# A tibble: 2 × 5\n  date       revision final pct_revision type            \n  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;           \n1 1983-09-01      383  1116         34.3 Largest Positive\n2 1980-05-01     -303  -483        -62.7 Largest Negative\n\n---  2000-2019  ---\n# A tibble: 2 × 5\n  date       revision final pct_revision type            \n  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;           \n1 2006-09-01      152   203         74.9 Largest Positive\n2 2008-09-01     -244  -403        -60.5 Largest Negative\n\n---  Post-2020  ---\n# A tibble: 2 × 5\n  date       revision final pct_revision type            \n  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;           \n1 2021-11-01      437   647         67.5 Largest Positive\n2 2020-03-01     -672 -1373        -48.9 Largest Negative\n\n\n\n\nAnalysis 2: Top 10 Largest Negative Revisions (All Time)\n\n\nCode\nlibrary(tidyverse)\nlibrary(DT)\n\ntop_10_negative &lt;- ces_analysis |&gt;\n  mutate(\n    year = year(date),\n    month_abbr = as.character(month(date, label = TRUE)),\n    month = month.name[month(date)]  # Convert abbreviation to full name\n  ) |&gt;\n  left_join(\n    presidents_party |&gt;\n      mutate(month = as.character(month)),\n    by = c(\"year\" = \"year\", \"month\" = \"month\")\n  ) |&gt;\n  filter(!is.na(revision)) |&gt;\n  arrange(revision) |&gt;\n  slice(1:50) |&gt;\n  select(date, year, month, president, party, revision, final, pct_revision, era) |&gt;\n  mutate(rank = row_number())\n\ndatatable(top_10_negative,\n          caption = \"Top 50 Largest Negative (Downward) Revisions\",\n          options = list(pageLength = 10))\n\n\n\n\n\n\n\n\nAnalysis 3: Pre-2020 vs Post-2020 Average % Revision Comparison\n\n\nCode\ncat(\"(DIRECTLY ADDRESSES TRUMP'S CONCERN OF COOKING THE BOOKS)\\n\\n\")\n\n\n(DIRECTLY ADDRESSES TRUMP'S CONCERN OF COOKING THE BOOKS)\n\n\nCode\nera_comparison &lt;- ces_analysis |&gt;\n  filter(!is.na(revision) & final &gt; 0) |&gt;  # Add filter for final &gt; 0\n  mutate(pre_post = if_else(year &lt; 2020, \"Pre-2020\", \"Post-2020\")) |&gt;\n  group_by(pre_post) |&gt;\n  summarise(\n    mean_revision = mean(revision, na.rm = TRUE),\n    mean_abs_revision = mean(abs_revision, na.rm = TRUE),\n    mean_pct_revision = mean(pct_revision, na.rm = TRUE),\n    median_abs_revision = median(abs_revision, na.rm = TRUE),\n    sd_pct_revision = sd(pct_revision, na.rm = TRUE),\n    n_months = n(),\n    .groups = 'drop'\n  )\n\ndatatable(era_comparison,\n          caption = \"Pre-2020 vs Post-2020 Revision Comparison\",\n          options = list(pageLength = 10))\n\n\n\n\n\n\nCode\n# Calculate the difference\npre_2020_mean &lt;- era_comparison |&gt; filter(pre_post == \"Pre-2020\") |&gt; pull(mean_pct_revision)\npost_2020_mean &lt;- era_comparison |&gt; filter(pre_post == \"Post-2020\") |&gt; pull(mean_pct_revision)\ndifference &lt;- post_2020_mean - pre_2020_mean\npct_increase &lt;- (difference / pre_2020_mean) * 100\n\ncat(\"\\n\\nKEY FINDING:\\n\")\n\n\n\n\nKEY FINDING:\n\n\nCode\ncat(\"Pre-2020 average % revision:\", round(pre_2020_mean, 4), \"%\\n\")\n\n\nPre-2020 average % revision: 38.4443 %\n\n\nCode\ncat(\"Post-2020 average % revision:\", round(post_2020_mean, 4), \"%\\n\")\n\n\nPost-2020 average % revision: 34.0598 %\n\n\nCode\ncat(\"Difference:\", round(difference, 4), \"percentage points\\n\")\n\n\nDifference: -4.3845 percentage points\n\n\nCode\ncat(\"Percent decrease:\", round(pct_increase, 2), \"%\\n\")\n\n\nPercent decrease: -11.4 %\n\n\n\n\nAnalysis 4: Which Calendar Months Overall Have Highest % Positive Revisions?\n\n\nCode\nmonthly_pattern &lt;- ces_analysis |&gt;\n  filter(!is.na(revision)) |&gt;\n  mutate(month_name = month(date, label = TRUE)) |&gt;\n  group_by(month_name) |&gt;\n  summarise(\n    pct_positive = mean(positive_revision) * 100,\n    pct_negative = (1 - mean(positive_revision)) * 100,\n    n_total = n(),\n    mean_abs_revision = mean(abs_revision),\n    .groups = 'drop'\n  ) |&gt;\n  arrange(desc(pct_positive))\n\ndatatable(monthly_pattern,\n          caption = \"Seasonal Patterns: % of Positive vs Negative Revisions by Month\",\n          options = list(pageLength = 12))\n\n\n\n\n\n\nCode\ncat(\"\\n\\nINTERPRETATION:\\n\")\n\n\n\n\nINTERPRETATION:\n\n\nCode\ncat(\"Months with HIGHEST % positive revisions (upward revisions):\\n\")\n\n\nMonths with HIGHEST % positive revisions (upward revisions):\n\n\nCode\nprint(head(monthly_pattern |&gt; select(month_name, pct_positive, n_total), 3))\n\n\n# A tibble: 3 × 3\n  month_name pct_positive n_total\n  &lt;ord&gt;             &lt;dbl&gt;   &lt;int&gt;\n1 Sep                80.4      46\n2 Aug                65.2      46\n3 Jul                63.0      46\n\n\nCode\ncat(\"\\n\\nMonths with LOWEST % positive revisions (more downward revisions):\\n\")\n\n\n\n\nMonths with LOWEST % positive revisions (more downward revisions):\n\n\nCode\nprint(tail(monthly_pattern |&gt; select(month_name, pct_positive, n_total), 3))\n\n\n# A tibble: 3 × 3\n  month_name pct_positive n_total\n  &lt;ord&gt;             &lt;dbl&gt;   &lt;int&gt;\n1 Feb                46.8      47\n2 Dec                45.7      46\n3 Oct                41.3      46\n\n\nCode\ncat(\"\\n\\nPOSSIBLE EXPLANATIONS:\\n\")\n\n\n\n\nPOSSIBLE EXPLANATIONS:\n\n\nCode\ncat(\"- January: Preliminary estimates made in winter with incomplete data\\n\")\n\n\n- January: Preliminary estimates made in winter with incomplete data\n\n\nCode\ncat(\"- August/September: Summer hiring changes, back-to-school employment\\n\")\n\n\n- August/September: Summer hiring changes, back-to-school employment\n\n\nCode\ncat(\"- December/January: Holiday season employment fluctuations\\n\")\n\n\n- December/January: Holiday season employment fluctuations\n\n\nCode\ncat(\"- First month of quarter (Jan, April, July, October): Catching up on prior quarter data issues\\n\")\n\n\n- First month of quarter (Jan, April, July, October): Catching up on prior quarter data issues\n\n\n\n\nAnalysis 5 : Annual Rolling Standard Deviation by Era (Revision Volatility):\nINTERPRETATION:\nHigher rolling SD = revisions are more unpredictable\nIf Post-2020 &gt; Pre-2020, suggests recent revisions are less stable\nIf Post-2020 &lt; Pre-2020, suggests recent revisions follow more predictable patterns\n\n\nCode\n# Calculate 12-month rolling standard deviation manually\nrolling_volatility &lt;- ces_stats %&gt;%\n  arrange(date) %&gt;%\n  filter(!is.na(revision)) %&gt;%\n  mutate(\n    year = year(date),\n    # Create lag columns for 12-month window\n    rolling_sd_revision = sapply(row_number(), function(i) {\n      start_idx &lt;- max(1, i - 11)\n      if (i &lt; 12) return(NA_real_)\n      sd(revision[start_idx:i], na.rm = TRUE)\n    }),\n    rolling_sd_abs = sapply(row_number(), function(i) {\n      start_idx &lt;- max(1, i - 11)\n      if (i &lt; 12) return(NA_real_)\n      sd(abs_revision[start_idx:i], na.rm = TRUE)\n    }),\n    rolling_sd_pct = sapply(row_number(), function(i) {\n      start_idx &lt;- max(1, i - 11)\n      if (i &lt; 12) return(NA_real_)\n      sd(pct_revision[start_idx:i], na.rm = TRUE)\n    })\n  ) %&gt;%\n  drop_na(rolling_sd_revision)\n\n# Summary statistics by era\ncat(\"\\n\\n=== VOLATILITY STATISTICS BY ERA ===\\n\\n\")\n\n\n\n\n=== VOLATILITY STATISTICS BY ERA ===\n\n\nCode\nvolatility_by_era &lt;- rolling_volatility %&gt;%\n  mutate(\n    era = case_when(\n      year &lt; 2000 ~ \"Pre-2000\",\n      year &gt;= 2000 & year &lt; 2020 ~ \"2000-2019\",\n      year &gt;= 2020 ~ \"Post-2020\"\n    )\n  ) %&gt;%\n  group_by(era) %&gt;%\n  summarise(\n    mean_volatility_raw = mean(rolling_sd_revision, na.rm = TRUE),\n    median_volatility_raw = median(rolling_sd_revision, na.rm = TRUE),\n    max_volatility_raw = max(rolling_sd_revision, na.rm = TRUE),\n    min_volatility_raw = min(rolling_sd_revision, na.rm = TRUE),\n    mean_volatility_abs = mean(rolling_sd_abs, na.rm = TRUE),\n    mean_volatility_pct = mean(rolling_sd_pct, na.rm = TRUE),\n    n_months = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(factor(era, levels = c(\"Pre-2000\", \"2000-2019\", \"Post-2020\")))\n\ndatatable(volatility_by_era,\n          caption = \"Volatility Statistics by Era\",\n          options = list(pageLength = 10))\n\n\n\n\n\n\nCode\n# Year-by-year volatility\ncat(\"\\n\\n=== AVERAGE VOLATILITY BY YEAR ===\\n\\n\")\n\n\n\n\n=== AVERAGE VOLATILITY BY YEAR ===\n\n\nCode\nvolatility_by_year &lt;- rolling_volatility %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    mean_sd_revision = mean(rolling_sd_revision, na.rm = TRUE),\n    mean_sd_abs = mean(rolling_sd_abs, na.rm = TRUE),\n    mean_sd_pct = mean(rolling_sd_pct, na.rm = TRUE),\n    n_months = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(mean_sd_revision))\n\ndatatable(volatility_by_year,\n          caption = \"Average Volatility by Year\",\n          options = list(pageLength = 20))\n\n\n\n\n\n\nCode\n# Key insights\ncat(\"\\n\\n=== KEY FINDINGS ===\\n\\n\")\n\n\n\n\n=== KEY FINDINGS ===\n\n\nCode\npre_2020_vol &lt;- volatility_by_era %&gt;%\n  filter(era %in% c(\"Pre-2000\", \"2000-2019\")) %&gt;%\n  pull(mean_volatility_raw) %&gt;%\n  mean()\n\npost_2020_vol &lt;- volatility_by_era %&gt;%\n  filter(era == \"Post-2020\") %&gt;%\n  pull(mean_volatility_raw)\n\nvolatility_increase &lt;- ((post_2020_vol - pre_2020_vol) / pre_2020_vol) * 100\n\ncat(\"Pre-2020 average volatility (SD of revisions):\", round(pre_2020_vol, 4), \"\\n\")\n\n\nPre-2020 average volatility (SD of revisions): 64.5099 \n\n\nCode\ncat(\"Post-2020 average volatility (SD of revisions):\", round(post_2020_vol, 4), \"\\n\")\n\n\nPost-2020 average volatility (SD of revisions): 110.5064 \n\n\nCode\ncat(\"Increase:\", round(volatility_increase, 2), \"%\\n\\n\")\n\n\nIncrease: 71.3 %\n\n\nCode\n# Most volatile year\nmost_volatile_year &lt;- volatility_by_year %&gt;% slice_max(mean_sd_revision, n = 1)\ncat(\"Most volatile year:\", most_volatile_year$year, \n    \"with avg volatility of\", round(most_volatile_year$mean_sd_revision, 4), \"\\n\")\n\n\nMost volatile year: 2020 with avg volatility of 190.9841 \n\n\nCode\n# Least volatile year\nleast_volatile_year &lt;- volatility_by_year %&gt;% slice_min(mean_sd_revision, n = 1)\ncat(\"Least volatile year:\", least_volatile_year$year,\n    \"with avg volatility of\", round(least_volatile_year$mean_sd_revision, 4), \"\\n\")\n\n\nLeast volatile year: 2016 with avg volatility of 28.6641 \n\n\n\n\nAnalysis 6: Normalized Revision Ratio (Revision / Employment Level)\nKEY INSIGHT:\nThis addresses the ‘working population is larger’ argument:\nIf revisions are growing WITH the employment level, normalized ratios should be stable\nIf normalized ratios are increasing, revisions are growing faster than employment\n\n\nCode\ncat(\"=== NORMALIZED REVISION RATIO: REVISION AS % OF EMPLOYMENT LEVEL ===\\n\\n\")\n\n\n=== NORMALIZED REVISION RATIO: REVISION AS % OF EMPLOYMENT LEVEL ===\n\n\nCode\nnormalized_analysis &lt;- ces_analysis |&gt;\n  filter(!is.na(revision) & !is.na(level)) |&gt;\n  mutate(\n    normalized_ratio = abs_revision / level,\n    pct_of_level = (abs_revision / level) * 100\n  ) |&gt;\n  group_by(era) |&gt;\n  summarise(\n    mean_normalized_ratio = mean(normalized_ratio, na.rm = TRUE),\n    median_normalized_ratio = median(normalized_ratio, na.rm = TRUE),\n    mean_pct_of_level = mean(pct_of_level, na.rm = TRUE),\n    sd_pct_of_level = sd(pct_of_level, na.rm = TRUE),\n    max_pct_of_level = max(pct_of_level, na.rm = TRUE),\n    n_months = n(),\n    .groups = 'drop'\n  ) |&gt;\n  arrange(factor(era, levels = c(\"Pre-2000\", \"2000-2019\", \"Post-2020\")))\n\ndatatable(normalized_analysis,\n          caption = \"Normalized Revision Ratios by Era\",\n          options = list(pageLength = 10))\n\n\n\n\n\n\nCode\n# Trend analysis\npre_2020_norm &lt;- normalized_analysis |&gt; \n  filter(era %in% c(\"Pre-2000\", \"2000-2019\")) |&gt; \n  pull(mean_pct_of_level) |&gt;\n  mean()\n\npost_2020_norm &lt;- normalized_analysis |&gt; \n  filter(era == \"Post-2020\") |&gt; \n  pull(mean_pct_of_level)\n\ncat(\"Pre-2020 average normalized ratio:\", round(pre_2020_norm, 4), \"%\\n\")\n\n\nPre-2020 average normalized ratio: 0.0466 %\n\n\nCode\ncat(\"Post-2020 normalized ratio:\", round(post_2020_norm, 4), \"%\\n\")\n\n\nPost-2020 normalized ratio: 0.0584 %\n\n\nCode\ncat(\"Difference:\", round(post_2020_norm - pre_2020_norm, 4), \"percentage points or 1.18%\\n\")\n\n\nDifference: 0.0118 percentage points or 1.18%"
  },
  {
    "objectID": "mp04.html#visual-ces-analysis",
    "href": "mp04.html#visual-ces-analysis",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Visual CES Analysis:",
    "text": "Visual CES Analysis:\n\nVisualization 1: Heatmap - Year × Month Revision Magnitude\nINTERPRETATION:\nDarker colors = larger revision magnitudes\nLook for entire rows that are darker = that year had consistently large revisions\nLook for columns that are darker = that month tends to have large revisions\nCompare 2020-2025 to earlier years to see if recent period looks anomalous\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\n\n# Prepare data for heatmap\nheatmap_data &lt;- ces_stats %&gt;%\n  filter(!is.na(revision)) %&gt;%\n  mutate(\n    year = year(date),\n    month_num = month(date),\n    month_name = month(date, label = TRUE)\n  ) %&gt;%\n  select(year, month_num, month_name, abs_revision) %&gt;%\n  arrange(year, month_num)\n\n# Create heatmap\np_heatmap &lt;- ggplot(heatmap_data, aes(x = month_name, y = year, fill = abs_revision)) +\n  geom_tile(color = \"white\", linewidth = 0.2) +\n  scale_fill_gradient(\n    low = \"white\",\n    high = \"darkred\",\n    name = \"Absolute Revision\\n(thousands)\"\n  ) +\n  scale_y_continuous(breaks = seq(1979, 2025, by = 5), limits = c(1978.5, 2025.5)) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank(),\n    legend.position = \"right\"\n  ) +\n  labs(\n    title = \"CES Revision Magnitude Heatmap: Year × Month (1979-2025)\",\n    subtitle = \"Darker red = larger revisions | Look for anomalous years/months\",\n    x = \"Month\",\n    y = \"Year\",\n    caption = \"Source: BLS CES Data\"\n  )\n\nprint(p_heatmap)\n\n\n\n\n\n\n\n\n\n\n\nVisualization 2: Line Graph - Monthly Revisions Over Time\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\n\n# Prepare data with month and year\nviz2_data &lt;- ces_stats |&gt;\n  filter(!is.na(revision)) |&gt;\n  mutate(\n    month_name = month(date, label = TRUE),\n    month_num = month(date),\n    year = year(date)\n  )\n\n# Create line plot: one line per month, showing revisions over the years\np2 &lt;- ggplot(viz2_data, aes(x = year, y = revision, color = month_name, group = month_name)) +\n  geom_line(linewidth = 0.8, alpha = 0.7) +\n  geom_point(size = 1.5, alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\", linewidth = 0.5) +\n  scale_color_viridis_d(option = \"turbo\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    legend.position = \"right\",\n    legend.title = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    title = \"Monthly Revisions Over Time by Calendar Month (1979-2025)\",\n    subtitle = \"Each colored line represents one month (Jan-Dec) showing its revision pattern across 46 years\",\n    x = \"Year\",\n    y = \"Revision (thousands)\",\n    color = \"Month\",\n    caption = \"Source: BLS CES Data\"\n  )\n\nprint(p2)\n\n\n\n\n\n\n\n\n\nCode\n# Summary: average revision for each month across all years\ncat(\"\\n\\nAverage Revision by Month (Across All Years 1979-2025):\\n\")\n\n\n\n\nAverage Revision by Month (Across All Years 1979-2025):\n\n\nCode\nmonthly_summary &lt;- viz2_data |&gt;\n  group_by(month_name) |&gt;\n  summarise(\n    mean_revision = mean(revision),\n    median_revision = median(revision),\n    sd_revision = sd(revision),\n    min_revision = min(revision),\n    max_revision = max(revision),\n    n_years = n(),\n    .groups = 'drop'\n  ) |&gt;\n  arrange(factor(month_name, levels = month.abb))\n\ndatatable(monthly_summary,\n          caption = \"Average Revision by Month (1979-2025)\",\n          options = list(pageLength = 12))\n\n\n\n\n\n\n\n\nVisualization 3: Dual-Axis Chart - Employment Level vs Revision\n\n\nCode\n# Prepare data for dual axis\nviz3_data &lt;- ces_stats |&gt;\n  select(date, level, revision, abs_revision) |&gt;\n  drop_na()\n\n# Calculate scaling factor for secondary axis\nscale_factor &lt;- max(viz3_data$level) / max(viz3_data$abs_revision)\n\n# Create dual-axis plot\np3 &lt;- ggplot(viz3_data, aes(x = date)) +\n  # Primary axis: Employment level (area under curve)\n  geom_area(aes(y = level, fill = \"Employment Level\"), alpha = 0.3, color = \"steelblue\") +\n  geom_line(aes(y = level, color = \"Employment Level\"), linewidth = 1) +\n  # Secondary axis: Absolute revision (bar chart)\n  geom_col(aes(y = abs_revision * scale_factor, fill = \"Revision Magnitude\"), alpha = 0.5) +\n  # Dual y-axes\n  scale_y_continuous(\n    name = \"Employment Level (thousands)\",\n    sec.axis = sec_axis(~ . / scale_factor, name = \"Absolute Revision (thousands)\")\n  ) +\n  scale_fill_manual(\n    values = c(\"Employment Level\" = \"steelblue\", \"Revision Magnitude\" = \"red\"),\n    guide = \"legend\"\n  ) +\n  scale_color_manual(\n    values = c(\"Employment Level\" = \"steelblue\"),\n    guide = \"none\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    legend.position = \"bottom\",\n    legend.title = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    title = \"Employment Level vs Revision Magnitude Over Time (1979-2025)\",\n    subtitle = \"Blue area: Total nonfarm employment | Red bars: Absolute revision amount\",\n    x = \"Date\",\n    y = \"Employment Level (thousands)\",\n    caption = \"Source: BLS CES Data\"\n  )\n\nprint(p3)\n\n\n\n\n\n\n\n\n\n\n\nVisualization 4: Pre-2020 vs Post-2020 Distrbution of Revision Calculations\nINTERPRETATION:\nIf curves have same shape = revisions behave similarly\nIf Post-2020 (red) is wider = revisions are more variable/unpredictable\nIf Post-2020 is shifted right = revisions tend to be larger (positive)\nIf Post-2020 is shifted left = revisions tend to be larger (negative)\n\n\nCode\n# Prepare data for density plot\ndensity_data &lt;- ces_stats %&gt;%\n  filter(!is.na(revision)) %&gt;%\n  mutate(\n    year = year(date),\n    period = if_else(year &lt; 2020, \"Pre-2020\", \"Post-2020\")\n  )\n\n# Create density plot\np_density &lt;- ggplot(density_data, aes(x = revision, fill = period, color = period)) +\n  geom_density(alpha = 0.5, linewidth = 1) +\n  scale_fill_manual(values = c(\"Pre-2020\" = \"steelblue\", \"Post-2020\" = \"red\")) +\n  scale_color_manual(values = c(\"Pre-2020\" = \"darkblue\", \"Post-2020\" = \"darkred\")) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"black\", linewidth = 0.8) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  ) +\n  labs(\n    title = \"Distribution of CES Revisions: Pre-2020 vs Post-2020\",\n    subtitle = \"Overlaid density curves show shape and spread of revision distributions\",\n    x = \"Revision Amount (thousands)\",\n    y = \"Density\",\n    caption = \"Source: BLS CES Data\"\n  )\n\nprint(p_density)"
  },
  {
    "objectID": "mp04.html#has-the-fraction-of-revisions-1-increased-post-2020",
    "href": "mp04.html#has-the-fraction-of-revisions-1-increased-post-2020",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Has the fraction of revisions > 1% increased post-2020?",
    "text": "Has the fraction of revisions &gt; 1% increased post-2020?\nNull Hypothesis: Fraction of revisions &gt;1% is equal Pre-2020 and Post-2020\nA proportion test will be applied to determine if revision size is correlated with employment change.\n\n\nCode\n# Prepare data\ntest4_data &lt;- ces_analysis %&gt;%\n  filter(!is.na(revision)) %&gt;%\n  mutate(\n    year = year(date),\n    period = if_else(year &lt; 2020, \"Pre-2020\", \"Post-2020\"),\n    large_revision = pct_revision &gt; 1.0  # TRUE if revision &gt; 1%, FALSE otherwise\n  ) %&gt;%\n  select(large_revision, period)\n\n# Perform proportion test\ntest4_result &lt;- test4_data %&gt;%\n  prop_test(large_revision ~ period, order = c(\"Pre-2020\", \"Post-2020\"))\n\ncat(\"Proportion Test Results:\\n\")\n\n\nProportion Test Results:\n\n\nCode\nprint(test4_result)\n\n\n# A tibble: 1 × 6\n  statistic chisq_df p_value alternative lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1      2.01        1   0.156 two.sided     -0.177   0.0129\n\n\nCode\ncat(\"\\n\\nDETAILED BREAKDOWN:\\n\")\n\n\n\n\nDETAILED BREAKDOWN:\n\n\nCode\n# Calculate actual proportions\nprop_summary_test4 &lt;- test4_data %&gt;%\n  group_by(period) %&gt;%\n  summarise(\n    num_large_revisions = sum(large_revision),\n    total_revisions = n(),\n    pct_large = (sum(large_revision) / n()) * 100,\n    .groups = 'drop'\n  )\n\ndatatable(prop_summary_test4,\n          caption = \"Count and Proportion of Revisions &gt; 1%\",\n          options = list(pageLength = 10))\n\n\n\n\n\n\nCode\ncat(\"\\n\\n RESULTS AND KEY STATISTICS:\\n\")\n\n\n\n\n RESULTS AND KEY STATISTICS:\n\n\nCode\npre_2020_large &lt;- prop_summary_test4 %&gt;% filter(period == \"Pre-2020\") %&gt;% pull(pct_large)\npost_2020_large &lt;- prop_summary_test4 %&gt;% filter(period == \"Post-2020\") %&gt;% pull(pct_large)\ndiff_large &lt;- post_2020_large - pre_2020_large\n\ncat(\"Pre-2020:\", round(pre_2020_large, 2), \"% of revisions were &gt; 1%\\n\")\n\n\nPre-2020: 79.67 % of revisions were &gt; 1%\n\n\nCode\ncat(\"Post-2020:\", round(post_2020_large, 2), \"% of revisions were &gt; 1%\\n\")\n\n\nPost-2020: 87.88 % of revisions were &gt; 1%\n\n\nCode\ncat(\"Difference:\", round(diff_large, 2), \"percentage points\\n\")\n\n\nDifference: 8.2 percentage points\n\n\nCode\ncat(\"Relative increase:\", round((diff_large / pre_2020_large) * 100, 2), \"%\\n\\n\")\n\n\nRelative increase: 10.3 %\n\n\nCode\nif (test4_result$p_value &lt; 0.05) {\n  cat(\"INTERPRETATION:✓ SIGNIFICANT (p &lt; 0.05): The fraction of large revisions (&gt;1%) IS significantly different\\n\")\n  if (post_2020_large &gt; pre_2020_large) {\n    cat(\"  Post-2020 has MORE large revisions than Pre-2020 - SUPPORTS Trump's concern\\n\")\n  } else {\n    cat(\"  Post-2020 has FEWER large revisions than Pre-2020 - CONTRADICTS Trump's concern\\n\")\n  }\n} else {\n  cat(\"INTERPRETATION: ✗ NOT SIGNIFICANT (p ≥ 0.05): No significant difference in large revision frequency\\n\")\n  cat(\"  Observed differences could be due to random variation\\n\")\n}\n\n\nINTERPRETATION: ✗ NOT SIGNIFICANT (p ≥ 0.05): No significant difference in large revision frequency\n  Observed differences could be due to random variation\n\n\nCode\ncat(\"\\n95% Confidence Innterval for difference:\", round(test4_result$lower_ci, 4), \"to\", round(test4_result$upper_ci, 4), \"\\n\")\n\n\n\n95% Confidence Innterval for difference: -0.177 to 0.0129 \n\n\n\nResults & Analysis:\nThe proportion test examining whether the fraction of revisions exceeding 1% has increased post-2020 yielded a p-value of 0.1561. The pre-2020 period saw 79.67% of monthly revisions exceed the 1% threshold, while the post-2020 period experienced 87.88% of revisions at this magnitude. This represents a 8.2 percentage point increase, or roughly a 10.3% relative increase in the frequency of large revisions. The 95% confidence interval for the difference ranges from -0.177 to 0.0129. Despite the observed increase in the percentage of revisions exceeding 1%, this difference does not reach statistical significance (p ≥ 0.05).\nThis suggests that the observed pattern could reasonably occur due to natural variation rather than representing a fundamental shift in BLS methodology or data quality. The absence of statistical significance implies that while recent years may have experienced some larger revisions, the frequency of such events is not dramatically different from historical norms. This finding supports the interpretation that recent revisions, while sometimes substantial, fall within the range of normal variation observed throughout the 45-year study period."
  },
  {
    "objectID": "mp04.html#are-revisions-larger-when-ces-change-is-larger",
    "href": "mp04.html#are-revisions-larger-when-ces-change-is-larger",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Are revisions larger when CES change is larger?",
    "text": "Are revisions larger when CES change is larger?\nResearch Question: Do larger employment changes predict larger revisions? We’ll answer with a T-Test through measuring revision magnitude when employment change is above/below median (Large Employment Changes vs Revisions)\nIf YES: Suggests big revisions are justified by volatile employment.\nIf NO: Suggests revisions are independent of employment volatility.\n\n\nCode\nlibrary(tidyverse)\n\n# Prepare data - calculate month-over-month employment change\ntest5_data &lt;- ces_stats %&gt;%\n  arrange(date) %&gt;%\n  filter(!is.na(revision) & !is.na(level)) %&gt;%\n  mutate(\n    year = year(date),\n    employment_change = level - lag(level),\n    abs_employment_change = abs(employment_change),\n    period = if_else(year &lt; 2020, \"Pre-2020\", \"Post-2020\")\n  ) %&gt;%\n  drop_na(employment_change)\n\nmedian_emp_change &lt;- median(test5_data$abs_employment_change, na.rm = TRUE)\n\ntest5_binary &lt;- test5_data %&gt;%\n  mutate(\n    large_emp_change = abs_employment_change &gt; median_emp_change\n  ) %&gt;%\n  select(abs_revision, large_emp_change)\n\ntest5_ttest &lt;- test5_binary %&gt;%\n  t_test(abs_revision ~ large_emp_change, order = c(\"TRUE\", \"FALSE\"))\n\ncat(\"T-Test Results (Large vs Small Employment Changes):\\n\")\n\n\nT-Test Results (Large vs Small Employment Changes):\n\n\nCode\nprint(test5_ttest)\n\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      1.70  466.  0.0899 two.sided       8.92    -1.39     19.2\n\n\nCode\ncat(\"\\n\\nDETAILED COMPARISON:\\n\")\n\n\n\n\nDETAILED COMPARISON:\n\n\nCode\nemp_change_summary &lt;- test5_data %&gt;%\n  mutate(\n    large_emp_change = abs_employment_change &gt; median_emp_change,\n    emp_category = if_else(large_emp_change, \"Large Emp Change\", \"Small Emp Change\")\n  ) %&gt;%\n  group_by(emp_category) %&gt;%\n  summarise(\n    mean_revision = mean(abs_revision),\n    median_revision = median(abs_revision),\n    sd_revision = sd(abs_revision),\n    n = n(),\n    .groups = 'drop'\n  )\n\ndatatable(emp_change_summary,\n          caption = \"Revision Magnitude by Employment Change Size\",\n          options = list(pageLength = 10))\n\n\n\n\n\n\nCode\nif (test5_ttest$p_value &lt; 0.05) {\n  cat(\"INTERPRETATION: ✓ SIGNIFICANT (p &lt; 0.05): Revisions ARE larger when employment changes are larger\\n\")\n  cat(\"  This supports the explanation that big revisions follow big employment swings\\n\")\n} else {\n  cat(\"INTERPRETATION: ✗ NOT SIGNIFICANT (p ≥ 0.05): Revision size doesn't depend on employment change\\n\")\n  cat(\"  Big revisions occur regardless of employment volatility\\n\")\n}\n\n\nINTERPRETATION: ✗ NOT SIGNIFICANT (p ≥ 0.05): Revision size doesn't depend on employment change\n  Big revisions occur regardless of employment volatility\n\n\nCode\ncat(\"\\nEstimated difference:\", round(test5_ttest$estimate, 4), \"thousand\\n\")\n\n\n\nEstimated difference: 8.9151 thousand\n\n\nCode\ncat(\"\\n95% Confidence Innterval for difference:\", round(test5_ttest$lower_ci, 4), \"to\", round(test5_ttest$upper_ci, 4), \"thousand\\n\")\n\n\n\n95% Confidence Innterval for difference: -1.3941 to 19.2243 thousand\n\n\nCode\nlibrary(tidyverse)\n\n\n\nResults & Analysis:\nThe two-sample t-test comparing revision magnitudes when employment changes are large versus small revealed a p-value of 0.0899. When monthly employment changes exceeded the median of 202 thousand workers, the mean absolute revision was 61.32 thousand. In contrast, months with smaller employment changes had mean absolute revisions of 52.4 thousand. The estimated difference between these groups is 8.92 thousand, with a 95% confidence interval of -1.39 to 19.22 thousand.\nThe lack of statistical significance in this relationship (p ≥ 0.05) indicates that revision magnitude is not systematically tied to the size of employment changes. This finding suggests that large revisions occur relatively independently of employment volatility, which raises concerns about whether the BLS’s initial estimation methodology adequately captures labor market dynamics even when employment movements are substantial. If revisions were a natural consequence of large employment swings, we would expect to see this statistical relationship. The absence of such a relationship implies that the drivers of recent large revisions may lie elsewhere—potentially in methodology, data collection challenges, or other systematic factors. This pattern supports the interpretation that recent revisions represent a departure from historical norms rather than simply reflecting normal variation."
  },
  {
    "objectID": "mp04.html#hypothesis-i-democratic-presidents-have-more-negative-revisions-and-often-lie-about-raw-employment-numbers-to-inflate-their-public-perceptions.",
    "href": "mp04.html#hypothesis-i-democratic-presidents-have-more-negative-revisions-and-often-lie-about-raw-employment-numbers-to-inflate-their-public-perceptions.",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Hypothesis I: Democratic Presidents have more negative revisions and often lie about raw employment numbers to inflate their public perceptions.",
    "text": "Hypothesis I: Democratic Presidents have more negative revisions and often lie about raw employment numbers to inflate their public perceptions.\nSource: Donald Trump: “The Harris-Biden Administration has been caught fraudulently manipulating Job Statistics.” August 21, 2024\nPolitifact Rating: PANTS ON FIRE\nAnalysis: This claims that Democratic Administrations since 1979 are lying about the true workforce in America through fraudulent BLS reporting that is trying to kill the American workforce with their policies. Trump claims here that the administration at the time “Falsely created”818,000 jobs”.\nHistorical Evidence: According to the top 50 negative revisions of all time (Analysis 2) and the Monthly Revisions Line and Magnitude graphs to some extent (Visualizations 2 & 3), it is actually Republican Administrations who hold this unfortunate distinction, this can largely be attributed to the COVID-19 Pandemic during Donald Trump’s first term and the Great Recession during George W. Bush’s presidency dramatically lowering employment numbers, though it can also be attributed to Republican Policies allowing businesses operate in this manner for their own benefit. While Democrats aren’t immune to the reporting discrepancies, namely during Jimmy Carter’s administration, specifically in this scenario there was zero evidence that the administration was playing with the job numbers to inflate their own image to the general public."
  },
  {
    "objectID": "mp04.html#hypothesis-ii-the-most-egregious-month-for-absolute-revisions-is-september-in-preparation-for-eoy-valuations.",
    "href": "mp04.html#hypothesis-ii-the-most-egregious-month-for-absolute-revisions-is-september-in-preparation-for-eoy-valuations.",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Hypothesis II: The most egregious month for absolute revisions is September in preparation for EOY valuations.",
    "text": "Hypothesis II: The most egregious month for absolute revisions is September in preparation for EOY valuations.\nSource: Scott Walker: Figures for September 2014’s job growth in Wisconsin mark the “largest private-sector job creation we’ve had in the month of September in more than a decade” October 17, 2014\nPolitifact Rating: Mostly True\nAnalysis: This claims that companies will more accurately report their labor statistics in preparation for Q4/Yearly earnings towards the end of the year to seem more enticing to shareholders and in turn drive up their value into the new fiscal year, while also promoting transparent and “audit-Ready FInancial Records”.\nHistorical Evidence: According to the Heatmap (Visualization 1) that tracks Absolute Revisions and the Calendar Months that have the highest positive revision (Analysis 4); historically, the months with the highest total revisions are September, August, and July, respectively. Additionally, September has the highest percent of positive and average absolute revisions across all months by a a sizable margin, showing that companies are showing more honesty to correct reporting mistakes earlier in the year and try to elude any claims of fraudulent inflation that may arise come Q4 Earnings once it is audit season. In terms of the non corporate workforce, September still sees high positive revisions due to education/municipal related hirings being reported around this time of year."
  }
]